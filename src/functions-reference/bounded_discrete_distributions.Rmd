# Bounded Discrete Distributions

Bounded discrete probability functions have support on $\{ 0, \ldots,
N \}$ for some upper bound $N$.

```{r results='asis', echo=FALSE}
if (knitr::is_html_output()) {
cat(' * <a href="binomial-distribution.html">Binomial Distribution</a>\n')
cat(' * <a href="binomial-distribution-logit-parameterization.html">Binomial Distribution, Logit Parameterization</a>\n')
cat(' * <a href="beta-binomial-distribution.html">Beta-Binomial Distribution</a>\n')
cat(' * <a href="hypergeometric-distribution.html">Hypergeometric Distribution</a>\n')
cat(' * <a href="categorical-distribution.html">Categorical Distribution</a>\n')
cat(' * <a href="categorical-logit-glm.html">Categorical Logit Generalised Linear Model (Softmax Regression)</a>\n')
cat(' * <a href="ordered-logistic-distribution.html">Ordered Logistic Distribution</a>\n')
cat(' * <a href="ordered-logistic-glm.html">Ordered Logistic Generalised Linear Model (Ordinal Regression)</a>\n')
cat(' * <a href="ordered-probit-distribution.html">Ordered Probit Distribution</a>\n')
}
```

## Binomial Distribution

### Probability Mass Function

Suppose $N \in \mathbb{N}$ and $\theta \in [0,1]$, and $n \in
\{0,\ldots,N\}$. \[ \text{Binomial}(n~|~N,\theta) = \binom{N}{n}
\theta^n (1 - \theta)^{N - n}. \]

### Log Probability Mass Function

\begin{eqnarray*} \log \text{Binomial}(n~|~N,\theta) & = & \log
\Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt] & & {
} + n \log \theta + (N - n) \log (1 - \theta), \end{eqnarray*}

### Gradient of Log Probability Mass Function

\[ \frac{\partial}{\partial \theta} \log \text{Binomial}(n~|~N,\theta)
= \frac{n}{\theta} - \frac{N - n}{1 - \theta} \]

### Sampling Statement

`n ~ ` **`binomial`**`(N, theta)`

Increment target log probability density with `binomial_lpmf(n | N, theta)`
dropping constant additive terms.
<!-- real ; binomial ~; -->
\index{{\tt \bfseries binomial }!sampling statement|hyperpage}

### Stan Functions

<!-- real; binomial_lpmf; (ints n | ints N, reals theta); -->
\index{{\tt \bfseries binomia\_lpmf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

`real` **`binomial_lpmf`**`(ints n | ints N, reals theta)`<br>\newline
The log binomial probability mass of n successes in N trials given
chance of success theta

<!-- real; binomial_cdf; (ints n, ints N, reals theta); -->
\index{{\tt \bfseries binomia\_cdf }!{\tt (ints n, ints N, reals theta): real}|hyperpage}

`real` **`binomial_cdf`**`(ints n, ints N, reals theta)`<br>\newline
The binomial cumulative distribution function of n successes in N
trials given chance of success theta

<!-- real; binomial_lcdf; (ints n | ints N, reals theta); -->
\index{{\tt \bfseries binomia\_lcdf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

`real` **`binomial_lcdf`**`(ints n | ints N, reals theta)`<br>\newline
The log of the binomial cumulative distribution function of n
successes in N trials given chance of success theta

<!-- real; binomial_lccdf; (ints n | ints N, reals theta); -->
\index{{\tt \bfseries binomia\_lccdf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

`real` **`binomial_lccdf`**`(ints n | ints N, reals theta)`<br>\newline
The log of the binomial complementary cumulative distribution function
of n successes in N trials given chance of success theta

<!-- R; binomial_rng; (ints N, reals theta); -->
\index{{\tt \bfseries binomial\_rng }!{\tt (ints N, reals theta): R}|hyperpage}

`R` **`binomial_rng`**`(ints N, reals theta)`<br>\newline
Generate a binomial variate with N trials and chance of success theta;
may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
[vectorized PRNG functions](#prng-vectorization).

## Binomial Distribution, Logit Parameterization

Stan also provides a version of the binomial probability mass function
distribution with the chance of success parameterized on the
unconstrained logistic scale.

### Probability Mass Function

Suppose $N \in \mathbb{N}$, $\alpha \in \mathbb{R}$, and $n \in
\{0,\ldots,N\}$.  Then  \begin{eqnarray*}
\text{BinomialLogit}(n~|~N,\alpha) & = &
\text{Binomial}(n~|~N,\text{logit}^{-1}(\alpha)) \\[6pt] & = &
\binom{N}{n} \left( \text{logit}^{-1}(\alpha) \right)^{n}  \left( 1 -
\text{logit}^{-1}(\alpha) \right)^{N - n}.  \end{eqnarray*}

### Log Probability Mass Function

\begin{eqnarray*} \log \text{BinomialLogit}(n~|~N,\alpha) & = & \log
\Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt]   & &
{ } + n \log \text{logit}^{-1}(\alpha) + (N - n) \log \left( 1 -
\text{logit}^{-1}(\alpha) \right), \end{eqnarray*}

### Gradient of Log Probability Mass Function

\[ \frac{\partial}{\partial \alpha} \log
\text{BinomialLogit}(n~|~N,\alpha) =
\frac{n}{\text{logit}^{-1}(-\alpha)} - \frac{N -
n}{\text{logit}^{-1}(\alpha)} \]

### Sampling Statement

`n ~ ` **`binomial_logit`**`(N, alpha)`

Increment target log probability density with `binomial_logit_lpmf(n | N, alpha)`
dropping constant additive terms.
<!-- real; binomial_logit ~; -->
\index{{\tt \bfseries binomial\_logit }!sampling statement|hyperpage}

### Stan Functions

<!-- real; binomial_logit_lpmf; (ints n | ints N, reals alpha); -->
\index{{\tt \bfseries binomial\_logit\_lpmf }!{\tt (ints n \textbar\ ints N, reals alpha): real}|hyperpage}

`real` **`binomial_logit_lpmf`**`(ints n | ints N, reals alpha)`<br>\newline
The log binomial probability mass of n successes in N trials given
logit-scaled chance of success alpha

## Beta-Binomial Distribution

### Probability Mass Function

If $N \in \mathbb{N}$, $\alpha \in \mathbb{R}^+$, and $\beta \in
\mathbb{R}^+$, then for $n \in {0,\ldots,N}$, \[
\text{BetaBinomial}(n~|~N,\alpha,\beta) = \binom{N}{n}
\frac{\mathrm{B}(n+\alpha, N -n +   \beta)}{\mathrm{B}(\alpha,\beta)},
\] where the beta function $\mathrm{B}(u,v)$ is defined for $u \in
\mathbb{R}^+$ and $v \in \mathbb{R}^+$ by \[ \mathrm{B}(u,v) =
\frac{\Gamma(u) \ \Gamma(v)}{\Gamma(u + v)}. \]

### Sampling Statement

`n ~ ` **`beta_binomial`**`(N, alpha, beta)`

Increment target log probability density with `beta_binomial_lpmf(n | N, alpha, beta)`
dropping constant additive terms.
<!-- real; beta_binomial ~; -->
\index{{\tt \bfseries beta\_binomial }!sampling statement|hyperpage}

### Stan Functions

<!-- real; beta_binomial_lpmf; (ints n | ints N, reals alpha, reals beta); -->
\index{{\tt \bfseries beta\_binomial\_lpmf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

`real` **`beta_binomial_lpmf`**`(ints n | ints N, reals alpha, reals beta)`<br>\newline
The log beta-binomial probability mass of n successes in N trials
given prior success count (plus one) of alpha and prior failure count
(plus one) of beta

<!-- real; beta_binomial_cdf; (ints n, ints N, reals alpha, reals beta); -->
\index{{\tt \bfseries beta\_binomial\_cdf }!{\tt (ints n, ints N, reals alpha, reals beta): real}|hyperpage}

`real` **`beta_binomial_cdf`**`(ints n, ints N, reals alpha, reals beta)`<br>\newline
The beta-binomial cumulative distribution function of n successes in N
trials given prior success count (plus one) of alpha and prior failure
count (plus one) of beta

<!-- real; beta_binomial_lcdf; (ints n | ints N, reals alpha, reals beta); -->
\index{{\tt \bfseries beta\_binomial\_lcdf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

`real` **`beta_binomial_lcdf`**`(ints n | ints N, reals alpha, reals beta)`<br>\newline
The log of the beta-binomial cumulative distribution function of n
successes in N trials given prior success count (plus one) of alpha
and prior failure count (plus one) of beta

<!-- real; beta_binomial_lccdf; (ints n | ints N, reals alpha, reals beta); -->
\index{{\tt \bfseries beta\_binomial\_lccdf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

`real` **`beta_binomial_lccdf`**`(ints n | ints N, reals alpha, reals beta)`<br>\newline
The log of the beta-binomial complementary cumulative distribution
function of n successes in N trials given prior success count (plus
one) of alpha and prior failure count (plus one) of beta

<!-- R; beta_binomial_rng; (ints N, reals alpha, reals beta); -->
\index{{\tt \bfseries beta\_binomial\_rng }!{\tt (ints N, reals alpha, reals beta): R}|hyperpage}

`R` **`beta_binomial_rng`**`(ints N, reals alpha, reals beta)`<br>\newline
Generate a beta-binomial variate with N trials, prior success count
(plus one) of alpha, and prior failure count (plus one) of beta; may
only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
[vectorized PRNG functions](#prng-vectorization).

## Hypergeometric Distribution

### Probability Mass Function

If $a \in \mathbb{N}$, $b \in \mathbb{N}$, and $N \in
\{0,\ldots,a+b\}$, then for $n \in \{\max(0,N-b),\ldots,\min(a,N)\}$,
\[ \text{Hypergeometric}(n~|~N,a,b) = \frac{\normalsize{\binom{a}{n}
\binom{b}{N - n}}}      {\normalsize{\binom{a + b}{N}}}. \]

### Sampling Statement

`n ~ ` **`hypergeometric`**`(N, a, b)`

Increment target log probability density with `hypergeometric_lpmf(n | N, a, b)`
dropping constant additive terms.
<!-- real; hypergeometric ~; -->
\index{{\tt \bfseries hypergeometric }!sampling statement|hyperpage}

### Stan Functions

<!-- real; hypergeometric_lpmf; (int n ~|~ int N, int a, int b); -->
\index{{\tt \bfseries hypergeometric\_lpmf }!{\tt (int n ~|~ int N, int a, int b): real}|hyperpage}

`real` **`hypergeometric_lpmf`**`(int n ~|~ int N, int a, int b)`<br>\newline
The log hypergeometric probability mass of n successes in N trials
given total success count of a and total failure count of b

<!-- int; hypergeometric_rng; (int N, int a, int2 b); -->
\index{{\tt \bfseries hypergeometric\_rng }!{\tt (int N, int a, int2 b): int}|hyperpage}

`int` **`hypergeometric_rng`**`(int N, int a, int2 b)`<br>\newline
Generate a hypergeometric variate with N trials, total success count
of a, and total failure count of b; may only be used in transformed data and
generated quantities blocks

## Categorical Distribution {#categorical-distribution}

### Probability Mass Functions

If $N \in \mathbb{N}$, $N > 0$, and if $\theta \in \mathbb{R}^N$ forms
an $N$-simplex (i.e., has nonnegative entries summing to one), then
for $y \in \{1,\ldots,N\}$, \[ \text{Categorical}(y~|~\theta) =
\theta_y. \] In addition, Stan provides a log-odds scaled categorical
distribution, \[ \text{CategoricalLogit}(y~|~\beta) =
\text{Categorical}(y~|~\text{softmax}(\beta)). \] See section
[softmax](#softmax) for the definition of the softmax function.

### Sampling Statement

`y ~ ` **`categorical`**`(theta)`

Increment target log probability density with `categorical_lpmf(y | theta)`
dropping constant additive terms.
<!-- real; categorical ~; -->
\index{{\tt \bfseries categorical }!sampling statement|hyperpage}

### Sampling Statement

`y ~ ` **`categorical_logit`**`(beta)`

Increment target log probability density with `categorical_logit_lpmf(y | beta)`
dropping constant additive terms.
<!-- real; categorical_logit ~; -->
\index{{\tt \bfseries categorical\_logit }!sampling statement|hyperpage}

### Stan Functions

All of the categorical distributions are vectorized so that the
outcome y can be a single integer (type `int`) or an array of integers
(type `int[]`).

<!-- real; categorical_lpmf; (ints y | vector theta); -->
\index{{\tt \bfseries categorical\_lpmf }!{\tt (ints y \textbar\ vector theta): real}|hyperpage}

`real` **`categorical_lpmf`**`(ints y | vector theta)`<br>\newline
The log categorical probability mass function with outcome(s) y in
$1:N$ given $N$-vector of outcome probabilities theta. The parameter
theta must have non-negative entries that sum to one, but it need not
be a variable declared as a simplex.

<!-- real; categorical_logit_lpmf; (ints y | vector beta); -->
\index{{\tt \bfseries categorical\_logit\_lpmf }!{\tt (ints y \textbar\ vector beta): real}|hyperpage}

`real` **`categorical_logit_lpmf`**`(ints y | vector beta)`<br>\newline
The log categorical probability mass function with outcome(s) y in
$1:N$ given log-odds of outcomes beta.

<!-- int; categorical_rng; (vector theta); -->
\index{{\tt \bfseries categorical\_rng }!{\tt (vector theta): int}|hyperpage}

`int` **`categorical_rng`**`(vector theta)`<br>\newline
Generate a categorical variate with $N$-simplex distribution parameter
theta; may only be used in transformed data and generated quantities blocks

<!-- int; categorical_logit_rng; (vector beta); -->
\index{{\tt \bfseries categorical\_logit\_rng }!{\tt (vector beta): int}|hyperpage}

`int` **`categorical_logit_rng`**`(vector beta)`<br>\newline
Generate a categorical variate with outcome in range $1:N$ from
log-odds vector beta; may only be used in transformed data and generated
quantities blocks

## Categorical Logit Generalised Linear Model (Softmax Regression) {#categorical-logit-glm}

Stan also supplies a single primitive for a Generalised Linear Model
with Categorical likelihood and logit link function, i.e. a primitive
for a softmax regression. This should provide a more efficient
implementation of softmax regression than a manually written
regression in terms of a Categorical likelihood and matrix
multiplication.

Note that the implementation does not put any restrictions on the coefficient matrix $\beta$. It is up to the user to use a reference category, a suitable prior or some other means of avoiding non-identifiability. See Multi-logit in the [Stan User's Guide](https://mc-stan.org/docs/2_21/stan-users-guide/multi-logit-section.html).

### Probability Mass Functions

If $N,M,K \in \mathbb{N}$, $N,M,K > 0$, and if $x\in \mathbb{R}^{M\cdot K}, \alpha \in \mathbb{R}^N, \beta\in \mathbb{R}^{K\cdot N}$, then for $y \in \{1,\ldots,N\}^M$, 
\[ \text{CategoricalLogitGLM}(y~|~x,\alpha,\beta) = \\[5pt]
\prod_{1\leq i \leq M}\text{CategoricalLogit}(y_i~|~\alpha+x_i\cdot\beta) = \\[15pt] 
\prod_{1\leq i \leq M}\text{Categorical}(y_i~|~softmax(\alpha+x_i\cdot\beta)). \] 
See section [softmax](#softmax) for the definition of the softmax function.

### Sampling Statement

`y ~ ` **`categorical_logit_glm`**`(x, alpha, beta)`

Increment target log probability density with `categorical_logit_glm(y | x, alpha, beta)`
dropping constant additive terms.
<!-- real; categorical_logit_glm ~; -->
\index{{\tt \bfseries categorical\_logit\_glm }!sampling statement|hyperpage}


### Stan Functions

<!-- real; categorical_logit_glm_lpmf; (int y | row_vector x, vector alpha, matrix beta); -->
\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

`real` **`categorical_logit_glm_lpmf`**`(int y | row_vector x, vector alpha, matrix beta)`<br>\newline
The log categorical probability mass function with outcome `y` in
$1:N$ given $N$-vector of log-odds of outcomes `alpha + x * beta`.
The size of independent variable row vector `x` needs to match the number of rows of the
weight matrix `beta`. The size of intercept vector `alpha` must match number 
of columns of the weight matrix `beta`.

<!-- real; categorical_logit_glm_lpmf; (int y | matrix x, vector alpha, matrix beta); -->
\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

`real` **`categorical_logit_glm_lpmf`**`(int y | matrix x, vector alpha, matrix beta)`<br>\newline
The log categorical probability mass function with outcomes `y` in
$1:N$ given $N$-vector of log-odds of outcomes `alpha + x * beta`.
Same vector of intercepts `alpha` and same dependant variable value `y` are used for all instances.
The number of columns of independent variable `x` needs to match the number of rows of the
weight matrix `beta`. The size of intercept vector `alpha` must match number 
of columns of the weight matrix `beta`. If `x` and `y` are data (not parameters) this function can be executed on a GPU.

<!-- real; categorical_logit_glm_lpmf; (int[] y | vector theta); -->
\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

`real` **`categorical_logit_glm_lpmf`**`(int[] y | row_vector x, vector alpha, matrix beta)`<br>\newline
The log categorical probability mass function with outcomes `y` in
$1:N$ given $N$-vector of log-odds of outcomes `alpha + x * beta`.
Same vector of intercepts `alpha` and same row vector of independent variables `x` are used for all instances.
The size of independent variable matrix `x` needs to match the number of rows of the
weight vector `beta`. The size of intercept vector `alpha` must match number 
of columns of the weight vector `beta`.

<!-- real; categorical_logit_glm_lpmf; (int[] y | vector theta); -->
\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

`real` **`categorical_logit_glm_lpmf`**`(int[] y | matrix x, vector alpha, matrix beta)`<br>\newline
The log categorical probability mass function with outcomes `y` in
$1:N$ given $N$-vector of log-odds of outcomes `alpha + x * beta`. 
Same vector of intercepts `alpha` is used for all instances.
The number of rows of the independent variable
matrix `x` needs to match the length of the dependent variable vector
`y`. The number of columns of independnt variable `x` needs to match the number of rows of the
weight matrix `beta`. The size of intercept vector `alpha` must match number 
of columns of the weight matrix `beta`. If `x` and `y` are data (not parameters) this function can be executed on a GPU.


## Ordered Logistic Distribution

### Probability Mass Function

If $K \in \mathbb{N}$ with $K > 2$, $c \in \mathbb{R}^{K-1}$ such that
$c_k < c_{k+1}$ for $k \in \{1,\ldots,K-2\}$, and $\eta \in
\mathbb{R}$, then for $k \in \{1,\ldots,K\}$, \[
\text{OrderedLogistic}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 -
\text{logit}^{-1}(\eta - c_1)  &  \text{if } k = 1, \\[4pt]
\text{logit}^{-1}(\eta - c_{k-1}) - \text{logit}^{-1}(\eta - c_{k})  &
\text{if } 1 < k < K, \text{and} \\[4pt] \text{logit}^{-1}(\eta -
c_{K-1}) - 0  &  \text{if } k = K. \end{array} \right. \] The $k=K$
case is written with the redundant subtraction of zero to illustrate
the parallelism of the cases; the $k=1$ and $k=K$ edge cases can be
subsumed into the general definition by setting $c_0 = -\infty$ and
$c_K = +\infty$ with $\text{logit}^{-1}(-\infty) = 0$ and
$\text{logit}^{-1}(\infty) = 1$.

### Sampling Statement

`k ~ ` **`ordered_logistic`**`(eta, c)`

Increment target log probability density with `ordered_logistic_lpmf(k | eta, c)`
dropping constant additive terms.
<!-- real; ordered_logistic ~; -->
\index{{\tt \bfseries ordered\_logistic }!sampling statement|hyperpage}

### Stan Functions

<!-- real; ordered_logistic_lpmf; (ints k | vector eta, vectors c); -->
\index{{\tt \bfseries ordered\_logistic\_lpmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

`real` **`ordered_logistic_lpmf`**`(ints k | vector eta, vectors c)`<br>\newline
The log ordered logistic probability mass of k given linear predictors
eta, and cutpoints c.

<!-- int; ordered_logistic_rng; (real eta, vector c); -->
\index{{\tt \bfseries ordered\_logistic\_rng }!{\tt (real eta, vector c): int}|hyperpage}

`int` **`ordered_logistic_rng`**`(real eta, vector c)`<br>\newline
Generate an ordered logistic variate with linear predictor eta and
cutpoints c; may only be used in transformed data and generated quantities blocks

## Ordered Logistic Generalised Linear Model (Ordinal Regression)

### Probability Mass Function

If $N,M,K \in \mathbb{N}$ with $N, M > 0$, $K > 2$, $c \in \mathbb{R}^{K-1}$ such that
$c_k < c_{k+1}$ for $k \in \{1,\ldots,K-2\}$, and $x\in \mathbb{R}^{N\cdot M}, \beta\in \mathbb{R}^M$, then for $y \in \{1,\ldots,K\}^N$, 
\[\text{OrderedLogisticGLM}(y~|~x,\beta,c) = \\[4pt]
\prod_{1\leq i \leq N}\text{OrderedLogistic}(y_i~|~x_i\cdot \beta,c) = \\[17pt]
\prod_{1\leq i \leq N}\left\{ \begin{array}{ll}
1 - \text{logit}^{-1}(x_i\cdot \beta - c_1)  &  \text{if } y = 1, \\[4pt]
\text{logit}^{-1}(x_i\cdot \beta - c_{y-1}) - \text{logit}^{-1}(x_i\cdot \beta - c_{y}) & \text{if } 1 < y < K, \text{and} \\[4pt]
\text{logit}^{-1}(x_i\cdot \beta - c_{K-1}) - 0  &  \text{if } y = K. 
\end{array} \right. \] The $k=K$
case is written with the redundant subtraction of zero to illustrate
the parallelism of the cases; the $y=1$ and $y=K$ edge cases can be
subsumed into the general definition by setting $c_0 = -\infty$ and
$c_K = +\infty$ with $\text{logit}^{-1}(-\infty) = 0$ and
$\text{logit}^{-1}(\infty) = 1$.

### Sampling Statement

`y ~ ` **`ordered_logistic_glm`**`(x, beta, c)`

Increment target log probability density with `ordered_logistic_lpmf(y | x, beta, c)`
dropping constant additive terms.
<!-- real; ordered_logistic ~; -->
\index{{\tt \bfseries ordered\_logistic\_glm }!sampling statement|hyperpage}

### Stan Functions

<!-- real; ordered_logistic_glm_lpmf; (int y | row_vector x, vector beta, vector c); -->
\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

`real` **`ordered_logistic_glm_lpmf`**`(int y | row_vector x, vector beta, vector c)`<br>\newline
The log ordered logistic probability mass of y, given linear predictors `x * beta`, and cutpoints c. 
The size of independent variable row vector `x` needs to match the size of the weight vector `beta`. 
Cutpoints `c` must be ordered.

<!-- real; ordered_logistic_glm_lpmf; (int y | matrix x, vector beta, vector c); -->
\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

`real` **`ordered_logistic_glm_lpmf`**`(int y | matrix x, vector beta, vector c)`<br>\newline
The log ordered logistic probability mass of y, given linear predictors `x * beta`, and cutpoints c. 
Same value of independent variable `y` is used for all instances.
The number of columns of independent variable row vector `x` needs to match the size of the weight vector `beta`.
Cutpoints `c` must be ordered. If `x` and `y` are data (not parameters) this function can be executed on a GPU.

<!-- real; ordered_logistic_glm_lpmf; (int[] y | row_vector x, vector beta, vector c); -->
\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

`real` **`ordered_logistic_glm_lpmf`**`(int[] y | row_vector x, vector beta, vector c)`<br>\newline
The log ordered logistic probability mass of y, given linear predictors `x * beta`, and cutpoints c. 
Same row vector of independent variables `x` is used for all instances.
The size of independent variable row vector `x` needs to match the size of the weight vector `beta`. 
Cutpoints `c` must be ordered.

<!-- real; ordered_logistic_glm_lpmf; (int[] y | matrix x, vector beta, vector c); -->
\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

`real` **`ordered_logistic_glm_lpmf`**`(int[] y | matrix x, vector beta, vector c)`<br>\newline
The log ordered logistic probability mass of y, given linear predictors
`x * beta`, and cutpoints c. 
The number of rows of the independent variable matrix `x` needs to match the length of the dependent variable vector `y`.
The number of columns of independent variable row vector `x` needs to match the size of the weight vector `beta`.
Cutpoints `c` must be ordered. If `x` and `y` are data (not parameters) this function can be executed on a GPU.


## Ordered Probit Distribution

### Probability Mass Function

If $K \in \mathbb{N}$ with $K > 2$, $c \in \mathbb{R}^{K-1}$ such that
$c_k < c_{k+1}$ for $k \in \{1,\ldots,K-2\}$, and $\eta \in
\mathbb{R}$, then for $k \in \{1,\ldots,K\}$, \[
\text{OrderedProbit}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 -
\Phi(\eta - c_1) & \text{if } k = 1, \\[4pt] \Phi(\eta - c_{k-1}) -
\Phi(\eta - c_{k})  & \text{if } 1 < k < K, \text{and} \\[4pt]
\Phi(\eta - c_{K-1}) - 0 & \text{if } k = K. \end{array} \right. \]
The $k=K$ case is written with the redundant subtraction of zero to
illustrate the parallelism of the cases; the $k=1$ and $k=K$ edge
cases can be subsumed into the general definition by setting $c_0 =
-\infty$ and $c_K = +\infty$ with $\Phi(-\infty) = 0$ and
$\Phi(\infty) = 1$.

### Sampling Statement

`k ~ ` **`ordered_probit`**`(eta, c)`

Increment target log probability density with `ordered_probit_lpmf(k | eta, c)`
dropping constant additive terms.
<!-- real; ordered_probit ~; -->
\index{{\tt \bfseries ordered\_probit }!sampling statement|hyperpage}

### Stan Functions

<!-- real; ordered_probit_lpmf; (ints k | vector eta, vectors c); -->
\index{{\tt \bfseries ordered\_probit\_lpmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

`real` **`ordered_probit_lpmf`**`(ints k | vector eta, vectors c)`<br>\newline
The log ordered probit probability mass of k given linear predictors
eta, and cutpoints c.

<!-- int; ordered_probit_rng; (real eta, vector c); -->
\index{{\tt \bfseries ordered\_probit\_rng }!{\tt (real eta, vector c): int}|hyperpage}

`int` **`ordered_probit_rng`**`(real eta, vector c)`<br>\newline
Generate an ordered probit variate with linear predictor eta and
cutpoints c; may only be used in transformed data and generated quantities blocks

