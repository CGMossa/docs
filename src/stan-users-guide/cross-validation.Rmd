# Held-Out and Cross-Validation

Held-out evaluation involves splitting a data set into two parts, a
training data set and a test data set.  The training data is used to
estimate the model and the test data is used for evaluation.  Held-out
evaluation is commonly used to declare winners in predictive modeling
competitions such as those run by Kaggle.

Cross-validation involves repeated held-out evaluations with the same
data set.  The training/test split can be done either by randomly
selecting the test set, or by partitioning the data set into several
equally-sized subsets and then using each subset in turn as the test
data with the other folds as training data.

Held-out evaluation, and hence cross-validation, may involve any kind
of predictive statistics, with common choices being the predictive log
density on test data, squared error of parameter estimates, or accuracy
in a classification task.

This chapter begins with evaluating the posterior predictive density
for test data.

## Evaluating posterior predictive densities

Given training data $(x, y)$ consisting of parallel sequences of
predictors and observations and test data $(\tilde{x}, \tilde{y})$ of
the same structure, the posterior predictive density is
$$
p(\tilde{y} \mid \tilde{x}, x, y)
=
\int
  p(\tilde{y} \mid \tilde{x}, \theta)
  \cdot p(\theta \mid x, y)
\, \textrm{d}\theta,
$$

where $\theta$ is the vector of model parameters.  This predictive
density is the density of the test observations, conditioned on both
the test predictors $\tilde{x}$ and the training data $(x, y).$

This integral may be calculated with Monte Carlo methods as usual,
$$
p(\tilde{y} \mid \tilde{x}, x, y)
\approx
\frac{1}{M} \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
$$
where the $\theta^{(m)} \sim p(\theta \mid x, y)$ are draws from the
posterior given only the training data $(x, y).$

To avoid underflow in calculations, it will be more stable
to compute densities on the log scale.  Taking the logarithm and
pushing it through results in a stable computation,
\begin{eqnarray*}
\log p(\tilde{y} \mid \tilde{x}, x, y)
& \approx &
\log \frac{1}{M} \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
\\[4pt]
& = & -\log M + \log \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
\\[4pt]
& = & -\log M + \log \sum_{m = 1}^M \exp(\log p(\tilde{y} \mid \tilde{x}, \theta^{(m)}))
\\[4pt]
& = & -\log M + \textrm{log-sum-exp}_{m = 1}^M \log p(\tilde{y} \mid \tilde{x}, \theta^{(m)})
\end{eqnarray*}
where the log sum of exponentials function is defined so as
to make the above equation hold,
$$
\textrm{log-sum-exp}_{m = 1}^M \mu_m
= \log \sum_{m=1}^M \exp \mu_m.
$$
The log sum of exponentials function can be implemented so as to avoid
underflow and maintain high arithmetic precision as
$$
\textrm{log-sum-exp}_{m = 1}^M \mu_m
= \textrm{max}(\mu)
+ \sum_{m = 1}^M \exp(\mu_m - \textrm{max}(\mu)).
$$
Pulling the maximum out preserves all of its precision.  By
subtracting the maximum, the terms $\mu_m - \textrm{max}(\mu) \leq 0$,
and thus will not overflow.

### Stan program

To evaluate the log predictive density of a model, it suffices to
implement the log predictive density of the test data given the generated quantities
block.  The log sum of exponentials calculation must be done on the
outside using the posterior draws of $\log p(\tilde{y} \mid \tilde{x},
\theta^{(m)}).$

Here is the code for evaluating the log posterior predictive density
in a simple linear regression of the test data $\tilde{y}$ given
predictors $\tilde{x}$ and training data $(x, y).$
```
data {
  int<lower = 0> N;
  vector[N] y;
  vector[N] x;
  int<lower = 0> N_tilde;
  vector[N_tilde] x_tilde;
  vector[N_tilde] y_tilde;
}
parameters {
  real alpha;
  real beta;
  real<lower = 0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
generated quantities {
  real log_p = normal_lpdf(y_tilde | alpha + beta * x_tilde, sigma);
}
```
Only the training data `x` and `y` are used in the model block.  The
test data `y_tilde` and test predictors `x_tilde` appear in only the
generated quantities block.  Thus the program is not cheating by using
the test data during training.  Although this model does not do so,
it would be fair to use `x_tilde` in the model block---only the
test observations `y_tilde` are unknown before they are predicted.

Given $M$ posterior draws from Stan, the sequence `log_p[1:M]` will be
available, so that the log posterior predictive density of the test
data given training data and predictors is just `log_sum_exp(log_p) -
log(M)`.


## Estimation error

### Parameter estimates

Estimation is usually considered for unknown parameters.  If the
data from which the parameters were estimated came from simulated
data, the true value of the parameters may be known.  If $\theta$ is
the true value and $\hat{\theta}$ the estimate, then error is just
$$
\textrm{err} = \hat{\theta} - \theta.
$$
If the estimate is larger than the true value, the error is positive.
Typically absolute error,
$$
\textrm{abs-err} = \left| \hat{\theta} - \theta \right|,
$$
or square error are used
$$
\textrm{sq-err} = \left( \hat{\theta} - \theta \right)^2.
$$

Bayesian posterior means minimize expected square error, whereas
posterior medians minimize expected absolute error.  Estimates based
on modes rather than probability, such as (penalized) maximum
likelihood estimates or maximum a posterior estimates, do not have
these properties.

### Predictive estimates

In addition to parameters, other unknown quantities may be estimated,
such as the score of a football match or the effect of a medical
treatment given to a subject.  In these cases, square error is defined
in the same way.  If there are multiple exchangeable outcomes being
estimated, $z_1, \ldots, z_N,$ then it is common to report mean square
error,
$$
\textrm{mean-sq-err}
= \frac{1}{N} \sum_{n = 1}^N \left( \hat{z}_n - z_n\right)^2.
$$
To put the error back on the scale of the original value, the square
root may be applied, resulting in what is known prosaically
as *root mean square error,*
$$
\textrm{rmse} = \sqrt{\textrm{mean-sq-err}}.
$$

### Predictive estimates in Stan

Consider again the linear regression from the previous section, with
parameters $\alpha$ and $\beta$ and predictor $\tilde{x}_n$.  The
standard Bayesian estimate is the expected value of $\tilde{y}$ given
the predictors and training data,
\begin{eqnarray*}
\hat{\tilde{y}}_n
& = & \mathbb{E}\left[
          \alpha + \beta \cdot \tilde{x}_n
          \mid \tilde{x}, x, y
       \right]
\\[4pt]
& \approx &
\frac{1}{M} \sum_{m = 1}^M
  \alpha^{(m)} + \beta^{(m)} \cdot \tilde{x}_n
\\[4pt]
& = &
\hat{\alpha} + \hat{\beta} \cdot \tilde{x}_n.
\end{eqnarray*}
The last line only follows because the model is linear; in the general
non-linear case, the mean in the second line must be used.  The mean
may be calculated by defining the per-draw quantity
$$
\tilde{y}_n^{(m)}
= \alpha^{(m)} + \beta^{(m)} \cdot \tilde{x}_n.
$$
In Stan, this is just the mean of the usual predictive quantities.  It
would be defined in the generated quantities block as
```
data {
  int<lower = 0> N_tilde;
  vector[N_tilde] x_tilde;
  ...
generated quantities {
  vector[N_tilde] tilde_y = alpha + beta * x_tilde;
```
The posterior mean of `tilde_y` calculated by Stan is the Bayesian
estimate $\hat{\tilde{y}}.$  The posterior median may also be
calculated and used as an estimate, though square error and the
posterior mean are more commonly reported.

## Cross-validation

Cross-validation involves choosing multiple subsets of a data set as
the test set and using the other data as training.  This can be done
by partitioning the data and using each subset in turn as the test set
with the remaining subsets as training data.  A partition into ten
subsets is common to reduce computational overhead.  In the limit,
when the test set is just a single item, the result is known as
leave-one-out (LOO) cross-validation.

An alternative to partitioning, which is very index fiddly, is to use
random subsets for testing and their complement for training.  This
allows any size test set to be conveniently used.  It can also be
easily implemented in Stan.

### Stan implementation with random folds

For the simple linear regression model, randomized cross-validation
can be implemented in a single model.

```
data {
  int<lower = 0> N;
  vector[N] x;
  vector[N] y;
  int<lower = 0, upper = N> N_test;
}
transformed data {
  int N_train = N - N_test;
  int permutation[N] = permutation(N);
  vector[N_train] x_train = x[permutation[1 : N_train]];
  vector[N_train] y_train = y[permutation[1 : N_train]];
  vector[N_test] x_test = x[permutation[N_train + 1 : N]];
  vector[N_test] y_test = y[permutation[N_train + 1 : N]];
}
parameters {
  real alpha;
  real beta;
  real<lower = 0> sigma;
}
model {
  y_test ~ normal(alpha + beta * x_test, sigma);
  { alpha, beta, sigma } ~ normal(0, 1);
}
generated quantities {
  vector[N] y_test_hat = normal_rng(alpha + beta * x_test, sigma);
  vector[N] err = y_test_hat - y_test;
}
```
The posterior mean is the estimate for the test observations,
$$
\hat{y}^{\textrm{test}}
= \mathbb{E}\left[ y^{\textrm{test}} \mid \tilde{x}, x, y \right].
$$

Because the test set is constant and the expectation operator is
linear, the posterior mean of `err` as defined in the Stan program
will be the error of the posterior mean estimate,
\begin{eqnarray*}
\mathbb{E}\left[
  y^{\textrm{test}} - y^{\textrm{test}}
\right]
& = &
\mathbb{E}\left[ y^{\textrm{test}} \right]
  - y^{\textrm{test}}
\\[4pt]
& = & \hat{y} - y^{\textrm{test}}
\\[4pt]
& \approx &
\frac{1}{M} \sum_{m = 1}^M \hat{y}^{\textrm{test}(m)} - y^{\textrm{test}}.
\end{eqnarray*}
This is the estimate of the error.  Squaring the estimate is the
estimate of squared error.  Squaring within the Stan program should
not be done as that will produce the wrong answer, because the
estimate itself is just the average.
