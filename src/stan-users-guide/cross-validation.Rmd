# Held-Out Evaluation and Cross-Validation

Held-out evaluation involves splitting a data set into two parts, a
training data set and a testing data set.  The training set is used to
estimate the model and the test set is used for evaluation.  Held-out
evaluation is commonly used to declare winners in predictive modeling
competitions such as those run by Kaggle.

Cross-validation involves partitioning a data set into several folds.
For each fold, held-out evaluation is performed using that fold as a
test data and the remaining folds as training data.

Held-out evaluation and hence cross-validation may involve any kind of
predictive statistics from log density (aka "log loss") to squared
error in the estimation of parameters or event probabilities, to
evaluation of accuracy in a classification task (aka "0/1 loss").

This chapter begins with coding posterior predictive inference for new
data, which forms the basis for not only prediction, but also held-out
evaluation and cross-validation.


## Evaluating posterior predictive densities

Suppose we want to measure square error between an actual observation
$\tilde{y}$ and that predicted by a model based on training data $(x,
y)$ and observed predictors $\tilde{x}.$   Here's an example using
linear regression that provides the basis for computing
$p(\tilde{y} \mid \tilde{x}, x, y)$ on the log scale.

```
data {
  int<lower = 0> N;
  vector[N] y;
  vector[N] x;
  int<lower = 0> N_tilde;
  vector[N_tilde] x_tilde;
  vector[N_tilde] y_tilde;
}
parameters {
  real alpha;
  real beta;
  real<lower = 0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
generated quantities {
  real log_p = normal_lpdf(y_tilde | alpha + beta * x_tilde, sigma);
}
```

Only the training data `x` and `y` are used in the model block.  The
test data `y_tilde` and test predictors `x_tilde` are only used in the
generated quantities block.  Thus the program is not cheating by using
the test data during training.^[It would be fair to use `x_tilde` in
training the mode; in machine learning this is called "semi-supervised
learning" and in statistics a "missing data problem."]

The problem is now that the results are on the log scale as they
pretty much have to be in order to avoid underflow with even
small- to medium-sized data sets.  The desired quantity is
$$
p(\tilde{y} \mid \tilde{x}, x, y)
\approx
\frac{1}{M} \sum_{m = 1}^M
p(\tilde{y}^{(m)} \mid \tilde{x}, \alpha,
\beta, \sigma).
$$
Taking the logarithm of both sides yields
$$
\log p(\tilde{y} \mid \tilde{x}, x, y)
\approx
\log \frac{1}{M} \sum_{m = 1}^M
p(\tilde{y}^{(m)} \mid \tilde{x}, \alpha, \beta, \sigma).
$$
But instead of $p(\tilde{y}^{(m)} \mid \tilde{x}, \alpha, \beta,
\sigma),$ Stan is providing $\log p(\tilde{y}^{(m)} \mid \tilde{x}, \alpha, \beta,
\sigma).$  The log density needs to be exponentiated to plug into the
above, yielding the numerically friendly log-sum-exp formulation,
$$
\log p(\tilde{y} \mid \tilde{x}, x, y)
\approx
\log \frac{1}{M} \sum_{m = 1}^M
\exp(\log(p(\tilde{y}^{(m)} \mid \tilde{x}, \alpha, \beta, \sigma))).
$$
That is, given the posterior draws `log_p[1], ..., log_p[M]` given by
Stan's MCMC algorithms, the function `log_sum_exp(log_p)` returns the
required value $\log p(\tilde{y} \mid \tilde{x}, x, y)$ with
high arithmetic precision.  It can be implemented outside of Stan as
```
log_sum_exp(a) = max(a) + log(sum(exp(a - max(a))))
```
where the `exp` function is assumed to be vectorized, as is the
subtraction of `max(a)`.  This form ensures that full precision is
preserved in `max(a)` while not allowing any of the exponentiations to
overflow (because the largest entry in `a - max(a)` is zero).

## Estimation error

There are two ways to compute predictions in a Bayesian setting,
either as the error of an expectation or the expectation of an error.
The first is the traditional approach, where a point estimate is
defined as a posterior mean conditioned on the predictor $\tilde{x}$
and training data $(x, y),$
$$
\hat{\tilde{y}}
= \mathbb{E}[\tilde{y} \mid \tilde{x}, x, y].
$$
The MCMC calculation of $\hat{\tilde{y}}$ is
$$
\hat{\tilde{y}} = \frac{1}{M} \sum_{m=1}^M \tilde{y}^{(m)},
$$
where
$$
\tilde{y}^{(m)} \sim p(\tilde{y} \mid \tilde{x}, x, y).
$$
After the MCMC computation, the value $\hat{\tilde{y}}$ can be compared to
the true $\tilde{y}$ to compute squared error,
$$
\textrm{se}
= \left( \hat{\tilde{y}} - \tilde{y} \right)^2.
$$
There is no way to access $\hat{\tilde{y}}$ within a Stan program---it
is computed by averaging over multiple executions.

The second approach is to use the Bayesian posterior to compute
expected square error.
$$
\textrm{expectedSe}
= \mathbb{E}\left[\left( \tilde{y}^{\textrm{pred}}
                    - \tilde{y} \right)^2
		  \mid \tilde{x}, x, y \right].
$$
Because this is an expectation, the interior term can be carried out
within Stan.  For instance, for the linear regression model, square
error for a given iteration is given by
```
generated quantities {
  double sq_err
    = dot(tilde_y,
          to_vector(normal_rng(alpha + x * x_tilde,
	                       sigma)));
  double mse = sq_err / N_tilde;
  double rmse = sqrt(mse);
}
```

The posterior mean of `sq_err` gives `expect_sq_err`.  Typically, this
is reported as average square error per item, otherwise known as mean
square error, or `mse` in the program.  Because mean square error does
not have the same units as the item being predicted, its square root,
known as root mean square error (`rmse` in the program) is often
reported.  Here, the Stan program will be computing expected root mean
square error, expected mean square error, and expected square error.
The division by `N_tilde` distributes through the summation, but the
square root is non-linear and doesn't.  That means that the square
root of the expected mean square error is not the same as the expected
value root mean square error.

##



## Cross-validation predictive tests

Rather than posterior predictive checks that generate replicate data
conditioned on all of the data, $p(y_n^{\textrm{rep}} \mid y)$, the
cross-validation predictive distribution can be used,
$$
p(y_n^{\textrm{rep}} \mid y_{-n}),
$$
where if $y$ is of size $N$, the notation $y_{-n}$ is all of $y$
except $y_n,$
$$
y_{-n} = y_{1:n - 1, n + 1:N}.
$$
Thus the leave-one-out cross-validation predictive distribtuion
replicates data item $y_n$ based on all of the data oher than $y_n$.

Care must be taken that cross-validation one item at a time makes
sense---sometimes these will have to be grouped because the
$y_n^{\textrm{rep}}$ are not all conditionally independent.

Cross-validation predictive tests can be expensive to run by brute
force.  Rather than full leave-one-out cross-validation, mini-batched
cross-validation can be used.  A traditional approach is to partition
the data into ten groups and generate each batch based on the data
in the other batches.  In the cross-validation literature, these
mini-batches are called "folds" and using ten mini-batches is called
"ten-fold cross-validation."  This can get tricky to implement with
indexing if the data is not evenly divisible into ten groups.

In Stan, $K$-fold cross-validation predictive checks can be
implemented as follows for the simple linear regression model.

```
function {
  int fold_size(int K, int k) {
    return k < K ? K / k
                 : K / k + K % k;  // last fold gets remainder
  }
  int start_fold(int K, k) {
    return (k - 1) * fold_size(K, k) + 1;
  }
  int end_fold(int K, k) {
    return k < K ? start_fold(K, k) + fold_size(K, k)
                 : K;
  }
  vector test_fold(vector x, int K, int k)
    return x[start_fold(K, k), end_fold(K, k)];
  }
  vector train_folds(vector x, int K, int k) {
    return append_row(x[1:start_fold(K, k) - 1],
                      x[end_fold(K, k), ]);
  }
}
data {
  int<lower = 1> K;
  int<lower = 1, upper = K> k;
  int<lower = 1> N;
  vector[N] x;
  vector[N] y;
}
transformed data {
  int<lower = 1> N_k = N / K;  // rounds down
}
model {
  train_folds(y, K, k) ~ normal(alpha + beta * train_folds(x, K, k), sigma);
}
generated quantities {
  vector[fold_size(K, k)] y_rep
    = normal_rng(alpha + beta * test_fold(x,  K, k), sigma);
}
```

In the limit when `k = 1`, the result is leave-one-out
cross-validation predictive inference.

This is obviously going to be much more complicated to run from the
outside with ten model fits whose `y_rep` values need to be
reassembled to generate a predictive test set.

## Junk

The Monte Carlo estimate for this posterior predictive density for
a given $\tilde{y}$ is
$$
p(\tilde{y} \mid y) \approx \frac{1}{M} \sum_{m=1}^M p(\tilde{y} \mid \theta^{(m)}),
$$
where each $\theta^{(m)} \sim p(\theta \mid y)$ is distributed
according to the posterior.
