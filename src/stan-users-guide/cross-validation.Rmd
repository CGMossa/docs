# Held-Out Evaluation and Cross-Validation

Held-out evaluation involves splitting a data set into two parts, a
training data set and a testing data set.  The training set is used to
estimate the model and the test set is used for evaluation.  Held-out
evaluation is commonly used to declare winners in predictive modeling
competitions such as those run by Kaggle.

Cross-validation involves partitioning a data set into several folds.
Each fold is then used in turn as test data with the remaining
folds serving as training data.

Held-out evaluation, and hence cross-validation, may involve any kind
of predictive statistics, with common choices being the predictive log
density on test data, squared error of parameter estimates, or accuacy
in a classification task.

This chapter begins with coding posterior predictive inference for new
data, which forms the basis for not only prediction, but also held-out
evaluation and cross-validation.


## Evaluating posterior predictive densities

Given training data $(x, y)$ consisting of parallel sequences of
predictors and observations and test data $(\tilde{x}, \tilde{y})$ of
the same structure, the posterior predictive density is
$$
p(\tilde{y} \mid \tilde{x}, x, y)
=
\int
  p(\tilde{y} \mid \tilde{x}, \theta)
  \cdot p(\theta \mid x, y)
\, \textrm{d}\theta,
$$
where $\theta$ is the vector of model parameters.

This predictive density is the density of the test observations,
conditioned on both the test predictors $\tilde{x}$ and the training
data $(x, y).$  

This integral may be calculated with Monte Carlo methods as usual,
$$
p(\tilde{y} \mid \tilde{x}, x, y)
\approx
\frac{1}{M} \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
$$
where the $\theta^{(m)} \sim p(\theta \mid x, y)$ are draws from the
posterior given training data $(x, y).$ 

For the usual numerical reasons, it will be more stable
to compute densities on the log scale, where the calculation
becomes
\begin{eqnarray*}
\log p(\tilde{y} \mid \tilde{x}, x, y)
& \approx &
\log \frac{1}{M} \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
\\[4pt]
& = & -\log M + \log \sum_{m = 1}^M p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
\\[4pt]
& = & -\log M + \log \sum_{m = 1}^M \exp(\log p(\tilde{y} \mid \tilde{x}, \theta^{(m)}))
\\[4pt]
& = & -\log M + \textrm{log-sum_exp}_{m = 1}^M \log p(\tilde{y} \mid \tilde{x}, \theta^{(m)})
\end{eqnarray*}
where the log sum of exponentials function is defined so as
to make the above equation hold, 
$$
\textrm{log-sum-exp}(\mu_1, \ldots, \mu_M)
= \log (\exp(\mu_1) + \cdot + \exp(\mu_M)).
$$
The log sum of exponentials function can be implemented so as to avoid
underflow and maintain high arithmetic precision as
$$
\textrm{log-sum-exp}(\mu_1, \ldots, \mu_M)
= \textrm{max}(\mu)
+ \log \left(\exp(\mu_1 - \textrm{max}(\mu))
        + \cdots
	+ \exp(\mu_M - \textrm{max}(\mu))\right).
$$
Pulling the maximum out preserves all of its precision, and by
subtracting the maximum the exponentiations are guaranteed not to
overflow;  if they underflow that is OK because the precision is
maintained through the maximum.

### Stan program

To evaluate the log predictive density of a model, it suffices to
implement the log predictive density in the generated quantities
block.  The log sum of exponentials calculation must be done on the
outside using the posterior draws of $\log p(\tilde{y} \mid \tilde{x},
\theta^{(m)}).$

Here is the code for evaluating the posterior predictive density of a
simple linear regression given training data $(x, y)$ and test
data $(\tilde{x}, \tilde{y}).$
```
data {
  int<lower = 0> N;
  vector[N] y;
  vector[N] x;
  int<lower = 0> N_tilde;
  vector[N_tilde] x_tilde;
  vector[N_tilde] y_tilde;
}
parameters {
  real alpha;
  real beta;
  real<lower = 0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
generated quantities {
  real log_p = normal_lpdf(y_tilde | alpha + beta * x_tilde, sigma);
}
```
Only the training data `x` and `y` are used in the model block.  The
test data `y_tilde` and test predictors `x_tilde` are only used in the
generated quantities block.  Thus the program is not cheating by using
the test data during training.  Although this model does not do so,
it would be fair to use `x_tilde` in the model block---only the
test observations `y_tilde` are unknown before they are predicted.

After running Stan, the sequende `log_p[1:M]` will be available and
all that is needed is to calculate the log posterior predictive
density is `log_sum_exp(log_p[1], ..., log_p[M]) - log(M)`.


## Estimation error

Consider again the linear regression from the previous section.
Given parameters $\alpha$ and $\beta$, a prediction for training data
can be formulated as a posterior mean,
$$
\hat{\tilde{y}}_n
= \alpha + \beta \cdot \tilde{x}_n.
$$


the estimated value of the held
out data
$\tilde{y}_n$ 


each test item $n$, it is possible to predi

An alternative evaluatto evaluating the log density on test data is
estimating a result and then measuring square error on test data.




Two Bayesian estimators are in wide use, the posterior mean and the
posterior median.  The posterior mean minimizes expected squared error
on new data, whereas the posterior median minimizes expected absolute
error.  These are both reported in Stan interfaces or calculated
directly from the posterior draws.

For example, considering the linear regression from the last section,
the parameters being estimated are 


The first is the traditional approach, where a point estimate is
defined as a posterior mean conditioned on the predictor $\tilde{x}$
and training data $(x, y),$
$$
\hat{\tilde{y}}
= \mathbb{E}[\tilde{y} \mid \tilde{x}, x, y].
$$
The MCMC calculation of $\hat{\tilde{y}}$ is
$$
\hat{\tilde{y}} = \frac{1}{M} \sum_{m=1}^M \tilde{y}^{(m)},
$$
where
$$
\tilde{y}^{(m)} \sim p(\tilde{y} \mid \tilde{x}, x, y).
$$
After the MCMC computation, the value $\hat{\tilde{y}}$ can be compared to
the true $\tilde{y}$ to compute squared error,
$$
\textrm{se}
= \left( \hat{\tilde{y}} - \tilde{y} \right)^2.
$$
There is no way to access $\hat{\tilde{y}}$ within a Stan program---it
is computed by averaging over multiple executions.

The second approach is to use the Bayesian posterior to compute
expected square error.
$$
\textrm{expectedSe}
= \mathbb{E}\left[\left( \tilde{y}^{\textrm{pred}}
                    - \tilde{y} \right)^2
		  \mid \tilde{x}, x, y \right].
$$
Because this is an expectation, the interior term can be carried out
within Stan.  For instance, for the linear regression model, square
error for a given iteration is given by
```
generated quantities {
  double sq_err
    = dot(tilde_y,
          to_vector(normal_rng(alpha + x * x_tilde,
	                       sigma)));
  double mse = sq_err / N_tilde;
  double rmse = sqrt(mse);
}
```

The posterior mean of `sq_err` gives `expect_sq_err`.  Typically, this
is reported as average square error per item, otherwise known as mean
square error, or `mse` in the program.  Because mean square error does
not have the same units as the item being predicted, its square root,
known as root mean square error (`rmse` in the program) is often
reported.  Here, the Stan program will be computing expected root mean
square error, expected mean square error, and expected square error.
The division by `N_tilde` distributes through the summation, but the
square root is non-linear and doesn't.  That means that the square
root of the expected mean square error is not the same as the expected
value root mean square error.

##



## Cross-validation predictive tests

Rather than posterior predictive checks that generate replicate data
conditioned on all of the data, $p(y_n^{\textrm{rep}} \mid y)$, the
cross-validation predictive distribution can be used,
$$
p(y_n^{\textrm{rep}} \mid y_{-n}),
$$
where if $y$ is of size $N$, the notation $y_{-n}$ is all of $y$
except $y_n,$
$$
y_{-n} = y_{1:n - 1, n + 1:N}.
$$
Thus the leave-one-out cross-validation predictive distribtuion
replicates data item $y_n$ based on all of the data oher than $y_n$.

Care must be taken that cross-validation one item at a time makes
sense---sometimes these will have to be grouped because the
$y_n^{\textrm{rep}}$ are not all conditionally independent.

Cross-validation predictive tests can be expensive to run by brute
force.  Rather than full leave-one-out cross-validation, mini-batched
cross-validation can be used.  A traditional approach is to partition
the data into ten groups and generate each batch based on the data
in the other batches.  In the cross-validation literature, these
mini-batches are called "folds" and using ten mini-batches is called
"ten-fold cross-validation."  This can get tricky to implement with
indexing if the data is not evenly divisible into ten groups.

In Stan, $K$-fold cross-validation predictive checks can be
implemented as follows for the simple linear regression model.

```
function {
  int fold_size(int K, int k) {
    return k < K ? K / k
                 : K / k + K % k;  // last fold gets remainder
  }
  int start_fold(int K, k) {
    return (k - 1) * fold_size(K, k) + 1;
  }
  int end_fold(int K, k) {
    return k < K ? start_fold(K, k) + fold_size(K, k)
                 : K;
  }
  vector test_fold(vector x, int K, int k)
    return x[start_fold(K, k), end_fold(K, k)];
  }
  vector train_folds(vector x, int K, int k) {
    return append_row(x[1:start_fold(K, k) - 1],
                      x[end_fold(K, k), ]);
  }
}
data {
  int<lower = 1> K;
  int<lower = 1, upper = K> k;
  int<lower = 1> N;
  vector[N] x;
  vector[N] y;
}
transformed data {
  int<lower = 1> N_k = N / K;  // rounds down
}
model {
  train_folds(y, K, k) ~ normal(alpha + beta * train_folds(x, K, k), sigma);
}
generated quantities {
  vector[fold_size(K, k)] y_rep
    = normal_rng(alpha + beta * test_fold(x,  K, k), sigma);
}
```

In the limit when `k = 1`, the result is leave-one-out
cross-validation predictive inference.

This is obviously going to be much more complicated to run from the
outside with ten model fits whose `y_rep` values need to be
reassembled to generate a predictive test set.

## Junk

The Monte Carlo estimate for this posterior predictive density for
a given $\tilde{y}$ is
$$
p(\tilde{y} \mid y) \approx \frac{1}{M} \sum_{m=1}^M p(\tilde{y} \mid \theta^{(m)}),
$$
where each $\theta^{(m)} \sim p(\theta \mid y)$ is distributed
according to the posterior.
