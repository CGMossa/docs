# Posterior and Prior Predictive Checks {#ppcs.chapter}

Posterior predictive checks are a way of measuring whether a model
does a good job of capturing relevant aspects of the data, such as
means, standard deviations, and quantiles.  Posterior predictive
checking works by simulating new replicated data sets based on the
fitted model parameters and then comparing statistics applied to the
replicated data set with the same statistic applied to the original
data set.

## Simulating from the posterior predictive distribution

The posterior predictive distribution is the distribution over new
observations given previous observations.  It's predictive in the
sense that it's predicting behavior on new data that is not part of
the training set.  It's posterior in that everything is conditioned on
observed data $y$.

The posterior predictive distribution for replications
$y^{\textrm{rep}}$ of the original data set $y$ given model parameters
$\theta$ is defined by
$$
p(y^{\textrm{rep}} \mid y)
= \int p(y^{\textrm{rep}} \mid \theta)
       \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$

As with other posterior predictive quantities, generating a replicated
data set $y^{\textrm{rep}}$ from the posterior predictive distribution is
straightforward using the generated quantities block.  Consider a simple regression
model with parameters $\theta = (\alpha, \beta, \sigma).$
```
data {
  int<lower = 0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower = 0> sigma;
}
model {
  alpha ~ normal(0, 2);
  beta ~ normal(0, 1);
  sigma ~ normal(0, 1);
  y ~ normal(alpha + beta * x, sigma);
}
```
To generate a replicated data set `x_rep` for this simple model, the
following generated quantities block suffices.
```
generated quantities {
  vector[N] y_rep = normal_rng(alpha + beta * x, sigma);
}
```
The replicated data variable is declared to be the same shape and size
as the original data `y`.  The vectorized form of the normal random
number generator is used with the original predictors `x` and the
model parameters `alpha, beta`, and `sigma.`

## Plotting multiples

A standard posterior predictive check would plot a histogram of each
replicated data set along with the original data set and compare them
by eye.  For this purpose, only a few replications are needed.  These
should be taken by thinning a larger set of replications down to the
size needed to ensure rough independence of the replications.

Here's a complete example where the model is a simple Poisson with
improper prior.

```
data {
  int<lower = 0> N;
  int<lower = 0> y[N];
}
transformed data {
  real<lower = 0> mean_y = mean(to_vector(y));
  real<lower = 0> sd_y = sd(to_vector(y));
}
parameters {
  real<lower = 0> lambda;
}
model {
  y ~ poisson(lambda);
}
generated quantities {
  int<lower = 0> y_rep[N] = poisson_rng(rep_array(lambda, N));
  real<lower = 0> mean_y_rep = mean(to_vector(y_rep));
  real<lower = 0> sd_y_rep = sd(to_vector(y_rep));
  int<lower = 0, upper = 1> mean_gte = (mean_y_rep >= mean_y);
  int<lower = 0, upper = 1> sd_gte = (sd_y_rep >= sd_y);
}
```
The generated quantities block creates a variable `y_rep` for the
replicated data, variables `mean_y_rep` and `sd_y_rep` for the
statistics of the replicated data, and indicator variables
`mean_gte` and `sd_gte` for whether the replicated statistic
is greater than or equal to the statistic applied to the original data.

Now consider generating data $y \sim \textrm{Poisson}(5)$.  The
resulting small multiples plot shows the original data plotted in the
upper left and eight different posterior replications plotted in the
remaining boxes.


```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Posterior predictive checks for Poisson data generating process and Poisson model."}
knitr::include_graphics("./img/ppc-pois-pois.jpg", auto_pdf = TRUE)
```

With a Poisson data-generating process and Poisson model, the
posterior replications look similar to the original data.  If it were
easy to pick the original data out of the lineup, there would be a
problem.

Now consider generating over-dispersed data $y \sim \textrm{negative-binomial2}(5, 1).$  This has the same mean as
$\textrm{Poisson}(5)$, namely $5$, but a standard deviation of
$\sqrt{5 + 5^2 /1} \approx 5.5.$ There is no way to fit this data with
the Poisson model, because a variable distributed as
$\textrm{Poisson}(\lambda)$ has mean $\lambda$ and standard deviation
$\sqrt{\lambda},$ which is $\sqrt{5}$ for $\textrm{Poisson}(5).$
Here's the resulting small multiples plot, again with original data in
the upper left.

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Posterior predictive checks for negative binomial data generating process and Poisson model."}
knitr::include_graphics("./img/ppc-nb-pois.jpg", auto_pdf = TRUE)
```

This time, the original data stands out in stark contrast to the
replicated data sets, all of which have clearly lower variance than
the original data.  That is, the model's not appropriately capturing
the variance of the data.


## Posterior p-values

If a model captures the data well, summary statistics such as
sample mean and standard deviation, should have similar values in
the original and replicated data sets.  This can be tested
by means of a p-value, which is technically just the probability the
test statistic $s(\cdot)$ in a replicated data set exceeds that in
the original data,
$$
\textrm{Pr}\left[ s(y^{\textrm{rep}}) \geq s(y) \mid y \right]
=
\int
\textrm{I}\left[ s(y^{\textrm{rep}}) \geq s(y) \mid y \right]
\cdot p\left( y^{\textrm{rep}} \mid y \right)
\, \textrm{d}{y^{\textrm{rep}}}.
$$
Values of posterior p-values very close to 0 or very close to 1 are
cause for concern that the model is not fitting the data well.

To calculate event probabilities in Stan, it suffices to define
indicator variables that take on value 1 if the event occurs and
0 if it does not.  The posterior mean is then the event probability.
For efficiency, indicator variables are defined in the
generated quantities block.
```
generated quantities {
  int<lower = 0, upper = 1> mean_gt;
  int<lower = 0, upper = 1> sd_gt;
  {
    vector[N] y_rep = normal_rng(alpha + beta * x, sigma);
    mean_gt = mean(y_rep) > mean(y);
    sd_gt = sd(y_rep) > sd(y);
  }
}
```
The indicator variable `mean_gt` will have value 1 if the mean of the
simulated data `y_rep` is greater than or equal to the mean of he
original data `y`.  This program saves output space by using a local
variable for `x_rep`.  The actual variables are not needed for the
posterior predictive checks.  The statistics `mean(x_rep)` and
`sd(x_rep)` could also be saved---their equivalent is used in the
following plots.

For the example in the previous section above where over-dispersed
data generated by a negative binomial distribution was fit with a
simple Poisson model, the following plot illustrates the posterior
p-value calculation for the mean statistic.

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Scatterplot of means of replicated data sets; vertical red line at mean of original data."}
knitr::include_graphics("./img/ppc-pvalue-nb-pois-mean.jpg", auto_pdf = TRUE)
```

The p-value for the mean is just the percentage of replicated data
sets whose statistic is greater than or equal that of the original
data.  Using a Poisson model for negative binomial data still fits the
mean well, with a posterior $p$-value of 0.49.  In Stan terms, it is
extracted as the posterior mean of the indicator variable `mean_gt`.

The standard deviation statistic tells a different story.

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Scatterplot of standard deviations of replicated data sets; vertical red line at standard deviation of original data."}
knitr::include_graphics("./img/ppc-pvalue-nb-pois-sd.jpg", auto_pdf = TRUE)
```

Here, the original data has much higher standard deviation than any of
the replicated data sets.  The resulting $p$-value estimated by Stan
after a large number of iterations is exactly 0.  In other words,
there were no posterior draws in which the replicated data set had a
standard deviation greater than or equal to that of the original data
set.  Clearly, the model is not capturing the dispersion of the
original data.  The point of this exercise isn't just to figure out
that there's a problem with a model, but to isolate where it is.
Seeing that the data is over-dispersed compared to the Poisson model
would be reason to fit a more general model like the negative binomial
or a latent varying effects (aka random effects) model that can
account for the over-dispersion.

### Which statistics to test?

Any statistic may be used for the data, but these can be guided by the
quantities of interest in the model itself.  A popular choice in
addition to mean and standard deviation are quantiles, such as the
median, 5% or 95% quantiles, or even the maximum or minimum value to
test extremes.

## Prior predictive checks

Prior predictive checks generate data according to the prior in order
to asses whether a prior is appropriate [@GabryEtAl:2019].  A
posterior predictive check generates replicated data according to the
posterior predictive distribution.  In contrast, the prior predictive
check generates data according to the prior predictive distribution,
$$
y^{\textrm{sim}} \sim p(y).
$$
The prior predictive distribution is just like the posterior
predictive distribution with no observed data, so that a prior
predictive check is nothing more than the limiting case of a posterior
predictive check with no data..

This is easy to carry out mechanically by simulating parameters
$$
\theta^{\textrm{sim}} \sim p(\theta)
$$
according to the priors, then simulating data
$$
y^{\textrm{sim}} \sim p(y \mid \theta^{\textrm{sim}}
$$
according to the sampling distribution given the simulated
parameters.  The result is a simulation from the joint
distribution,
$$
y^{\textrm{sim}}, \theta^{\textrm{sim}} \sim p(y, \theta)
$$
and thus
$$
y^{\textrm{sim}} \sim p(y)
$$
is a simulation from the prior predictive distribution.

### Coding prior predictive checks in Stan

A prior predictive check is coded just like a posterior predictive
check.  If a posterior predictive check has already been coded and
it's possible to set the data to be empty, then no additional coding
is necessary.  The disadvantage to coding prior predictive checks as
posterior predictive checks with no data is that Markov chain Monte
Carlo will be used to sample the parameters, which is less efficient
than taking independent draws using random number generation.

Prior predictive checks can be coded entirely within the generated
quantities block using random number generation.  The resulting draws
will be independent.  Predictors must be read in from the actual data
set---they do not have a generative model from which to be simulated.
For a Poisson regression, prior predictive sampling can be
encoded as the following complete Stan program.
```
data {
  int<loer = 0> N;
  vector[N] x;
}
generated quantities {
  real alpha = normal_rng(0, 1);
  real beta = normal_rng(0, 1);
  real y_sim[n] = poisson_log_rng(alpha + beta * x);
}
```
Running this program using Stan's fixed-parameter sampler
yields draws from the prior.  These may be plotted to
consider their appropriateness.

Here's a complete example.  Suppose we have a model for a football
(aka soccer) league where there are $J$ teams.  Each team has a
scoring rate $\lambda_j$ and in each game will be assumed to
score $\textrm{poisson}(\lambda_j)$ points.  Yes, this model completely
ignores defense.  Suppose the modeler does not want to "put their
thumb on the scale" and would rather "let the data speak for
themselves" and so uses a prior with very wide tails, because it seems
uninformative, such as the widely deployed
$$
\lambda_j \sim \textrm{gamma}(\epsilon_1, \epsilon_2).
$$
The BUGS book recommends setting $\epsilon = (0.5, 0.00001)$, which
corresponds to a Jeffreys prior for a Poisson rate parameter prior
[@LunnEtAl:2012, page 85].

Suppose the league plays a round-robin tournament wherein every team
plays every other team.  The following Stan model generates random team
abilities and the results of such a round-robin tournament, which may
be used to perform prior predictive checks.
```
data {
  int<lower = 0> J;
  real<lower = 0> epsilon[2];
}
generated quantities {
  real<lower = 0> lambda[J];
  int y[J, J];
  for (j in 1:J) lambda[j] = gamma_rng(epsilon[1], epsilon[2]);
  for (i in 1:J)
    for (j in 1:J)
      y[i, j] = poisson_rng(lambda[i]) - poisson_rng(lambda[j]);
}
```
In this simulation, teams play each other twice and play themselves
once.  This could be made more realistic by controlling the
combinatorics to only generate a single result for each pair of teams,
of which there are $\binom{J}{2} = \frac{J \cdot (J - 1)}{2}.$


Using the $\textrm{gamma}(0.5, 0.00001)$ reference prior on team
abilities, the following are the first 20 simulated point differences
for the match between the first two teams, $y^{(1:20)}_{1, 2}$.
```
2597 -26000   5725  22496   1270   1072   4502  -2809   -302   4987
7513   7527  -3268 -12374   3828   -158 -29889   2986  -1392     66
```
That's some pretty highly scoring football games being simulated; all
but one has a score differential greater than 100!  In other words, this
$\textrm{gamma}(0.5, 0.00001)$ prior is putting around 95% of its
weight on score differentials above 100.  Given that two teams
combined rarely score 10 points, this prior is way out of line with
prior knowledge about football matches; it is not only consistent with
outcomes that have never occurred in the history of the sport, it puts
most of the prior probability mass there.

The posterior predictive distribution can be strongly affected by the
prior when there is not much observed data and substantial prior mass
is concentrated around infeasible values [@Gelman:2006].

Just as with posterior predictive distributions, any statistics of the
generated data may be evaluated.  Here, the focus was on score
difference between a single pair of teams, but it could've been on
maximums, minimums, averages, variances, etc.

In this textbook example, the prior is univariate and directly related
to the expected number of points scored, and could thus be directly
inspected for consistency with prior knowledge about scoring rates in
football.  There will not be the same kind of direct connection when
the prior and sampling distributions are multivariate.  In these more
challenging situations, prior predictive checks are an easy way to get
a handle on the implications of a prior in terms of what it says the
data is going to look like.

Prior predictive checks can also be compared with the data, but one
should not expect them to be calibrated in the same way as posterior
predictive checks.  That would require guessing the posterior and
encoding it in the prior.  Neither should the prior be so wide
that it pulls probability mass away from feasible values.