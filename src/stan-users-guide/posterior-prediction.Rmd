# Posterior Predictive Sampling

The goal of inference is often posterior prediction, that is
evaluating $p(\tilde{y} \mid y),$ where $y$ is observed data and
$\tilde{y}$ is yet to be observed data.  Often there are unmodeled
predictors $x$ and $\tilde{x}$ for the observed and unobserved data,
in which case the posterior predictive density is $p(\tilde{y} \mid
\tilde{x}, x, y).$ This chapter explains how to sample from the
posterior predictive distribution in Stan, beginning with a simple
case and then continuing to a regression with predictors.

## Posterior predictive distribution

Given a full Bayesian model $p(y, \theta)$, the posterior predictive
density for new data $\tilde{y}$ given observed data $y$ is
$$
p(\tilde{y} \mid y)
= \int
p(\tilde{y} \mid \theta) \cdot p(\theta \mid y)
\, \textrm{d}\theta.
$$
The product under the integral reduces to the joint posterior density
$p(\tilde{y}, \theta \mid y),$ so that the integral simply
marginalizes out the parameters $\theta,$ leaving a predictive
density $p(\tilde{y} \mid y)$ of future observations given past
observations.

## Sampling from the posterior predictive distribution

Given draws from the posterior $\theta^{(m)} \sim p(\theta \mid y),$
draws from the posterior predictive $\tilde{y}^{(m)} \sim p(\tilde{y}
\mid y)$ can be generated by randomly generating from the sampling
distribution with the parameter draw plugged in,
$$
\tilde{y}^{(m)} \sim p(y \mid \theta^{(m)}).
$$

Randomly drawing $\tilde{y}$ from the sampling distribution is
critical because there are two forms of uncertainty in posterior
predictive quantities, sampling uncertainty and estimation
uncertainty.  Estimation uncertainty arises because $\theta$ is being
estimated based only on $y$; sampling uncertainty arises because even
a known value of $\theta$ leads to a sampling distribution
$p(\tilde{y} \mid \theta).$ Both forms of uncertainty show up in the
factored form of the posterior predictive distribution,
$$
p(\tilde{y} \mid y)
=
\int
\underbrace{p(\tilde{y} \mid \theta)}_{\begin{array}{l}
                                         \textrm{sampling}
                                         \\[-2pt] \textrm{uncertainty}
                                       \end{array}}
\cdot \underbrace{p(\theta \mid y)}_{\begin{array}{l}
                                         \textrm{estimation}
                                         \\[-2pt] \textrm{uncertainty}
                                       \end{array}}
\, \textrm{d}\theta.
$$


## Posterior predictive simulation in Stan

Posterior predictive quantities can be coded in Stan
using the generated quantities block.

### Simple Poisson model

For example, consider a simple Poisson model for count data with a
rate parameter $\lambda > 0$ following a gamma-distributed prior,
$$
\lambda \sim \textrm{gamma}(1, 1).
$$
The likelihood for $N$ observations $y_1, \ldots, y_N$ is Poisson,
$$
y_n \sim \textrm{poisson}(\lambda).
$$

### Stan code

The following Stan program includes a variable from $\tilde{y}$
defined by random number generation in the generated quantities
block.
```
data {
  int<lower = 0> N;
  int<lower = 0> y[N];
}
parameters {
  real<lower = 0> lambda;
}
model {
  lambda ~ gamma(1, 1);
  y ~ poisson(lambda);
}
generated quantities {
  int<lower = 0> y_tilde = poisson_rng(lambda);
}
```
The random draw from the sampling distribution for $\tilde{y}$ is
coded using Stan's Poisson random number generator in the generated
quantities block.  This accounts for the sampling component of the
uncertainty; Stan's posterior sampler will account for the estimation
uncertainty, generating a new $\tilde{y}^{(m)} \sim p(y \mid
\lambda^{(m)})$ for each posterior draw $\lambda^{(m)} \sim p(\theta
\mid y).$

The posterior draws $\tilde{y}^{(m)}$ may be used to estimate the
expected value of $\tilde{y}$ or any of its quantiles or posterior
intervals, as well as event probabilities involving $\tilde{y}$.
In general, $\mathbb{E}[f(\tilde{y}, \theta) \mid y]$ may be evaluated
as
$$
\mathbb{E}[f(\tilde{y}, \theta) \mid y]
\approx \frac{1}{M} \sum_{m=1}^M f(\tilde{y}^{(m)}, \theta^{(m)}),
$$
which is just the posterior mean of $f(\tilde{y}, \theta).$  This
quantity is computed by Stan if the function is defined in the
generated quantities block.

### Analytic posterior and posterior predictive

The gamma distribution is the conjugate prior distribution for the
Poisson distribution, so the posterior density $p(\lambda \mid y)$
will also follow a gamma distribution.

Because the posterior follows a gamma distribution and the sampling
distribution is Poisson, the posterior predictive $p(\tilde{y} \mid
y)$ will follow a negative binomial distribution.  The negative
binomial may be defined as a compound gamma-Poisson.  Rather than
marginalizing out the rate analytically as in the negative binomial,
it is sampled from the posterior and then used to generate a draw of
$\tilde{y}.$

## Posterior prediction for regressions

### Posterior predictive distributions for regressions

Consider a regression with a single predictor $x_n$ for the
training data $y_n$ and $\tilde{x}_n$ for the test data $\tilde{y}_n.$
Without considering the parametric form of any of the distributions,
the posterior predictive distribution for a general regression in
\begin{align}
p(\tilde{y} \mid \tilde{x}, y, x)
& = & \int p(\tilde{y} \mid x, \theta) \cdot p(\theta \mid y, x) \cdot
\textrm{d}\theta
\\[4pt]
& \approx & \frac{1}{M} \sum_{m=1}^M p(\tilde{y} \mid x, \theta^{(m)}),
\end{align}
where $\theta^{(m)} \sim p(y \mid x, \theta^{(m)}).$

### Stan program

The following example is of a Poisson regression with a single
predictor per observed value and per unobserved value.  These
predictors are all coded as data, as are their sizes.
Only the observed $y$ values are coded as data.
The predictive quantities $\tilde{y}$ appear in the generated
quantities block, where they are generated by random number
generation.
```
data {
  int<lower = 0> N;
  int<lower = 0> y[N];
  vector[N] x;
  int<lower = 0> N_tilde;
  vector[N_tilde] x_tilde;
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ poisson_log(alpha + beta * x);
  { alpha, beta }  ~ normal(0, 1);
}
generated quantities {
  int<lower = 0> y_tilde[N_tilde]
    = poisson_log_rng(alpha + beta * x_tilde);
}
```
The Poisson distributions in both the model and generated quantities
block are coded using the function taking the log rate as a
parameter.  The regression coefficients, an intercept `alpha` and
slope `beta`, are given standard normal priors.

In the model block, the log rate for the Poisson is a linear function of
the training data $x$, whereas in the generated quantities block it is
a function of the test data $\tilde{x}$.  Because the generated
quantities block does not affect the posterior draws, the model fits
$\alpha$ and $\beta$ using only the training data, reserving
$\tilde{x}$ to generate $\tilde{y}.$

The result from running Stan is a predictive sample $\tilde{y}^{(1)},
\ldots \tilde{y}^{(M)}$ where each $\tilde{y}^{(m)} \sim p(\tilde{y}
\mid \tilde{x}, x, y).$

The mean of the posterior predictive distribution is the expected value
\begin{align}
\mathbb{E}[\tilde{y} \mid \tilde{x}, x, y]
& = &
\int
\tilde{y}
\cdot p(\tilde{y} \mid \tilde{x}, \theta)
\cdot p(\theta \mid x, y)
\, \textrm{d}\theta
\\[4pt]
& \approx & \frac{1}{M} \sum_{m = 1}^M \tilde{y}^{(m)},
\end{align}
where the $\tilde{y}^{(m)} \sim p(\tilde{y} \mid \tilde{x}, x, y)$ are
drawn from the posterior predictive distribution.  Thus the posterior
mean of `y_tilde[n]` after running Stan is the expected value of
$\tilde{y}_n$ conditioned on the the training data $x, y$ and
predictor $\tilde{x}_n.$ This is the Bayesian estimate for $\tilde{y}$
with minimum expected squared error.  The posterior draws can also be
used to estimate quantiles for the median and any posterior intervals
of interest for $\tilde{y}$, as well as covariance of the $\tilde{y_n}.$
The posterior draws $\tilde{y}^{(m)}$ may also be used to estimate
predictive event probabilities, such as $\mbox{Pr}[\prod_{n =
1}^{\tilde{N}}(\tilde{y_n}) > 1],$ as expectations of indicator
functions.

All of this can be carried out by running Stan only a single time to
draw a single sample of $M$ draws,
$$
\tilde{y}^{(1)}, \ldots, \tilde{y}^{(M)} \sim p(\tilde{y} \mid
\tilde{x}, x, y).
$$
It's only when moving to cross-validation where multiple runs are
required.
