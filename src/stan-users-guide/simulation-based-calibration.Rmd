# Simulation-Based Calibration

A Bayesian posterior is calibrated if the posterior intervals have
appropriate coverage.  For example, 80% intervals are expected to
contain the true parameter 80% of the time.  If data is generated
according to a model, Bayesian posterior inference with respect to
that model is calibrated by construction.  Simulation-based
calibration (SBC) exploits this property of Bayesian inference to
asses the soundness of a posterior sampler.  Roughly, the way it works
is by simulating parameters according to the prior, then simulating
data conditioned on the simulated parameters, then testing posterior
calibration of the inference algorithm when given the simulated data.
This chapter follows [@TaltsEtAl:2018], which improves on the original
approach taken in [@CookGelmanRubin:2006].

## Bayesian inference is calibrated by construction

Suppose a Bayesian model is given in the form of a prior density
$p(\theta)$ and sampling density $p(y \mid \theta).$  First, simulate
parameters from the prior,
$$
\theta^{\textrm{sim}} \sim p(\theta).
$$
Second, simulate data given the paraemters,
$$
y^{\textrm{sim}} \sim p(y \mid \theta^{\textrm{sim}}).
$$
By the definition of conditional densities, the simulated parameters
constitute an independent draw from the model's joint distribution,
$$
(y^{\textrm{sim}}, \theta^{\textrm{sim}}) \sim p(y, \theta).
$$
From Bayes's rule, it follows that $p(\theta \mid y) \propto p(y,
\theta)$ for any $y$.  That means that the simulated parameters constitute a
draw from the posterior for the simulated data,
$$
\theta^{\textrm{sim}} \sim p(\theta \mid y^{\textrm{sim}}).
$$
Because $\theta^{\textrm{sim}}$ is distributed as a draw from the
posterior, given a sequence of draws from the posterior
$$
\theta^{(1)}, \ldots \theta^{(M)}
\sim p(\theta \mid y^{\textrm{sim}}),
$$
the rank statistics of $\theta^{\sim}$ with respect to $\theta^{(1)},
\ldots \theta^{(M)}$ should be uniform.  This is essentially the
definition of calibration, because it follows that posterior intervals
will have appropriate coverage [@Dawid:1982; @GneitingEtAl:2007].  If
the rank of $\theta^{\textrm{sim}}$ is uniform among the draws
$\theta^{(1)}, \ldots, \theta^{(M)},$ then for any 90% interval
selected, the probability the true value $\theta^{\textrm{sim}}$ falls
in it will also be 90%.  The same goes for any other posterior interval.


## Simulation-based calibration

Suppose the model to test is $p(y, \theta) = p(y \mid \theta) \cdot p(\theta),$
with data $y$ and a $K$-vector of parameters $\theta.$
Simulation-based calibration works by generating $N$ simulated
parameter and data pairs,
$$
(y^{\textrm{sim}(n)}, \theta^{\textrm{sim}(n)}) \sim p(y, \theta).
$$
For each simulated data set $y^{\textrm{sim}(n)}$, use the algorithm
to be tested to generate $M$ posterior draws,
$$
\theta^{(n, 1)}, \ldots, \theta^{(n, M)}
\sim p(\theta \mid y^{\textrm{sim}(n)}).
$$
For a simulation $n$ and parameter $k$, the rank of the simulated
parameter among the posterior draws is
\begin{eqnarray*}
r_{n, k}
& = &
\textrm{rank}(\theta_k^{\textrm{sim}(n)},
              (\theta^{(n, 1)}, \ldots, \theta^{(n,M)}))
\\[4pt]
& = &
\sum_{m = 1}^M
  \textrm{I}[\theta_k^{(n,m)} < \theta_k^{\textrm{sim}(n)}].
\end{eqnarray*}

If the algorithm generating posterior draws is well calibrated,
the ranks should be uniformly distributed,
$$
r_{n, k} \sim \textrm{discreteUniform}(0, M).
$$
Simulation-based calibration uses this expected behavior to test the
calibration of each parameter of a model on simulated data.
[@TaltsEtAl:2018] suggest plotting binned counts of $r_{1:N,
k}$ for different parameters $k$;  [@CookGelmanRubin:2006]
automate the process with a hypothesis test for uniformity.

## SBC in Stan

Running SBC in Stan will test whether Stan's sampling algorithm can
sample from the posterior associated with data generated according to
the model.  The data simulation and posterior fitting and rank
calculation can all be done with Stan.  Then Stan's posterior sampler
has to be run multiple times with the resulting ranks for each
parameter being assessed for uniformity.

### Example model

For illustration, a very simple model will suffice.  Suppose there are
two parameters $(\mu, \sigma)$ with independent priors,
$$
\mu \sim \textrm{normal}(0, 1),
$$
and
$$
\sigma \sim \textrm{lognormal}(0, 1).
$$
The datat $y = y_1, \ldots, y_N$ is drawn conditionally independently
given the parameters,
$$
y_n \sim \textrm{normal}(\mu, \sigma).
$$
Expressed as densities, the prior density is
$$
p(\mu, \sigma)
= \textrm{normal}(\mu \mid 0, 1)
  \cdot \textrm{lognormal}(\sigma \mid 0, 1),
$$
and the sampling density is
$$
p(y \mid \mu, \sigma)
= \prod_{n=1}^N \textrm{normal}(y_n \mid \mu, \sigma).
$$

Consider the first run of sampling, $n = 1$.  In simulating
simulating from the prior, suppose the following
two parameter values are drawn,
$$
(\mu^{\textrm{sim(1)}}, \sigma^{\textrm{sim(1)}}) = (1.01, 0.23).
$$
Suppose $M = 4$ posterior simulations are drawn, with results
$$
\begin{array}{r|rr}
m & \mu^{(1,m)} & \sigma^{(1,m)}
\\ \hline
1 & 1.07 & 0.33
\\
2 & -0.32 & 0.14
\\
3 & -0.99 & 0.26
\\
4 & 1.51 & 0.31
\end{array}
$$
Then the comparisons on which ranks are based look as follows,
$$
\begin{array}{r|cc}
m & \textrm{I}[\mu^{(1,m)} < \mu^{\textrm{sim}(1)}]
& \textrm{I}[\sigma^{(1,m)} < \sigma^{\textrm{sim}(1)}]
\\ \hline
1 & 0 & 0
\\
2 & 1 & 1
\\
3 & 1 & 0
\\
4 & 0 & 0
\end{array}
$$
The ranks are the column sums, or $r_{1,1} = 2$ and $r_{1,2} = 1$.
Because the simulated parameters are distributed according to the posterior,
these ranks should be distributed uniformly between $0$ and $M$, the
number of posterior draws.

### Stan program

To implement SBC to test Stan, the transformed data block is used to
simulate parameters and data from the model.  The parameters,
transformed parameters, and model block then define the model over the
simulated data.  Finally, the program records an indicator for whether
each parameter is less than the simulated value, which allows the rank
to be calculated by summing the simulated indicators.

```
transformed data {
  real mu_sim = normal_rng(0, 1);   //
  real<lower = 0> sigma_sim = lognormal_rng(0, 1);
  int<lower = 0> N = 10;
  for (n in 1:N)
    y_sim[n] = normal_rng(mu_sim, sigma_sim);
}
parameters {
  real mu;
  real<lower = 0> sigma;
}
model {
  mu ~ normal(0, 1);
  sigma ~ lognormal(0, 1);
  y_sim ~ normal(mu, sigma);
}
generated quantities {
  int<lower = 0, upper = 1> lt_sim[2]
      = { mu < mu_sim, sigma < sigma_sim };
}
```

The model is implemented twice---once as a data generating process
using random number generators in the transformed data block, then
again in the parameters and model block.  This duplication is a
blessing and a curse.  The curse is that it's more work and twice the
chance for errors.  The blessing is that by implementing the model
twice and comparing results, the chance of there being a mistake
in the model is reduced.

### Pseudocode for SBC

The entire SBC process is as follows, where `K` is the number of
parameters, `N` is the total number of simulated data sets and fits,
and `M` is the number of posterior draws per simulated data set.

```
1)   for (n in 1:N)
2)       theta(sim(n)) ~ p(theta)
3)       y(sim(n)) ~ p(y | theta(sim(n)))
4)       for (m in 1:M)
5)           theta(n, m) ~ p(theta | y(sim(n)))
6)       for (k in 1:K)
7)           rank(n, k) = SUM_m I[theta[k](n,m) < theta[k](sim(n))]
8)   for (k in 1:K)
9)     test uniformity of rank(1:N, k)
```

### The importance of thinning

The draws from the posterior are assumed to be roughly independent.
This will almost always require thinning down to the point where the
effective sample size is roughly the same as the number of thinned
draws.  This may require running the code a few times to judge the
number of draws required to get a high enough effective sample size,
an operation that can be put into a loop that doubles the number of
iterations until a large enough effective sample size is found that
the result can be thinned to roughly independent draws, that is, an
effective sample size of greater than $M$ for all parameters $\theta_n.$


## Testing uniformity

A simple, though not very highly powered, $\chi^2$-squared test for
uniformity can be formulated by binning the ranks $0:M$ into $J$
evenly sized bins and testing that the bins all have roughly the
expected number of draws in them.^[Many other tests for uniformity are
possible.  For example, [@CookGelmanRubin:2006] transform the ranks
using the inverse cumulative distribution function for the standard
normal and then perform a test for normality.  [@TaltsEtAl:2018]
recommend visual inspection of the binned plots.]

If there are $N$ total runs and the $J$ bins are of equal size, then
$N / J$ elements is expected to fall into each bin.

In general, if $b_j$ is the number of ranks that fall into bin $j$ and
$e_j$ is the number of ranks expected to fall into bin $j$, the test
statistic is
$$
X^2 = \sum_{j = 1}^J \frac{(b_j - e_j)^2}{e_j}.
$$
Under the null hypothesis of uniformity,
$$
X^2 \sim \textrm{chiSquared}(J - 1),
$$
with the corresponding $p$-value given by the complementary cumulative
distribution function (CCDF) of $\textrm{chiSquared}(J - 1)$ applied
to $X^2$.  Because this test relies on the binomial being
approximately normal, the traditional advice is to make sure the
expected count in each bin is at least five.

### Indexing for simplicity

Because there are $M + 1$ possible ranks, with $J$ bins, it is
easiest to have $M + 1$ be divisible by $J$.  For instance, if
$J = 20$ and $M = 999$, then there are $1000$ possible ranks and an
expected count in each bin of $\frac{M + 1}{J} = 50.$

Distributing the ranks into bins is another fiddly operation
that can be done with integer arithmetic or the floor operation.
$$
\textrm{bin}(r_{n, m}, M, J)
= 1 + \left\lfloor \frac{r_{n, m}}{(M + 1) / J} \right\rfloor.
$$.
For example, with $M = 999$ and $J = 20$, $(M + 1) / J = 50$.
The lowest rank checks out,
$$
\textrm{bin}(0, 999, 20) = 1 + \lfloor 0 / 50 \rfloor = 1 + 0  = 1,
$$
as does the 50th rank,
$$
\textrm{bin}(49, 999, 20) = 1 + \lfloor 49 / 50 \rfloor = 1,
$$
and the 51st is appropriately put in the second bin,
$$
\textrm{bin}(50, 999, 20) = 1 + \lfoor 50 / 50 \rfloor = 1 + 1 = 2.
$$
The highest rank also checks out, with $\textrm{bin}(1000, 999, 20) = 50.$

To summarize, the following pseudocode computes the $b_j$ values for
the $\chi^2$ test or for visualization in a histogram.^[Care must be
taken in visualizing histograms for discrete data because it's easy to
introduce off-by-one errors and artifacts at the edges because of the
way boundaries are computed by default.  That's why so much attention
is being devoted to indexing and binning here.]
```
Inputs: M draws, J bins, N parameters, ranks r[n, m]
b[1:J] = 0
for (m in 1:M)
  ++b[1 + floor(r[n, m] * J / (M + 1))]
```
where the `++b[n]` notation is a common form of syntactic sugar
for `b[n] = b[n] + 1.`


## Examples of simulation-based calibration

This section will show what the results look like when the tests pass
and then when they fail.  The passing test will compare a normal model
and normal data generating process, whereas the second will compare a
normal model with a Student-t data generating process.  The first will
produce calibrated posteriors, the second will not.

### When things go right

Consider the following simple model for a normal distribution with
standard normal and lognormal priors on the location and scale
parameters.
\begin{eqnarray*}
\mu & \sim & \textrm{normal}(0, 1)
\\[4pt]
\sigma & \sim & \textrm{lognormal}(0, 1)
\\[4pt]
y_{1:10} & \sim & \textrm{normal}(\mu, \sigma).
\end{eqnarray*}
The Stan program for evaluating SBC for this model is
```
transformed data {
  real mu_sim = normal_rng(0, 1);
  real<lower = 0> sigma_sim = lognormal_rng(0, 1);

  int<lower = 0> N = 10;
  vector[N] y_sim;
  for (n in 1:N)
    y_sim[n] = student_t_rng(4, mu_sim, sigma_sim);
}
parameters {
  real mu;
  real<lower = 0> sigma;
}
model {
  mu ~ normal(0, 1);
  sigma ~ lognormal(0, 1);

  y_sim ~ normal(mu, sigma);
}
generated quantities {
  int<lower = 0, upper = 1> I_lt_sim[2]
      = { mu < mu_sim, sigma < sigma_sim };
}
```

After running this for enough iterations so that the effective sample
size is larger than $M$, then thinning to $M$ draws (here $M = 999$),
the ranks are computed and binned, and then plotted.

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Simulation based calibration plots for location and scale  of a normal model with standard normal prior on the location  standard lognormal prior on the scale.  Both histograms appear uniform, which is consistent with inference being well calibrated."}
knitr::include_graphics("./img/sbc-normal-normal.png", auto_pdf = TRUE)
```

### When things go wrong

Now consider using a Student-t data generating process with a normal model.
Compare the apparent uniformity of the well specified model with the
ill-specified situation with Student-t generative process and normal
model.
```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Simulation based calibration plots for location and scale of a normal model with standard normal prior on the location standard lognormal prior on the scale with mismatched generative model using a Student-t likelihood with 4 degrees of freedom.  The mean histogram appears uniform, but the scale parameter shows simulated values much smaller than fit values, clearly signaling the lack of calibration."}
knitr::include_graphics("./img/sbc-student-t-normal.png", auto_pdf = TRUE)
```

### When Stan's sampler goes wrong

The example in the previous sections show hard-coded pathological
behavior.  The usual application of SBC is to diagnose problems with a
sampler.

This can happen in Stan with well-specified models if the posterior
geometry is too difficult (usually due to extreme stiffness that
varies). A simple example is the now-classic Eight Schools problem,
the data for which consists of sample means $y_j$ and standard
deviations $\sigma_j$ of differences in test score after the same
intervention in $J = 8$ different schools. [@Rubin:1981] applies a
hierarchical model for a meta-analysis of the results, estimating the
mean intervention effect and a varying effect for each school. With a
standard parameterization and weak priors, this model has very
challenging posterior geometry, as shown in [@TaltsEtAl:2018]; this
section replicates their results.

The meta-analysis model has parameters for a population mean $\mu$ and
standard deviation $\tau > 0$ as well as the effect $\theta_j$ of the
treatment in each school.  The model has weak normal and half-normal
priors for the population-level parameters,
\begin{eqnarray*}
\mu & \sim & \textrm{normal}(0, 5)
\\[4pt]
\tau & \sim & \textrm{normal}_{+}(0, 5)
\end{eqnarray*}
School level effects are modeled as normal given the population
parameters,
$$
\theta_j \sim \textrm{normal}(\mu, \tau).
$$
The data is modeled as in a meta-analysis, given the school effect and
sample standard deviation in the school,
$$
y_j \sim \textrm{normal}(\theta_j, \sigma_j).
$$

This model can be coded in Stan with a data-generating process that
simulates the parameters and then simulates data according to the
parameters.
```
transformed data {
  real mu_sim = normal_rng(0, 5);
  real tau_sim = fabs(normal_rng(0, 5));
  int<lower = 0> J = 8;
  real theta_sim[J] = normal_rng(rep_vector(mu_sim, J), tau_sim);
  real<lower=0> sigma[J] = fabs(normal_rng(rep_vector(0, J), 5));
  real y[J] = normal_rng(theta_sim, sigma);
}
parameters {
  real mu;
  real<lower=0> tau;
  real theta[J];
}
model {
  tau ~ normal(0, 5);
  mu ~ normal(0, 5);
  theta ~ normal(mu, tau);
  y ~ normal(theta, sigma);
}
generated quantities {
  int<lower = 0, upper = 1> mu_lt_sim = mu < mu_sim;
  int<lower = 0, upper = 1> tau_lt_sim = tau < tau_sim;
  int<lower = 0, upper = 1> theta1_lt_sim = theta[1] < theta_sim[1];
}
```

As usual for simulation-based calibration, the transformed data
encodes the data-generating process using random number generators.
Here, the population parameters $\mu$ and $\tau$ are first simulated,
then the school-level effects $\theta$, and then finally the observed
data $\sigma_j$ and $y_j.$  The parameters and model are a direct
encoding of the mathematical presentation using vectorized sampling
statements.  The generated quantities code indicators for parameter
comparisons, saving only $\theta_1$ because the schools are symmetric.

When fitting the model in Stan, multiple warning messages are
provided that the sampler has diverged.  The divergence warnings are
in Stan's sampler precisely to diagnose the sampler's inability
to follow the curvature in the posterior.

SBC also diagnoses the problem. Here's the rank plots for running $N =
200$ simulations with 1000 warmup iterations and $M = 999$ draws per
simulation used to compute the ranks.

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Simulation based calibration plots for the eight-schools model with  centered parameterization in Stan.  The geometry is too difficult for the NUTS sampler to handle, as indicated by the plot for theta[1]."}
knitr::include_graphics("./img/sbc-ctr-8-schools-mu.png", auto_pdf = TRUE)
knitr::include_graphics("./img/sbc-ctr-8-schools-tau.png", auto_pdf = TRUE)
knitr::include_graphics("./img/sbc-ctr-8-schools-theta1.png", auto_pdf = TRUE)
```
Although the population mean and standard deviation $\mu$ and $\tau$
appear well calibrated, $\theta_1$ tells a very different story. The
simulated values are much smaller than the values fit from the data.
This is because Stan's no-U-turn sampler is unable to sample with the
model formulated in the centered parameterization---the posterior
geometry has regions of extremely high curvature as $\tau$ approaches
zero and the $\theta_j$ become highly constrained.  The [chapter on
reparameterization](#change-of-variables.chapter) explains how to
remedy this problem and fit this kind of hierarchical model with Stan.
