<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Stan Reference Manual</title>
  <meta name="description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Stan Reference Manual" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://mc-stan.org/users/documentation" />
  <meta property="og:image" content="http://mc-stan.org/users/documentationimg/logo_tm.png" />
  <meta property="og:description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language." />
  <meta name="github-repo" content="stan-dev/stan" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stan Reference Manual" />
  <meta name="twitter:site" content="@mc-stan" />
  <meta name="twitter:description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language." />
  <meta name="twitter:image" content="http://mc-stan.org/users/documentationimg/logo_tm.png" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="algorithms.html">
<link rel="next" href="analysis-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="../stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan Reference Manual</li>

<li class="divider"></li>
<li><a href="index.html#overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Overview</i></a></li>
<li><a href="language.html#language"><i style="font-size: 110%; color:#990017;">Language</i></a></li>
<li class="chapter" data-level="1" data-path="character-encoding.html"><a href="character-encoding.html"><i class="fa fa-check"></i><b>1</b> Character Encoding</a></li>
<li class="chapter" data-level="2" data-path="includes-section.html"><a href="includes-section.html"><i class="fa fa-check"></i><b>2</b> Includes</a></li>
<li class="chapter" data-level="3" data-path="comments-section.html"><a href="comments-section.html"><i class="fa fa-check"></i><b>3</b> Comments</a></li>
<li class="chapter" data-level="4" data-path="whitespace.html"><a href="whitespace.html"><i class="fa fa-check"></i><b>4</b> Whitespace</a></li>
<li class="chapter" data-level="5" data-path="data-types-chapter.html"><a href="data-types-chapter.html"><i class="fa fa-check"></i><b>5</b> Data Types and Declarations</a></li>
<li class="chapter" data-level="6" data-path="expressions.html"><a href="expressions.html"><i class="fa fa-check"></i><b>6</b> Expressions</a></li>
<li class="chapter" data-level="7" data-path="statements.html"><a href="statements.html"><i class="fa fa-check"></i><b>7</b> Statements</a></li>
<li class="chapter" data-level="8" data-path="blocks-chapter.html"><a href="blocks-chapter.html"><i class="fa fa-check"></i><b>8</b> Program Blocks</a></li>
<li class="chapter" data-level="9" data-path="functions-chapter.html"><a href="functions-chapter.html"><i class="fa fa-check"></i><b>9</b> User-Defined Functions</a></li>
<li class="chapter" data-level="10" data-path="variable-transforms-chapter.html"><a href="variable-transforms-chapter.html"><i class="fa fa-check"></i><b>10</b> Constraint Transforms</a></li>
<li class="chapter" data-level="11" data-path="language-syntax.html"><a href="language-syntax.html"><i class="fa fa-check"></i><b>11</b> Language Syntax</a></li>
<li class="chapter" data-level="12" data-path="program-execution.html"><a href="program-execution.html"><i class="fa fa-check"></i><b>12</b> Program Execution</a></li>
<li class="chapter" data-level="13" data-path="deprecated-features-appendix.html"><a href="deprecated-features-appendix.html"><i class="fa fa-check"></i><b>13</b> Deprecated Features</a></li>
<li><a href="algorithms.html#algorithms"><i style="font-size: 110%; color:#990017;">Algorithms</i></a></li>
<li class="chapter" data-level="14" data-path="hmc-chapter.html"><a href="hmc-chapter.html"><i class="fa fa-check"></i><b>14</b> MCMC Sampling</a></li>
<li class="chapter" data-level="15" data-path="analysis-chapter.html"><a href="analysis-chapter.html"><i class="fa fa-check"></i><b>15</b> Posterior Analysis</a></li>
<li class="chapter" data-level="16" data-path="optimization-algorithms-chapter.html"><a href="optimization-algorithms-chapter.html"><i class="fa fa-check"></i><b>16</b> Optimization</a></li>
<li class="chapter" data-level="17" data-path="vi-algorithms-chapter.html"><a href="vi-algorithms-chapter.html"><i class="fa fa-check"></i><b>17</b> Variational Inference</a></li>
<li class="chapter" data-level="18" data-path="diagnostic-algorithms-chapter.html"><a href="diagnostic-algorithms-chapter.html"><i class="fa fa-check"></i><b>18</b> Diagnostic Mode</a></li>
<li><a href="usage.html#usage"><i style="font-size: 110%; color:#990017;">Usage</i></a></li>
<li class="chapter" data-level="19" data-path="reproducibility-chapter.html"><a href="reproducibility-chapter.html"><i class="fa fa-check"></i><b>19</b> Reproducibility</a></li>
<li class="chapter" data-level="20" data-path="licensing-appendix.html"><a href="licensing-appendix.html"><i class="fa fa-check"></i><b>20</b> Licenses and Dependencies</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan Reference Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hmc.chapter" class="section level1">
<h1><span class="header-section-number">14</span> MCMC Sampling</h1>
<p>This chapter presents the two Markov chain Monte Carlo (MCMC)
algorithms used in Stan, the Hamiltonian Monte Carlo (HMC) algorithm
and its adaptive variant the no-U-turn sampler (NUTS), along with
details of their implementation and configuration.</p>
<div id="hamiltonian-monte-carlo" class="section level2">
<h2><span class="header-section-number">14.1</span> Hamiltonian Monte Carlo</h2>
<p>Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC)
method that uses the derivatives of the density function being sampled
to generate efficient transitions spanning the posterior (see, e.g.,
<span class="citation">Betancourt and Girolami (<a href="#ref-Betancourt-Girolami:2013">2013</a>)</span>, <span class="citation">Neal (<a href="#ref-Neal:2011">2011</a>)</span> for more details). It uses an
approximate Hamiltonian dynamics simulation based on numerical
integration which is then corrected by performing a Metropolis
acceptance step.</p>
<p>This section translates the presentation of HMC by
<span class="citation">Betancourt and Girolami (<a href="#ref-Betancourt-Girolami:2013">2013</a>)</span> into the notation of <span class="citation">Gelman et al. (<a href="#ref-GelmanEtAl:2013">2013</a>)</span>.</p>
<div id="target-density" class="section level3 unnumbered">
<h3>Target Density</h3>
<p>The goal of sampling is to draw from a density <span class="math inline">\(p(\theta)\)</span> for
parameters <span class="math inline">\(\theta\)</span>. This is typically a Bayesian posterior
<span class="math inline">\(p(\theta|y)\)</span> given data <span class="math inline">\(y\)</span>, and in particular, a Bayesian posterior
coded as a Stan program.</p>
</div>
<div id="auxiliary-momentum-variable" class="section level3 unnumbered">
<h3>Auxiliary Momentum Variable</h3>
<p>HMC introduces auxiliary momentum variables <span class="math inline">\(\rho\)</span> and draws from a
joint density</p>
<p><span class="math display">\[
p(\rho, \theta) = p(\rho | \theta) p(\theta).
\]</span></p>
<p>In most applications of HMC, including Stan, the auxiliary density is
a multivariate normal that does not depend on the parameters <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[
\rho \sim \mathsf{MultiNormal}(0, \Sigma).
\]</span></p>
<p>The covariance matrix <span class="math inline">\(\Sigma\)</span> acts as a Euclidean metric to rotate
and scale the target distribution; see <span class="citation">Betancourt and Stein (<a href="#ref-Betancourt-Stein:2011">2011</a>)</span> for
details of the geometry.</p>
<p>In Stan, this matrix may be set to the identity matrix (i.e., unit
diagonal) or estimated from warmup draws and optionally restricted to
a diagonal matrix. The inverse <span class="math inline">\(\Sigma^{-1}\)</span> is known as the mass
matrix, and will be a unit, diagonal, or dense if <span class="math inline">\(\Sigma\)</span> is.</p>
</div>
<div id="the-hamiltonian" class="section level3 unnumbered">
<h3>The Hamiltonian</h3>
<p>The joint density <span class="math inline">\(p(\rho, \theta)\)</span> defines a Hamiltonian</p>
<p><span class="math display">\[
\begin{array}{rcl}
H(\rho, \theta) &amp; = &amp; - \log p(\rho, \theta)
\\[3pt]
&amp; = &amp; - \log p(\rho | \theta) - \log p(\theta).
\\[3pt]
&amp; = &amp; T(\rho | \theta) + V(\theta),
\end{array}
\]</span></p>
<p>where the term</p>
<p><span class="math display">\[
T(\rho | \theta) = - \log p(\rho | \theta)
\]</span></p>
<p>is called the “kinetic energy” and the term</p>
<p><span class="math display">\[
V(\theta) = - \log p(\theta)
\]</span></p>
<p>is called the “potential energy.” The potential energy is specified
by the Stan program through its definition of a log density.</p>
</div>
<div id="generating-transitions" class="section level3 unnumbered">
<h3>Generating Transitions</h3>
<p>Starting from the current value of the parameters <span class="math inline">\(\theta\)</span>, a
transition to a new state is generated in two stages before being
subjected to a Metropolis accept step.</p>
<p>First, a value for the momentum is drawn independently of the current
parameter values,</p>
<p><span class="math display">\[
\rho \sim \mathsf{MultiNormal}(0, \Sigma).
\]</span></p>
<p>Thus momentum does not persist across iterations.</p>
<p>Next, the joint system <span class="math inline">\((\theta,\rho)\)</span> made up of the current
parameter values <span class="math inline">\(\theta\)</span> and new momentum <span class="math inline">\(\rho\)</span> is evolved via
Hamilton’s equations,</p>
<p><span class="math display">\[
\begin{array}{rcccl}
\displaystyle
\frac{d\theta}{dt}
&amp; = &amp;
\displaystyle
+ \frac{\partial H}{\partial \rho}
&amp; = &amp;
\displaystyle
+ \frac{\partial T}{\partial \rho}
\\[12pt]
\displaystyle
\frac{d\rho}{dt}
&amp; = &amp;
\displaystyle
- \frac{\partial H}{\partial \theta }
&amp; = &amp;
\displaystyle
- \frac{\partial T}{\partial \theta}
- \frac{\partial V}{\partial \theta}.
\end{array}
\]</span></p>
<p>With the momentum density being independent of the target density,
i.e., <span class="math inline">\(p(\rho | \theta) = p(\rho)\)</span>, the first term in the
momentum time derivative, <span class="math inline">\({\partial T} / {\partial \theta}\)</span> is
zero, yielding the pair time derivatives</p>
<p><span class="math display">\[
\begin{array}{rcl}
\frac{d \theta}{d t} &amp; = &amp; +\frac{\partial T}{\partial \rho}
\\[2pt]
\frac{d \rho}{d t} &amp; = &amp; -\frac{\partial V}{\partial \theta}.
\end{array}
\]</span></p>
</div>
<div id="leapfrog-integrator" class="section level3 unnumbered">
<h3>Leapfrog Integrator</h3>
<p>The last section leaves a two-state differential equation to solve.
Stan, like most other HMC implementations, uses the leapfrog
integrator, which is a numerical integration algorithm that’s
specifically adapted to provide stable results for Hamiltonian systems
of equations.</p>
<p>Like most numerical integrators, the leapfrog algorithm takes discrete
steps of some small time interval <span class="math inline">\(\epsilon\)</span>. The leapfrog algorithm
begins by drawing a fresh momentum term independently of the parameter
values <span class="math inline">\(\theta\)</span> or previous momentum value.</p>
<p><span class="math display">\[
\rho \sim \mathsf{MultiNormal}(0, \Sigma).
\]</span>
It then alternates half-step updates of the momentum and full-step
updates of the position.</p>
<p><span class="math display">\[
\begin{array}{rcl}
\rho &amp; \leftarrow
     &amp; \rho \, - \, \frac{\epsilon}{2} \frac{\partial V}{\partial \theta}
\\[6pt]
\theta &amp; \leftarrow
       &amp; \theta \, + \, \epsilon \, \Sigma \, \rho
\\[6pt]
\rho &amp; \leftarrow
     &amp; \rho \, - \, \frac{\epsilon}{2} \frac{\partial V}{\partial \theta}.
\end{array}
\]</span></p>
<p>By applying <span class="math inline">\(L\)</span> leapfrog steps, a total of <span class="math inline">\(L \, \epsilon\)</span> time is
simulated. The resulting state at the end of the simulation (<span class="math inline">\(L\)</span>
repetitions of the above three steps) will be denoted
<span class="math inline">\((\rho^{*}, \theta^{*})\)</span>.</p>
<p>The leapfrog integrator’s error is on the order of <span class="math inline">\(\epsilon^3\)</span> per
step and <span class="math inline">\(\epsilon^2\)</span> globally, where <span class="math inline">\(\epsilon\)</span> is the time interval
(also known as the step size); <span class="citation">Leimkuhler and Reich (<a href="#ref-LeimkuhlerReich:2004">2004</a>)</span> provide a
detailed analysis of numerical integration for Hamiltonian systems,
including a derivation of the error bound for the leapfrog
integrator.</p>
</div>
<div id="metropolis-accept-step" class="section level3 unnumbered">
<h3>Metropolis Accept Step</h3>
<p>If the leapfrog integrator were perfect numerically, there would no
need to do any more randomization per transition than generating a
random momentum vector. Instead, what is done in practice to account
for numerical errors during integration is to apply a Metropolis
acceptance step, where the probability of keeping the proposal
<span class="math inline">\((\rho^{*}, \theta^{*})\)</span> generated by transitioning from
<span class="math inline">\((\rho, \theta)\)</span> is</p>
<p><span class="math display">\[
\min \!
\left(
1,
\ \exp \! \left( H(\rho, \theta) - H(\rho^{*}, \theta^{*}) \right)
\right).
\]</span></p>
<p>If the proposal is not accepted, the previous parameter value is
returned for the next draw and used to initialize the next iteration.</p>
</div>
<div id="algorithm-summary" class="section level3 unnumbered">
<h3>Algorithm Summary</h3>
<p>The Hamiltonian Monte Carlo algorithm starts at a specified initial
set of parameters <span class="math inline">\(\theta\)</span>; in Stan, this value is either
user-specified or generated randomly. Then, for a given number of
iterations, a new momentum vector is sampled and the current value of
the parameter <span class="math inline">\(\theta\)</span> is updated using the leapfrog integrator with
discretization time <span class="math inline">\(\epsilon\)</span> and number of steps <span class="math inline">\(L\)</span> according to
the Hamiltonian dynamics. Then a Metropolis acceptance step is
applied, and a decision is made whether to update to the new state
<span class="math inline">\((\theta^{*}, \rho^{*})\)</span> or keep the existing state.</p>
</div>
</div>
<div id="hmc-algorithm-parameters" class="section level2">
<h2><span class="header-section-number">14.2</span> HMC Algorithm Parameters</h2>
<p>The Hamiltonian Monte Carlo algorithm has three parameters which must
be set,</p>
<ul>
<li>discretization time <span class="math inline">\(\epsilon\)</span>,</li>
<li>mass matrix <span class="math inline">\(\Sigma^{-1}\)</span>, and</li>
<li>number of steps taken <span class="math inline">\(L\)</span>.</li>
</ul>
<p>In practice, sampling efficiency, both in terms of iteration speed and
iterations per effective sample, is highly sensitive to these three
tuning parameters <span class="citation">Neal (<a href="#ref-Neal:2011">2011</a>)</span>, <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>.</p>
<p>If <span class="math inline">\(\epsilon\)</span> is too large, the leapfrog integrator will be inaccurate
and too many proposals will be rejected. If <span class="math inline">\(\epsilon\)</span> is too small,
too many small steps will be taken by the leapfrog integrator leading
to long simulation times per interval. Thus the goal is to balance the
acceptance rate between these extremes.</p>
<p>If <span class="math inline">\(L\)</span> is too small, the trajectory traced out in each iteration will
be too short and sampling will devolve to a random walk. If <span class="math inline">\(L\)</span> is
too large, the algorithm will do too much work on each iteration.</p>
<p>If the mass matrix <span class="math inline">\(\Sigma\)</span> is poorly suited to the covariance of the
posterior, the step size <span class="math inline">\(\epsilon\)</span> will have to be decreased to
maintain arithmetic precision while at the same time, the number of
steps <span class="math inline">\(L\)</span> is increased in order to maintain simulation time to ensure
statistical efficiency.</p>
<div id="integration-time" class="section level3 unnumbered">
<h3>Integration Time</h3>
<p>The actual integration time is <span class="math inline">\(L \, \epsilon\)</span>, a function of number
of steps. Some interfaces to Stan set an approximate integration time
<span class="math inline">\(t\)</span> and the discretization interval (step size) <span class="math inline">\(\epsilon\)</span>. In these
cases, the number of steps will be rounded down as</p>
<p><span class="math display">\[
L = \left\lfloor \frac{t}{\epsilon} \right\rfloor.
\]</span></p>
<p>and the actual integration time will still be <span class="math inline">\(L \, \epsilon\)</span>.</p>
</div>
<div id="automatic-parameter-tuning" class="section level3 unnumbered">
<h3>Automatic Parameter Tuning</h3>
<p>Stan is able to automatically optimize <span class="math inline">\(\epsilon\)</span> to match an
acceptance-rate target, able to estimate <span class="math inline">\(\Sigma\)</span> based on warmup
sample iterations, and able to dynamically adapt <span class="math inline">\(L\)</span> on the fly during
sampling (and during warmup) using the no-U-turn sampling (NUTS)
algorithm <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>.</p>
<p><strong>Warmup Epochs Figure.</strong> <a id="adaptation.figure"></a>
<em>Adaptation during warmup occurs in three stages: an initial fast
adaptation interval (I), a series of expanding slow adaptation
intervals (II), and a final fast adaptation interval (III). For HMC,
both the fast and slow intervals are used for adapting the step size,
while the slow intervals are used for learning the (co)variance
necessitated by the metric. Iteration numbering starts at 1 on the
left side of the figure and increases to the right.</em></p>
<p><img src="/Users/mitzi/github/stan-dev/stan-docs/src/img/warmup-epochs.png" /></p>
<p>When adaptation is engaged (it may be turned off by fixing a step size
and mass matrix), the warmup period is split into three stages, as
illustrated in the <a href="hmc-chapter.html#adaptation.figure">warmup adaptation figure</a>,
with two <em>fast</em> intervals surrounding a series of growing
<em>slow</em> intervals. Here fast and slow refer to parameters that
adapt using local and global information, respectively; the
Hamiltonian Monte Carlo samplers, for example, define the step size as
a fast parameter and the (co)variance as a slow parameter. The size of
the the initial and final fast intervals and the initial size of the
slow interval are all customizable, although user-specified values may
be modified slightly in order to ensure alignment with the warmup
period.</p>
<p>The motivation behind this partitioning of the warmup period is to
allow for more robust adaptation. The stages are as follows.</p>
<ol style="list-style-type: decimal">
<li><p>In the initial fast interval the chain is allowed to converge towards the typical set,<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>
with only parameters that can learn from local information adapted.</p></li>
<li><p>After this initial stage parameters that require global
information, for example (co)variances, are estimated in a series of
expanding, memoryless windows; often fast parameters will be adapted
here as well.</p></li>
<li><p>Lastly, the fast parameters are allowed to adapt to the
final update of the slow parameters.</p></li>
</ol>
<p>These intervals may be controlled through the following configuration
parameters, all of which must be positive integers:</p>
<p><strong>Adaptation Parameters Table.</strong>
<em>The parameters controlling adaptation and their default values.</em></p>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">description</th>
<th align="center">default</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em>initial buffer</em></td>
<td align="left">width of initial fast adaptation interval</td>
<td align="center">75</td>
</tr>
<tr class="even">
<td align="left"><em>term buffer</em></td>
<td align="left">width of final fast adaptation interval</td>
<td align="center">50</td>
</tr>
<tr class="odd">
<td align="left"><em>window</em></td>
<td align="left">initial width of slow adaptation interval</td>
<td align="center">25</td>
</tr>
</tbody>
</table>
</div>
<div id="discretization-interval-adaptation-parameters" class="section level3 unnumbered">
<h3>Discretization-Interval Adaptation Parameters</h3>
<p>Stan’s HMC algorithms utilize dual averaging <span class="citation">Nesterov (<a href="#ref-Nesterov:2009">2009</a>)</span> to
optimize the step size.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<p>This warmup optimization procedure is extremely flexible and for
completeness, Stan exposes each tuning option for dual averaging,
using the notation of <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>. In practice, the efficacy
of the optimization is sensitive to the value of these parameters, but
we do not recommend changing the defaults without experience with the
dual-averaging algorithm. For more information, see the discussion of
dual averaging in <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2011">2011</a>)</span>, Hoffman-Gelman:2014.</p>
<p>The full set of dual-averaging parameters are</p>
<p><strong>Step Size Adaptation Parameters Table</strong>
<em>The parameters controlling step size adaptation, with constraints and
default values.</em></p>
<table>
<thead>
<tr class="header">
<th align="center">parameter</th>
<th align="left">description</th>
<th align="left">constraint</th>
<th align="center">default</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>delta</code></td>
<td align="left">target Metropolis acceptance rate</td>
<td align="left">[0, 1]</td>
<td align="center">0.8</td>
</tr>
<tr class="even">
<td align="center"><code>gamma</code></td>
<td align="left">adaptation regularization scale</td>
<td align="left">(0, infty)</td>
<td align="center">0.05</td>
</tr>
<tr class="odd">
<td align="center"><code>kappa</code></td>
<td align="left">adaptation relaxation exponent</td>
<td align="left">(0, infty)</td>
<td align="center">0.75</td>
</tr>
<tr class="even">
<td align="center"><code>t_0</code></td>
<td align="left">adaptation iteration offset</td>
<td align="left">(0, infty)</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>By setting the target acceptance parameter <span class="math inline">\(\delta\)</span> to a value closer
to 1 (its value must be strictly less than 1 and its default value is
0.8), adaptation will be forced to use smaller step sizes. This can
improve sampling efficiency (effective sample size per iteration) at
the cost of increased iteration times. Raising the value of <span class="math inline">\(\delta\)</span>
will also allow some models that would otherwise get stuck to overcome
their blockages.</p>
</div>
<div id="step-size-jitter" class="section level3 unnumbered">
<h3>Step-Size Jitter</h3>
<p>All implementations of HMC use numerical integrators requiring a step
size (equivalently, discretization time interval). Stan allows the
step size to be adapted or set explicitly. Stan also allows the step
size to be “jittered” randomly during sampling to avoid any poor
interactions with a fixed step size and regions of high curvature. The
jitter is a proportion that may be added or subtracted, so the maximum
amount of jitter is 1, which will cause step sizes to be selected in
the range of 0 to twice the adapted step size. The default value is 0,
producing no jitter.</p>
<p>Small step sizes can get HMC samplers unstuck that would otherwise get
stuck with higher step sizes. The downside is that jittering below the
adapted value will increase the number of leapfrog steps required and
thus slow down iterations, whereas jittering above the adapted value
can cause premature rejection due to simulation error in the
Hamiltonian dynamics calculation. See <span class="citation">Neal (<a href="#ref-Neal:2011">2011</a>)</span> for further
discussion of step-size jittering.</p>
</div>
<div id="euclidean-metric" class="section level3 unnumbered">
<h3>Euclidean Metric</h3>
<p>All HMC implementations in Stan utilize quadratic kinetic energy
functions which are specified up to the choice of a symmetric,
positive-definite matrix known as a <em>mass matrix</em> or, more
formally, a <em>metric</em> <span class="citation">Betancourt and Stein (<a href="#ref-Betancourt-Stein:2011">2011</a>)</span>.</p>
<p>If the metric is constant then the resulting implementation is known
as <em>Euclidean</em> HMC. Stan allows a choice among three Euclidean HMC
implementations,</p>
<ul>
<li>a unit metric (diagonal matrix of ones),</li>
<li>a diagonal metric (diagonal matrix with positive diagonal entries), and</li>
<li>a dense metric (a dense, symmetric positive definite matrix)</li>
</ul>
<p>to be configured by the user.</p>
<p>If the mass matrix is specified to be diagonal, then regularized
variances are estimated based on the iterations in each slow-stage
block (labeled II in the <a href="hmc-chapter.html#adaptation.figure">warmup adaptation stages
figure</a>). Each of these estimates
is based only on the iterations in that block. This allows early
estimates to be used to help guide warmup and then be forgotten later
so that they do not influence the final covariance estimate.</p>
<p>If the mass matrix is specified to be dense, then regularized
covariance estimates will be carried out, regularizing the estimate to
a diagonal matrix, which is itself regularized toward a unit matrix.</p>
<p>Variances or covariances are estimated using Welford accumulators
to avoid a loss of precision over many floating point operations.</p>
<div id="warmup-times-and-estimating-the-mass-matrix" class="section level4 unnumbered">
<h4>Warmup Times and Estimating the Mass Matrix</h4>
<p>The mass matrix can compensate for linear (i.e. global) correlations
in the posterior which can dramatically improve the performance of HMC
in some problems. This requires knowing the global correlations.</p>
<p>In complex models, the global correlations are usually difficult, if
not impossible, to derivate analytically; for example, nonlinear model
components convolve the scales of the data, so standardizing the data
does not always help. Therefore, Stan estimates these correlations
online with an adaptive warmup. In models with strong nonlinear
(i.e. local) correlations this learning can be slow, even with
regularization. This is ultimately why warmup in Stan often needs to
be so long, and why a sufficiently long warmup can yield such
substantial performance improvements.</p>
</div>
<div id="nonlinearity" class="section level4 unnumbered">
<h4>Nonlinearity</h4>
<p>The mass matrix compensates for only linear (equivalently global or
position-independent) correlations in the posterior. The hierarchical
parameterizations, on the other hand, affect some of the nasty
nonlinear (equivalently local or position-dependent) correlations
common in hierarchical models.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
<p>One of the biggest difficulties with dense mass matrices is the
estimation of the mass matrix itself which introduces a bit of a
chicken-and-egg scenario; in order to estimate an appropriate mass
matrix for sampling, convergence is required, and in order to
converge, an appropriate mass matrix is required.</p>
</div>
<div id="dense-vs.diagonal-mass-matrices" class="section level4 unnumbered">
<h4>Dense vs. Diagonal Mass Matrices</h4>
<p>Statistical models for which sampling is problematic are not typically
dominated by linear correlations for which a dense mass matrix can
adjust. Rather, they are governed by more complex nonlinear
correlations that are best tackled with better parameterizations or
more advanced algorithms, such as Riemannian HMC.</p>
</div>
<div id="warmup-times-and-curvature" class="section level4 unnumbered">
<h4>Warmup Times and Curvature</h4>
<p>MCMC convergence time is roughly equivalent to the autocorrelation
time. Because HMC (and NUTS) chains tend to be lowly autocorrelated
they also tend to converge quite rapidly.</p>
<p>This only applies when there is uniformity of curvature across the
posterior, an assumption which is violated in many complex models.
Quite often, the tails have large curvature while the bulk of the
posterior mass is relatively well-behaved; in other words, warmup is slow
not because the actual convergence time is slow but rather because the
cost of an HMC iteration is more expensive out in the tails.</p>
<p>Poor behavior in the tails is the kind of pathology that can be
uncovered by running only a few warmup iterations. By looking at the
acceptance probabilities and step sizes of the first few iterations
provides an idea of how bad the problem is and whether it must be
addressed with modeling efforts such as tighter priors or
reparameterizations.</p>
</div>
</div>
<div id="nuts-and-its-configuration" class="section level3 unnumbered">
<h3>NUTS and its Configuration</h3>
<p>The no-U-turn sampler (NUTS) automatically selects an appropriate
number of leapfrog steps in each iteration in order to allow the
proposals to traverse the posterior without doing unnecessary work.
The motivation is to maximize the expected squared jump distance (see,
e.g., <span class="citation">Roberts, Gelman, and Gilks (<a href="#ref-RobertsEtAl:1997">1997</a>)</span>) at each step and avoid the random-walk
behavior that arises in random-walk Metropolis or Gibbs samplers when
there is correlation in the posterior. For a precise definition of the
NUTS algorithm and a proof of detailed balance, see
<span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2011">2011</a>)</span>, <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>.</p>
<p>NUTS generates a proposal by starting at an initial position
determined by the parameters drawn in the last iteration. It then
generates an independent standard normal random momentum vector. It then
evolves the initial system both forwards and backwards in time to form
a balanced binary tree. At each iteration of the NUTS algorithm the
tree depth is increased by one, doubling the number of leapfrog steps
and effectively doubles the computation time. The algorithm terminates
in one of two ways, either</p>
<ul>
<li>the NUTS criterion (i.e., a U-turn in Euclidean space on a
subtree) is satisfied for a new subtree or the completed tree, or</li>
<li>the depth of the completed tree hits the maximum depth allowed.</li>
</ul>
<p>Rather than using a standard Metropolis step, the final parameter
value is selected via multinomial sampling with a bias toward the
second half of the steps in the trajectory <span class="citation">Betancourt (<a href="#ref-Betancourt:2016">2016</a><a href="#ref-Betancourt:2016">b</a>)</span>.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
<p>Configuring the no-U-turn sample involves putting a cap on the depth
of the trees that it evaluates during each iteration. This is
controlled through a maximum depth parameter. The number of leapfrog
steps taken is then bounded by 2 to the power of the maximum depth minus 1.</p>
<p>Both the tree depth and the actual number of leapfrog steps computed
are reported along with the parameters in the output as <code>treedepth__</code>
and <code>n_leapfrog__</code>, respectively. Because the final subtree may only
be partially constructed, these two will always satisfy</p>
<p><span class="math display">\[
2^{\mathrm{treedepth} - 1} - 1
\ &lt; \
N_{\mathrm{leapfrog}}
\ \le \
2^{\mathrm{treedepth} } - 1.
\]</span></p>
<p>Tree depth is an important diagnostic tool for NUTS. For example, a
tree depth of zero occurs when the first leapfrog step is immediately
rejected and the initial state returned, indicating extreme curvature
and poorly-chosen step size (at least relative to the current
position). On the other hand, a tree depth equal to the maximum depth
indicates that NUTS is taking many leapfrog steps and being terminated
prematurely to avoid excessively long execution time. Taking very many
steps may be a sign of poor adaptation, may be due to targeting a very
high acceptance rate, or may simply indicate a difficult posterior
from which to sample. In the latter case, reparameterization may help
with efficiency. But in the rare cases where the model is correctly
specified and a large number of steps is necessary, the maximum depth
should be increased to ensure that that the NUTS tree can grow as
large as necessary.</p>
</div>
</div>
<div id="sampling-without-parameters" class="section level2">
<h2><span class="header-section-number">14.3</span> Sampling without Parameters</h2>
<p>In some situations, such as pure forward data simulation in a directed
graphical model (e.g., where you can work down generatively from known
hyperpriors to simulate parameters and data), there is no need to
declare any parameters in Stan, the model block will be empty, and all
output quantities will be produced in the generated quantities block.
For example, to generate a sequence of <span class="math inline">\(N\)</span> draws from a binomial with
trials <span class="math inline">\(K\)</span> and chance of success <span class="math inline">\(\theta\)</span>, the following program suffices.</p>
<pre><code>data {
  real&lt;lower=0,upper=1&gt; theta;
  int&lt;lower=0&gt; K;
  int&lt;lower=0&gt; N;
}
model {
}
generated quantities {
  int&lt;lower=0,upper=K&gt; y[N];
  for (n in 1:N)
    y[n] = binomial_rng(K, theta);
}</code></pre>
<p>This program includes an empty model block because every Stan program
must have a model block, even if it’s empty. For this model, the
sampler must be configured to use the fixed-parameters setting because
there are no parameters. Without parameter sampling there is no need
for adaptation and the number of warmup iterations should be set to
zero.</p>
<p>Most models that are written to be sampled without parameters will not
declare any parameters, instead putting anything parameter-like in the
data block. Nevertheless, it is possible to include parameters for
fixed-parameters sampling and initialize them in any of the usual ways
(randomly, fixed to zero on the unconstrained scale, or with
user-specified values). For example, <code>theta</code> in the example above
could be declared as a parameter and initialized as a parameter.</p>
</div>
<div id="general-config.section" class="section level2">
<h2><span class="header-section-number">14.4</span> General Configuration Options</h2>
<p>Stan’s interfaces provide a number of configuration options that are
shared among the MCMC algorithms (this chapter), the <a href="optimization-algorithms-chapter.html#optimization-algorithms.chapter">optimization
algorithms chapter</a>, and the
<a href="diagnostic-algorithms-chapter.html#diagnostic-algorithms.chapter">diagnostics chapter</a>.</p>
<div id="random-number-generator" class="section level3 unnumbered">
<h3>Random Number Generator</h3>
<p>The random-number generator’s behavior is fully determined by the
unsigned seed (positive integer) it is started with. If a seed is not
specified, or a seed of 0 or less is specified, the system time is
used to generate a seed. The seed is recorded and included with Stan’s
output regardless of whether it was specified or generated randomly
from the system time.</p>
<p>Stan also allows a chain identifier to be specified, which is useful
when running multiple Markov chains for sampling. The chain identifier
is used to advance the random number generator a very large number of
random variates so that two chains with different identifiers draw
from non-overlapping subsequences of the random-number sequence
determined by the seed. When running multiple chains from a single
command, Stan’s interfaces will manage the chain identifiers.</p>
<div id="replication" class="section level4 unnumbered">
<h4>Replication</h4>
<p>Together, the seed and chain identifier determine the behavior of the
underlying random number generator. For complete reproducibility,
every aspect of the environment needs to be locked down from the OS
and version to the C++ compiler and version to the version of Stan and
all dependent libraries.</p>
</div>
</div>
<div id="initialization-1" class="section level3 unnumbered">
<h3>Initialization</h3>
<p>The initial parameter values for Stan’s algorithms (MCMC,
optimization, or diagnostic) may be either specified by the user or
generated randomly. If user-specified values are provided, all
parameters must be given initial values or Stan will abort with an
error message.</p>
<div id="user-defined-initialization" class="section level4 unnumbered">
<h4>User-Defined Initialization</h4>
<p>If the user specifies initial values, they must satisfy the
constraints declared in the model (i.e., they are on the constrained
scale).</p>
</div>
<div id="system-constant-zero-initialization" class="section level4 unnumbered">
<h4>System Constant Zero Initialization</h4>
<p>It is also possible to provide an initialization of 0, which causes
all variables to be initialized with zero values on the unconstrained
scale. The transforms are arranged in such a way that zero
initialization provides reasonable variable initializations for most
parameters, such as 0 for unconstrained parameters, 1 for parameters
constrained to be positive, 0.5 for variables to constrained to lie
between 0 and 1, a symmetric (uniform) vector for simplexes, unit
matrices for both correlation and covariance matrices, and so on.</p>
</div>
<div id="system-random-initialization" class="section level4 unnumbered">
<h4>System Random Initialization</h4>
<p>Random initialization by default initializes the parameter values with
values drawn at random from a <span class="math inline">\(\mathsf{Uniform}(-2, 2)\)</span> distribution.
Alternatively, a value other than 2 may be specified for the absolute
bounds. These values are on the unconstrained scale, so must be
inverse transformed back to satisfy the constraints declared for
parameters.</p>
<p>Because zero is chosen to be a reasonable default initial value for
most parameters, the interval around zero provides a fairly diffuse
starting point. For instance, unconstrained variables are initialized
randomly in <span class="math inline">\((-2, 2)\)</span>, variables constrained to be positive are
initialized roughly in <span class="math inline">\((0.14, 7.4)\)</span>, variables constrained to fall
between 0 and 1 are initialized with values roughly in <span class="math inline">\((0.12, 0.88)\)</span>.</p>
</div>
</div>
</div>
<div id="divergent-transitions" class="section level2">
<h2><span class="header-section-number">14.5</span> Divergent Transitions</h2>
<p>The Hamiltonian Monte Carlo algorithms (HMC and NUTS) simulate the
trajectory of a fictitious particle representing parameter values when
subject to a potential energy field, the value of which at a point is
the negative log posterior density (up to a constant that does not
depend on location). Random momentum is imparted independently in
each direction, by drawing from a standard normal distribution. The
Hamiltonian is defined to be the sum of the potential energy and
kinetic energy of the system. The key feature of the Hamiltonian is
that it is conserved along the trajectory the particle moves.</p>
<p>In Stan, we use the leapfrog algorithm to simulate the path of a
particle along the trajectory defined by the initial random momentum
and the potential energy field. This is done by alternating updates
of the position based on the momentum and the momentum based on the
position. The momentum updates involve the potential energy and are
applied along the gradient. This is essentially a stepwise
(discretized) first-order approximation of the trajectory.
<span class="citation">Leimkuhler and Reich (<a href="#ref-LeimkuhlerReich:2004">2004</a>)</span> provide details and error analysis for the
leapfrog algorithm.</p>
<p>A divergence arises when the simulated Hamiltonian trajectory departs
from the true trajectory as measured by departure of the Hamiltonian
value from its initial value. When this divergence is too high,<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>
the simulation has gone off the rails and cannot be trusted. The
positions along the simulated trajectory after the Hamiltonian
diverges will never be selected as the next draw of the MCMC
algorithm, potentially reducing Hamiltonian Monte Carlo to a simple
random walk and biasing estimates by not being able to thoroughly
explore the posterior distribution. <span class="citation">Betancourt (<a href="#ref-Betancourt:2016b">2016</a><a href="#ref-Betancourt:2016b">a</a>)</span> provides
details of the theory, computation, and practical implications of
divergent transitions in Hamiltonian Monte Carlo.</p>
<p>The Stan interfaces report divergences as warnings and provide ways to
access which iterations encountered divergences. ShinyStan provides
visualizations that highlight the starting point of divergent
transitions to diagnose where the divergences arise in parameter
space. A common location is in the neck of the funnel in a centered
parameterization, an example of which is provided in the user’s guide.</p>
<p>If the posterior is highly curved, very small step sizes are required
for this gradient-based simulation of the Hamiltonian to be accurate.
When the step size is too large (relative to the curvature), the
simulation diverges from the true Hamiltonian. This definition is
imprecise in the same way that stiffness for a differential equation
is imprecise; both are defined by the way they cause traditional
stepwise algorithms to diverge from where they should be.</p>
<p>The primary cause of divergent transitions in Euclidean HMC (other
than bugs in the code) is highly varying posterior curvature, for
which small step sizes are too inefficient in some regions and diverge
in other regions. If the step size is too small, the sampler becomes
inefficient and halts before making a U-turn (hits the maximum tree
depth in NUTS); if the step size is too large, the Hamiltonian
simulation diverges.</p>
<div id="diagnosing-and-eliminating-divergences" class="section level3 unnumbered">
<h3>Diagnosing and Eliminating Divergences</h3>
<p>In some cases, simply lowering the initial step size and increasing
the target acceptance rate will keep the step size small enough that
sampling can proceed. In other cases, a reparameterization is required
so that the posterior curvature is more manageable; see the funnel
example in the user’s guide for an example.</p>
<p>Before reparameterization, it may be helpful to plot the posterior
draws, highlighting the divergent transitions to see where they arise.
This is marked as a divergent transition in the interfaces; for
example, ShinyStan and RStan have special plotting facilities to
highlight where divergent transitions arise.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Betancourt-Girolami:2013">
<p>Betancourt, Michael, and Mark Girolami. 2013. “Hamiltonian Monte Carlo for Hierarchical Models.” <em>arXiv</em> 1312.0906. <a href="http://arxiv.org/abs/1312.0906" class="uri">http://arxiv.org/abs/1312.0906</a>.</p>
</div>
<div id="ref-Neal:2011">
<p>Neal, Radford. 2011. “MCMC Using Hamiltonian Dynamics.” In <em>Handbook of Markov Chain Monte Carlo</em>, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 116–62. Chapman; Hall/CRC.</p>
</div>
<div id="ref-GelmanEtAl:2013">
<p>Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. London: Chapman &amp;Hall/CRC Press.</p>
</div>
<div id="ref-Betancourt-Stein:2011">
<p>Betancourt, Michael, and Leo C. Stein. 2011. “The Geometry of Hamiltonian Monte Carlo.” <em>arXiv</em> 1112.4118. <a href="http://arxiv.org/abs/1112.4118" class="uri">http://arxiv.org/abs/1112.4118</a>.</p>
</div>
<div id="ref-LeimkuhlerReich:2004">
<p>Leimkuhler, Benedict, and Sebastian Reich. 2004. <em>Simulating Hamiltonian Dynamics</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-Hoffman-Gelman:2014">
<p>Hoffman, Matthew D., and Andrew Gelman. 2011. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>arXiv</em> 1111.4246. <a href="http://arxiv.org/abs/1111.4246" class="uri">http://arxiv.org/abs/1111.4246</a>.</p> 2014. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>Journal of Machine Learning Research</em> 15: 1593–1623. <a href="http://jmlr.org/papers/v15/hoffman14a.html" class="uri">http://jmlr.org/papers/v15/hoffman14a.html</a>.</p>
</div>
<div id="ref-Nesterov:2009">
<p>Nesterov, Y. 2009. “Primal-Dual Subgradient Methods for Convex Problems.” <em>Mathematical Programming</em> 120 (1). Springer: 221–59.</p>
</div>
<div id="ref-Hoffman-Gelman:2011">
<p>Hoffman, Matthew D., and Andrew Gelman. 2011. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>arXiv</em> 1111.4246. <a href="http://arxiv.org/abs/1111.4246" class="uri">http://arxiv.org/abs/1111.4246</a>.</p>
</div>
<div id="ref-RobertsEtAl:1997">
<p>Roberts, G.O., Andrew Gelman, and Walter R. Gilks. 1997. “Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms.” <em>Annals of Applied Probability</em> 7 (1): 110–20.</p>
</div>
<div id="ref-Betancourt:2016">
<p>Betancourt, Michael. 2016b. “Identifying the Optimal Integration Time in Hamiltonian Monte Carlo.” <em>arXiv</em> 1601.00225. <a href="https://arxiv.org/abs/1601.00225" class="uri">https://arxiv.org/abs/1601.00225</a>.</p>
</div>
<div id="ref-Betancourt:2016b">
<p>Betancourt, Michael. 2016a. “Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.” <em>arXiv</em> 1604.00695. <a href="https://arxiv.org/abs/1604.00695" class="uri">https://arxiv.org/abs/1604.00695</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="20">
<li id="fn20"><p>The typical set is a concept borrowed from information theory and refers to the neighborhood (or neighborhoods in multimodal models) of substantial posterior probability mass through which the Markov chain will travel in equilibrium.<a href="hmc-chapter.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>This optimization of step size during adaptation of the sampler should not be confused with running Stan’s optimization method.<a href="hmc-chapter.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>Only in Riemannian HMC does the metric, which can be thought of as a position-dependent mass matrix, start compensating for nonlinear correlations.<a href="hmc-chapter.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>Stan previously used slice sampling along the trajectory, following the original NUTS paper of <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>.<a href="hmc-chapter.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>The current default threshold is a factor of <span class="math inline">\(10^3\)</span>, whereas when the leapfrog integrator is working properly, the divergences will be around <span class="math inline">\(10^{-7}\)</span> and do not compound due to the symplectic nature of the leapfrog integrator.<a href="hmc-chapter.html#fnref24" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="algorithms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analysis-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": false,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
