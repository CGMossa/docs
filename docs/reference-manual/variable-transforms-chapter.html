<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Stan Reference Manual</title>
  <meta name="description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Stan Reference Manual" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://mc-stan.org/users/documentation" />
  <meta property="og:image" content="http://mc-stan.org/users/documentationimg/logo_tm.png" />
  <meta property="og:description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language." />
  <meta name="github-repo" content="stan-dev/stan" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stan Reference Manual" />
  <meta name="twitter:site" content="@mc-stan" />
  <meta name="twitter:description" content="Stan reference manual specifying the syntax and semantics of the Stan programming language." />
  <meta name="twitter:image" content="http://mc-stan.org/users/documentationimg/logo_tm.png" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="functions-chapter.html">
<link rel="next" href="language-syntax.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="../stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan Reference Manual</li>

<li class="divider"></li>
<li><a href="index.html#overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Overview</i></a></li>
<li><a href="language.html#language"><i style="font-size: 110%; color:#990017;">Language</i></a></li>
<li class="chapter" data-level="1" data-path="character-encoding.html"><a href="character-encoding.html"><i class="fa fa-check"></i><b>1</b> Character Encoding</a></li>
<li class="chapter" data-level="2" data-path="includes-section.html"><a href="includes-section.html"><i class="fa fa-check"></i><b>2</b> Includes</a></li>
<li class="chapter" data-level="3" data-path="comments-section.html"><a href="comments-section.html"><i class="fa fa-check"></i><b>3</b> Comments</a></li>
<li class="chapter" data-level="4" data-path="whitespace.html"><a href="whitespace.html"><i class="fa fa-check"></i><b>4</b> Whitespace</a></li>
<li class="chapter" data-level="5" data-path="data-types-chapter.html"><a href="data-types-chapter.html"><i class="fa fa-check"></i><b>5</b> Data Types and Declarations</a></li>
<li class="chapter" data-level="6" data-path="expressions.html"><a href="expressions.html"><i class="fa fa-check"></i><b>6</b> Expressions</a></li>
<li class="chapter" data-level="7" data-path="statements.html"><a href="statements.html"><i class="fa fa-check"></i><b>7</b> Statements</a></li>
<li class="chapter" data-level="8" data-path="blocks-chapter.html"><a href="blocks-chapter.html"><i class="fa fa-check"></i><b>8</b> Program Blocks</a></li>
<li class="chapter" data-level="9" data-path="functions-chapter.html"><a href="functions-chapter.html"><i class="fa fa-check"></i><b>9</b> User-Defined Functions</a></li>
<li class="chapter" data-level="10" data-path="variable-transforms-chapter.html"><a href="variable-transforms-chapter.html"><i class="fa fa-check"></i><b>10</b> Constraint Transforms</a></li>
<li class="chapter" data-level="11" data-path="language-syntax.html"><a href="language-syntax.html"><i class="fa fa-check"></i><b>11</b> Language Syntax</a></li>
<li class="chapter" data-level="12" data-path="program-execution.html"><a href="program-execution.html"><i class="fa fa-check"></i><b>12</b> Program Execution</a></li>
<li class="chapter" data-level="13" data-path="deprecated-features-appendix.html"><a href="deprecated-features-appendix.html"><i class="fa fa-check"></i><b>13</b> Deprecated Features</a></li>
<li><a href="algorithms.html#algorithms"><i style="font-size: 110%; color:#990017;">Algorithms</i></a></li>
<li class="chapter" data-level="14" data-path="hmc-chapter.html"><a href="hmc-chapter.html"><i class="fa fa-check"></i><b>14</b> MCMC Sampling</a></li>
<li class="chapter" data-level="15" data-path="analysis-chapter.html"><a href="analysis-chapter.html"><i class="fa fa-check"></i><b>15</b> Posterior Analysis</a></li>
<li class="chapter" data-level="16" data-path="optimization-algorithms-chapter.html"><a href="optimization-algorithms-chapter.html"><i class="fa fa-check"></i><b>16</b> Optimization</a></li>
<li class="chapter" data-level="17" data-path="vi-algorithms-chapter.html"><a href="vi-algorithms-chapter.html"><i class="fa fa-check"></i><b>17</b> Variational Inference</a></li>
<li class="chapter" data-level="18" data-path="diagnostic-algorithms-chapter.html"><a href="diagnostic-algorithms-chapter.html"><i class="fa fa-check"></i><b>18</b> Diagnostic Mode</a></li>
<li><a href="usage.html#usage"><i style="font-size: 110%; color:#990017;">Usage</i></a></li>
<li class="chapter" data-level="19" data-path="reproducibility-chapter.html"><a href="reproducibility-chapter.html"><i class="fa fa-check"></i><b>19</b> Reproducibility</a></li>
<li class="chapter" data-level="20" data-path="licensing-appendix.html"><a href="licensing-appendix.html"><i class="fa fa-check"></i><b>20</b> Licenses and Dependencies</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan Reference Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variable-transforms.chapter" class="section level1">
<h1><span class="header-section-number">10</span> Constraint Transforms</h1>
<p>To avoid having to deal with constraints while simulating the
Hamiltonian dynamics during sampling, every (multivariate) parameter
in a Stan model is transformed to an unconstrained variable behind
the scenes by the model compiler. The transform is based on the
constraints, if any, in the parameter’s definition. Scalars or the
scalar values in vectors, row vectors or matrices may be constrained
with lower and/or upper bounds. Vectors may alternatively be
constrained to be ordered, positive ordered, or simplexes. Matrices
may be constrained to be correlation matrices or covariance matrices.
This chapter provides a definition of the transforms used for each
type of variable.</p>
<p>Stan converts models to C++ classes which define probability
functions with support on all of <span class="math inline">\(\mathbb{R}^K\)</span>, where <span class="math inline">\(K\)</span> is the number
of unconstrained parameters needed to define the constrained
parameters defined in the program. The C++ classes also include
code to transform the parameters from unconstrained to constrained and
apply the appropriate Jacobians.</p>
<div id="change-of-variables.section" class="section level2">
<h2><span class="header-section-number">10.1</span> Changes of Variables</h2>
<p>The support of a random variable <span class="math inline">\(X\)</span> with density <span class="math inline">\(p_X(x)\)</span> is that
subset of values for which it has non-zero density,</p>
<p><span class="math display">\[
\mathrm{supp}(X) = \{ x | p_X(x) &gt; 0 \}.
\]</span></p>
<p>If <span class="math inline">\(f\)</span> is a total function defined on the support of <span class="math inline">\(X\)</span>, then <span class="math inline">\(Y = f(X)\)</span> is a new random variable. This section shows how to compute the
probability density function of <span class="math inline">\(Y\)</span> for well-behaved transforms <span class="math inline">\(f\)</span>.
The rest of the chapter details the transforms used by Stan.</p>
<div id="univariate-changes-of-variables" class="section level3 unnumbered">
<h3>Univariate Changes of Variables</h3>
<p>Suppose <span class="math inline">\(X\)</span> is one dimensional and <span class="math inline">\(f: \mathrm{supp}(X) \rightarrow \mathbb{R}\)</span> is a one-to-one, monotonic function with a differentiable
inverse <span class="math inline">\(f^{-1}\)</span>. Then the density of <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[
p_Y(y) = p_X(f^{-1}(y))
         \,
         \left| \, \frac{d}{dy} f^{-1}(y)\, \right|.
\]</span></p>
<p>The absolute derivative of the inverse transform measures how the
scale of the transformed variable changes with respect to the
underlying variable.</p>
</div>
<div id="multivariate-changes-of-variables" class="section level3 unnumbered">
<h3>Multivariate Changes of Variables</h3>
<p>The multivariate generalization of an absolute derivative is a
Jacobian, or more fully the absolute value of the determinant of the
Jacobian matrix of the transform. The Jacobian matrix measures the change of
each output variable relative to every input variable and the absolute
determinant uses that to determine the differential change in volume
at a given point in the parameter space.</p>
<p>Suppose <span class="math inline">\(X\)</span> is a <span class="math inline">\(K\)</span>-dimensional random variable with probability
density function <span class="math inline">\(p_X(x)\)</span>. A new random variable <span class="math inline">\(Y = f(X)\)</span> may be
defined by transforming <span class="math inline">\(X\)</span> with a suitably well-behaved function <span class="math inline">\(f\)</span>.
It suffices for what follows to note that if <span class="math inline">\(f\)</span> is one-to-one
and its inverse <span class="math inline">\(f^{-1}\)</span> has a well-defined Jacobian, then the
density of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
p_Y(y) = p_X(f^{-1}(y)) \, \left| \, \det \, J_{f^{-1}}(y) \, \right|,
\]</span></p>
<p>where <span class="math inline">\(\det{}\)</span> is the matrix determinant operation and <span class="math inline">\(J_{f^{-1}}(y)\)</span>
is the Jacobian matrix of <span class="math inline">\(f^{-1}\)</span> evaluated at <span class="math inline">\(y\)</span>. Taking <span class="math inline">\(x = f^{-1}(y)\)</span>, the Jacobian matrix is defined by</p>
<p><span class="math display">\[
J_{f^{-1}}(y) =
\left[
\begin{array}{ccc}\displaystyle
\frac{\partial x_1}{\partial y_1}
&amp; \cdots
&amp; \displaystyle \frac{\partial x_1}{\partial y_{K}}
\\
\vdots &amp; \vdots &amp; \vdots
\\
\displaystyle\frac{\partial x_{K}}{\partial y_1}
&amp; \cdots
&amp; \displaystyle\frac{\partial x_{K}}{\partial y_{K}}
\end{array}
\right].
\]</span></p>
<p>If the Jacobian matrix is triangular, the determinant reduces to the
product of the diagonal entries,</p>
<p><span class="math display">\[
\det \, J_{f^{-1}}(y)
= \prod_{k=1}^K \frac{\partial x_k}{\partial y_k}.
\]</span></p>
<p>Triangular matrices naturally arise in situations where the variables
are ordered, for instance by dimension, and each variable’s
transformed value depends on the previous variable’s transformed
values. Diagonal matrices, a simple form of triangular matrix, arise
if each transformed variable only depends on a single untransformed
variable.</p>
</div>
</div>
<div id="lower-bound-transform.section" class="section level2">
<h2><span class="header-section-number">10.2</span> Lower Bounded Scalar</h2>
<p>Stan uses a logarithmic transform for lower and upper bounds.</p>
<div id="lower-bound-transform" class="section level3 unnumbered">
<h3>Lower Bound Transform</h3>
<p>If a variable <span class="math inline">\(X\)</span> is declared to have lower bound <span class="math inline">\(a\)</span>, it is
transformed to an unbounded variable <span class="math inline">\(Y\)</span>, where</p>
<p><span class="math display">\[
Y = \log(X - a).
\]</span></p>
</div>
<div id="lower-bound-inverse-transform" class="section level3 unnumbered">
<h3>Lower Bound Inverse Transform</h3>
<p>The inverse of the lower-bound transform maps an unbounded
variable <span class="math inline">\(Y\)</span> to a variable <span class="math inline">\(X\)</span> that is bounded below by <span class="math inline">\(a\)</span> by</p>
<p><span class="math display">\[
X = \exp(Y) + a.
\]</span></p>
</div>
<div id="absolute-derivative-of-the-lower-bound-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Derivative of the Lower Bound Inverse Transform</h3>
<p>The absolute derivative of the inverse transform is</p>
<p><span class="math display">\[
\left| \,
\frac{d}{dy} \left( \exp(y) + a \right)
\, \right|
= \exp(y).
\]</span></p>
<p>Therefore, given the density <span class="math inline">\(p_X\)</span> of <span class="math inline">\(X\)</span>, the density of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
p_Y(y)
= p_X\!\left( \exp(y) + a \right) \cdot \exp(y).
\]</span></p>
</div>
</div>
<div id="upper-bounded-scalar" class="section level2">
<h2><span class="header-section-number">10.3</span> Upper Bounded Scalar</h2>
<p>Stan uses a negated logarithmic transform for upper bounds.</p>
<div id="upper-bound-transform" class="section level3 unnumbered">
<h3>Upper Bound Transform</h3>
<p>If a variable <span class="math inline">\(X\)</span> is declared to have an upper bound <span class="math inline">\(b\)</span>, it is
transformed to the unbounded variable <span class="math inline">\(Y\)</span> by</p>
<p><span class="math display">\[
Y = \log(b - X).
\]</span></p>
</div>
<div id="upper-bound-inverse-transform" class="section level3 unnumbered">
<h3>Upper Bound Inverse Transform</h3>
<p>The inverse of the upper bound transform converts the unbounded
variable <span class="math inline">\(Y\)</span> to the variable <span class="math inline">\(X\)</span> bounded above by <span class="math inline">\(b\)</span> through</p>
<p><span class="math display">\[
X = b - \exp(Y).
\]</span></p>
</div>
<div id="absolute-derivative-of-the-upper-bound-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Derivative of the Upper Bound Inverse Transform</h3>
<p>The absolute derivative of the inverse of the upper bound transform is</p>
<p><span class="math display">\[
\left| \,
\frac{d}{dy} \left( b - \exp(y) \right)
\, \right|
= \exp(y).
\]</span></p>
<p>Therefore, the density of the unconstrained variable <span class="math inline">\(Y\)</span> is defined in
terms of the density of the variable <span class="math inline">\(X\)</span> with an upper bound of <span class="math inline">\(b\)</span> by</p>
<p><span class="math display">\[
p_Y(y)
 =   p_X \!\left( b - \exp(y) \right) \cdot \exp(y).
\]</span></p>
</div>
</div>
<div id="logit-transform-jacobian.section" class="section level2">
<h2><span class="header-section-number">10.4</span> Lower and Upper Bounded Scalar</h2>
<p>For lower and upper-bounded variables, Stan uses a scaled and
translated log-odds transform.</p>
<div id="log-odds-and-the-logistic-sigmoid" class="section level3 unnumbered">
<h3>Log Odds and the Logistic Sigmoid</h3>
<p>The log-odds function is defined for <span class="math inline">\(u \in (0,1)\)</span> by</p>
<p><span class="math display">\[
\mathrm{logit}(u) = \log \frac{u}{1 - u}.
\]</span></p>
<p>The inverse of the log odds function is the logistic sigmoid, defined
for <span class="math inline">\(v \in (-\infty,\infty)\)</span> by</p>
<p><span class="math display">\[
\mathrm{logit}^{-1}(v) = \frac{1}{1 + \exp(-v)}.
\]</span></p>
<p>The derivative of the logistic sigmoid is</p>
<p><span class="math display">\[
\frac{d}{dy} \mathrm{logit}^{-1}(y)
= \mathrm{logit}^{-1}(y) \cdot \left( 1 - \mathrm{logit}^{-1}(y) \right).
\]</span></p>
</div>
<div id="lower-and-upper-bounds-transform" class="section level3 unnumbered">
<h3>Lower and Upper Bounds Transform</h3>
<p>For variables constrained to be in the open interval <span class="math inline">\((a, b)\)</span>, Stan
uses a scaled and translated log-odds transform. If variable <span class="math inline">\(X\)</span> is
declared to have lower bound <span class="math inline">\(a\)</span> and upper bound <span class="math inline">\(b\)</span>, then it is
transformed to a new variable <span class="math inline">\(Y\)</span>, where</p>
<p><span class="math display">\[
Y = \mathrm{logit} \left( \frac{X - a}{b - a} \right).
\]</span></p>
</div>
<div id="lower-and-upper-bounds-inverse-transform" class="section level3 unnumbered">
<h3>Lower and Upper Bounds Inverse Transform</h3>
<p>The inverse of this transform is</p>
<p><span class="math display">\[
X = a + (b - a) \cdot \mathrm{logit}^{-1}(Y).
\]</span></p>
</div>
<div id="absolute-derivative-of-the-lower-and-upper-bounds-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Derivative of the Lower and Upper Bounds Inverse Transform</h3>
<p>The absolute derivative of the inverse transform is given by</p>
<p><span class="math display">\[
\left|
  \frac{d}{dy}
    \left(
      a + (b - a) \cdot \mathrm{logit}^{-1}(y)
    \right)
  \right|
= (b - a)
    \cdot \mathrm{logit}^{-1}(y)
    \cdot \left( 1 - \mathrm{logit}^{-1}(y) \right).
\]</span></p>
<p>Therefore, the density of the transformed variable <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
p_Y(y)
=
 p_X \! \left( a + (b - a) \cdot \mathrm{logit}^{-1}(y) \right)
    \cdot (b - a)
    \cdot \mathrm{logit}^{-1}(y)
    \cdot \left( 1 - \mathrm{logit}^{-1}(y) \right).
\]</span></p>
<p>Despite the apparent complexity of this expression, most of the terms
are repeated and thus only need to be evaluated once. Most
importantly, <span class="math inline">\(\mathrm{logit}^{-1}(y)\)</span> only needs to be evaluated once,
so there is only one call to <span class="math inline">\(\exp(-y)\)</span>.</p>
</div>
</div>
<div id="affinely-transformed-scalar" class="section level2">
<h2><span class="header-section-number">10.5</span> Affinely Transformed Scalar</h2>
<p>Stan uses an affine transform to be able to specify parameters with a given
location and scale.</p>
<div id="affine-transform" class="section level3 unnumbered">
<h3>Affine Transform</h3>
<p>For variables with expected location <span class="math inline">\(\mu\)</span> and/or (positive) scale <span class="math inline">\(\sigma\)</span>,
Stan uses an affine transform. Such a variable <span class="math inline">\(X\)</span> is transformed to a new
variable <span class="math inline">\(Y\)</span>, where</p>
<p><span class="math display">\[
Y = \mu + \sigma * X.
\]</span></p>
<p>The default value for the location <span class="math inline">\(\mu\)</span> is <span class="math inline">\(0\)</span> and for the scale <span class="math inline">\(\sigma\)</span> is
<span class="math inline">\(1\)</span> in case not both are specified.</p>
</div>
<div id="affine-inverse-transform" class="section level3 unnumbered">
<h3>Affine Inverse Transform</h3>
<p>The inverse of this transform is</p>
<p><span class="math display">\[
X = \frac{Y-\mu}{\sigma}.
\]</span></p>
</div>
<div id="absolute-derivative-of-the-affine-inverse-transform" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Absolute Derivative of the Affine Inverse Transform</h3>
<p>The absolute derivative of the affine inverse transform is</p>
<p><span class="math display">\[
\left|
  \frac{d}{dy}
    \left(
      \frac{y-\mu}{\sigma}
    \right)
  \right|
= \frac{1}{\sigma}.
\]</span></p>
<p>Therefore, the density of the transformed variable <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
p_Y(y)
=
 p_X \! \left( \frac{y-\mu}{\sigma} \right)
    \cdot \frac{1}{\sigma}.
\]</span></p>
</div>
</div>
<div id="ordered-vector" class="section level2">
<h2><span class="header-section-number">10.6</span> Ordered Vector</h2>
<p>For some modeling tasks, a vector-valued random variable <span class="math inline">\(X\)</span> is
required with support on ordered sequences. One example is the set of
cut points in ordered logistic regression.</p>
<p>In constraint terms, an ordered <span class="math inline">\(K\)</span>-vector <span class="math inline">\(x \in \mathbb{R}^K\)</span> satisfies</p>
<p><span class="math display">\[
x_k &lt; x_{k+1}
\]</span></p>
<p>for <span class="math inline">\(k \in \{ 1, \ldots, K-1 \}\)</span>.</p>
<div id="ordered-transform" class="section level3 unnumbered">
<h3>Ordered Transform</h3>
<p>Stan’s transform follows the constraint directly. It maps an
increasing vector <span class="math inline">\(x \in \mathbb{R}^{K}\)</span> to an unconstrained vector <span class="math inline">\(y \in \mathbb{R}^K\)</span> by setting</p>
<p><span class="math display">\[
y_k
=
\left\{
\begin{array}{ll}
x_1 &amp; \mbox{if } k = 1, \mbox{ and}
\\
\log \left( x_{k} - x_{k-1} \right) &amp; \mbox{if } 1 &lt; k \leq K.
\end{array}
\right.
\]</span></p>
</div>
<div id="ordered-inverse-transform" class="section level3 unnumbered">
<h3>Ordered Inverse Transform</h3>
<p>The inverse transform for an unconstrained <span class="math inline">\(y \in \mathbb{R}^K\)</span> to an
ordered sequence <span class="math inline">\(x \in \mathbb{R}^K\)</span> is defined by the recursion</p>
<p><span class="math display">\[
x_k
=
\left\{
\begin{array}{ll}
y_1 &amp; \mbox{if } k = 1, \mbox{ and}
\\
x_{k-1} + \exp(y_k) &amp; \mbox{if } 1 &lt; k \leq K.
\end{array}
\right.
\]</span></p>
<p><span class="math inline">\(x_k\)</span> can also be expressed iteratively as</p>
<p><span class="math display">\[
x_k = y_1 + \sum_{k&#39;=2}^k \exp(y_{k&#39;}).
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-the-ordered-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of the Ordered Inverse Transform</h3>
<p>The Jacobian of the inverse transform <span class="math inline">\(f^{-1}\)</span> is lower triangular,
with diagonal elements for <span class="math inline">\(1 \leq k \leq K\)</span> of</p>
<p><span class="math display">\[
J_{k,k} =
\left\{
\begin{array}{ll}
1 &amp; \mbox{if } k = 1, \mbox{ and}
\\
\exp(y_k) &amp; \mbox{if } 1 &lt; k \leq K.
\end{array}
\right.
\]</span></p>
<p>Because <span class="math inline">\(J\)</span> is triangular, the absolute Jacobian determinant is</p>
<p><span class="math display">\[
\left| \, \det \, J \, \right|
\ = \
\left| \, \prod_{k=1}^K J_{k,k} \, \right|
\ = \
\prod_{k=2}^K \exp(y_k).
\]</span></p>
<p>Putting this all together, if <span class="math inline">\(p_X\)</span> is the density of <span class="math inline">\(X\)</span>, then the
transformed variable <span class="math inline">\(Y\)</span> has density <span class="math inline">\(p_Y\)</span> given by</p>
<p><span class="math display">\[
p_Y(y)
= p_X(f^{-1}(y))
\
\prod_{k=2}^K \exp(y_k).
\]</span></p>
</div>
</div>
<div id="simplex-transform.section" class="section level2">
<h2><span class="header-section-number">10.7</span> Unit Simplex</h2>
<p>Variables constrained to the unit simplex show up in multivariate
discrete models as both parameters (categorical and multinomial) and
as variates generated by their priors (Dirichlet and multivariate
logistic).</p>
<p>The unit <span class="math inline">\(K\)</span>-simplex is the set of points <span class="math inline">\(x \in \mathbb{R}^K\)</span> such that
for <span class="math inline">\(1 \leq k \leq K\)</span>,</p>
<p><span class="math display">\[
x_k &gt; 0,
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\sum_{k=1}^K x_k = 1.
\]</span></p>
<p>An alternative definition is to take the convex closure of the
vertices. For instance, in 2-dimensions, the simplex vertices are the
extreme values <span class="math inline">\((0,1)\)</span>, and <span class="math inline">\((1,0)\)</span> and the unit 2-simplex is the line
connecting these two points; values such as <span class="math inline">\((0.3,0.7)\)</span> and
<span class="math inline">\((0.99,0.01)\)</span> lie on the line. In 3-dimensions, the basis is
<span class="math inline">\((0,0,1)\)</span>, <span class="math inline">\((0,1,0)\)</span> and <span class="math inline">\((1,0,0)\)</span> and the unit 3-simplex is the
boundary and interior of the triangle with these vertices. Points in
the 3-simplex include <span class="math inline">\((0.5,0.5,0)\)</span>, <span class="math inline">\((0.2,0.7,0.1)\)</span> and all other
triplets of non-negative values summing to 1.</p>
<p>As these examples illustrate, the simplex always picks out a subspace
of <span class="math inline">\(K-1\)</span> dimensions from <span class="math inline">\(\mathbb{R}^K\)</span>. Therefore a point <span class="math inline">\(x\)</span> in the
<span class="math inline">\(K\)</span>-simplex is fully determined by its first <span class="math inline">\(K-1\)</span> elements <span class="math inline">\(x_1, x_2, \ldots, x_{K-1}\)</span>, with</p>
<p><span class="math display">\[
x_K = 1 - \sum_{k=1}^{K-1} x_k.
\]</span></p>
<div id="unit-simplex-inverse-transform" class="section level3 unnumbered">
<h3>Unit Simplex Inverse Transform</h3>
<p>Stan’s unit simplex inverse transform may be understood using the
following stick-breaking metaphor.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<ol style="list-style-type: decimal">
<li>Take a stick of unit length (i.e., length 1).</li>
<li>Break a piece off and label it as <span class="math inline">\(x_1\)</span>, and set it aside, keeping
what’s left.</li>
<li>Next, break a piece off what’s left, label it <span class="math inline">\(x_2\)</span>, and set it
aside, keeping what’s left.</li>
<li>Continue breaking off pieces of what’s left, labeling them, and
setting them aside for pieces <span class="math inline">\(x_3,\ldots,x_{K-1}\)</span>.</li>
<li>Label what’s left <span class="math inline">\(x_K\)</span>.</li>
</ol>
<p>The resulting vector <span class="math inline">\(x = [x_1,\ldots,x_{K}]^{\top}\)</span> is a unit
simplex because each piece has non-negative length and the sum of
the stick lengths is one by construction.</p>
<p>This full inverse mapping requires the breaks to be represented as the
fraction in <span class="math inline">\((0,1)\)</span> of the original stick that is broken off. These
break ratios are themselves derived from unconstrained values in
<span class="math inline">\((-\infty,\infty)\)</span> using the inverse logit transform as described
above for unidimensional variables with lower and upper bounds.</p>
<p>More formally, an intermediate vector <span class="math inline">\(z \in \mathbb{R}^{K-1}\)</span>, whose
coordinates <span class="math inline">\(z_k\)</span> represent the proportion of the stick broken off in
step <span class="math inline">\(k\)</span>, is defined elementwise for <span class="math inline">\(1 \leq k &lt; K\)</span> by</p>
<p><span class="math display">\[
z_k = \mathrm{logit}^{-1} \left( y_k
                             + \log \left( \frac{1}{K - k}
                                            \right)
                       \right).
\]</span></p>
<p>The logit term
<span class="math inline">\(\log\left(\frac{1}{K-k}\right) (i.e., \mathrm{logit}\left(\frac{1}{K-k+1}\right)\)</span>) in
the above definition adjusts the transform so that a
zero vector <span class="math inline">\(y\)</span> is mapped to the simplex <span class="math inline">\(x = (1/K,\ldots,1/K)\)</span>. For instance, if
<span class="math inline">\(y_1 = 0\)</span>, then <span class="math inline">\(z_1 = 1/K\)</span>; if <span class="math inline">\(y_2 = 0\)</span>, then <span class="math inline">\(z_2 = 1/(K-1)\)</span>; and
if <span class="math inline">\(y_{K-1} = 0\)</span>, then <span class="math inline">\(z_{K-1} = 1/2\)</span>.</p>
<p>The break proportions <span class="math inline">\(z\)</span> are applied to determine the stick sizes and
resulting value of <span class="math inline">\(x_k\)</span> for <span class="math inline">\(1 \leq k &lt; K\)</span> by</p>
<p><span class="math display">\[
x_k =
\left( 1 - \sum_{k&#39;=1}^{k-1} x_{k&#39;} \right) z_k.
\]</span></p>
<p>The summation term represents the length of the original stick left at
stage <span class="math inline">\(k\)</span>. This is multiplied by the break proportion <span class="math inline">\(z_k\)</span> to yield
<span class="math inline">\(x_k\)</span>. Only <span class="math inline">\(K-1\)</span> unconstrained parameters are required, with
the last dimension’s value <span class="math inline">\(x_K\)</span> set to the length of the remaining
piece of the original stick,</p>
<p><span class="math display">\[
x_K = 1 - \sum_{k=1}^{K-1} x_k.
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-the-unit-simplex-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of the Unit-Simplex Inverse Transform</h3>
<p>The Jacobian <span class="math inline">\(J\)</span> of the inverse transform <span class="math inline">\(f^{-1}\)</span> is
lower-triangular, with diagonal entries</p>
<p><span class="math display">\[
J_{k,k}
=
\frac{\partial x_k}{\partial y_k}
=
\frac{\partial x_k}{\partial z_k} \,
\frac{\partial z_k}{\partial y_k},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\frac{\partial z_k}{\partial y_k}
= \frac{\partial}{\partial y_k}
   \mathrm{logit}^{-1} \left(
                       y_k + \log \left( \frac{1}{K-k}
                                          \right)
                    \right)
= z_k (1 - z_k),
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{\partial x_k}{\partial z_k}
=
\left(
  1 - \sum_{k&#39; = 1}^{k-1} x_{k&#39;}
   \right)
.
\]</span></p>
<p>This definition is recursive, defining <span class="math inline">\(x_k\)</span> in terms of
<span class="math inline">\(x_{1},\ldots,x_{k-1}\)</span>.</p>
<p>Because the Jacobian <span class="math inline">\(J\)</span> of <span class="math inline">\(f^{-1}\)</span> is lower triangular and positive, its
absolute determinant reduces to</p>
<p><span class="math display">\[
\left| \, \det J \, \right|
\ = \
\prod_{k=1}^{K-1} J_{k,k}
\ = \
\prod_{k=1}^{K-1}
z_k
\,
(1 - z_k)
\
\left(
1 - \sum_{k&#39;=1}^{k-1} x_{k&#39;}
\right)
.
\]</span></p>
<p>Thus the transformed variable <span class="math inline">\(Y = f(X)\)</span> has a density given by</p>
<p><span class="math display">\[
p_Y(y)
= p_X(f^{-1}(y))
\,
\prod_{k=1}^{K-1}
z_k
\,
(1 - z_k)
\
\left(
1 - \sum_{k&#39;=1}^{k-1} x_{k&#39;}
\right)
.
\]</span></p>
<p>Even though it is expressed in terms of intermediate values <span class="math inline">\(z_k\)</span>,
this expression still looks more complex than it is. The exponential
function need only be evaluated once for each unconstrained parameter
<span class="math inline">\(y_k\)</span>; everything else is just basic arithmetic that can be computed
incrementally along with the transform.</p>
</div>
<div id="unit-simplex-transform" class="section level3 unnumbered">
<h3>Unit Simplex Transform</h3>
<p>The transform <span class="math inline">\(Y = f(X)\)</span> can be derived by reversing the stages of the
inverse transform. Working backwards, given the break proportions
<span class="math inline">\(z\)</span>, <span class="math inline">\(y\)</span> is defined elementwise by</p>
<p><span class="math display">\[
y_k
= \mathrm{logit}(z_k)
- \mbox{log}\left(
   \frac{1}{K-k}
   \right)
.
\]</span></p>
<p>The break proportions <span class="math inline">\(z_k\)</span> are defined to be the ratio of <span class="math inline">\(x_k\)</span> to
the length of stick left after the first <span class="math inline">\(k-1\)</span> pieces have been broken
off,</p>
<p><span class="math display">\[
z_k
= \frac{x_k}
       {1 - \sum_{k&#39; = 1}^{k-1} x_{k&#39;}}
.
\]</span></p>
</div>
</div>
<div id="unit-vector.section" class="section level2">
<h2><span class="header-section-number">10.8</span> Unit Vector</h2>
<p>An <span class="math inline">\(n\)</span>-dimensional vector <span class="math inline">\(x \in \mathbb{R}^n\)</span> is said to be a unit vector
if it has unit Euclidean length, so that</p>
<p><span class="math display">\[
\Vert x \Vert
\ = \ \sqrt{x^{\top}\,x}
\ = \ \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
\ = \ 1\ .
\]</span></p>
<div id="unit-vector-inverse-transform" class="section level3 unnumbered">
<h3>Unit Vector Inverse Transform</h3>
<p>Stan divides an unconstrained vector <span class="math inline">\(y \in \mathbb{R}^{n}\)</span> by its norm,
<span class="math inline">\(\Vert y \Vert = \sqrt{y^\top y}\)</span>, to obtain a unit vector <span class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[
x = \frac{y}{\Vert y \Vert}.
\]</span></p>
<p>To generate a unit vector, Stan generates points at random in
<span class="math inline">\(\mathbb{R}^n\)</span> with independent unit normal distributions, which are then
standardized by dividing by their Euclidean length.
<span class="citation">Marsaglia (<a href="#ref-Marsaglia:1972">1972</a>)</span> showed this generates points uniformly at random
on <span class="math inline">\(S^{n-1}\)</span>. That is, if we draw <span class="math inline">\(y_n \sim \mathsf{Normal}(0, 1)\)</span>
for <span class="math inline">\(n \in 1{:}n\)</span>, then <span class="math inline">\(x = \frac{y}{\Vert y \Vert}\)</span> has a uniform
distribution over <span class="math inline">\(S^{n-1}\)</span>. This allows us to use an <span class="math inline">\(n\)</span>-dimensional
basis for <span class="math inline">\(S^{n-1}\)</span> that preserves local neighborhoods in that points
that are close to each other in <span class="math inline">\(\mathbb{R}^n\)</span> map to points near each
other in <span class="math inline">\(S^{n-1}\)</span>. The mapping is not perfectly distance preserving,
because there are points arbitrarily far away from each other in
<span class="math inline">\(\mathbb{R}^n\)</span> that map to identical points in <span class="math inline">\(S^{n-1}\)</span>.</p>
<div id="warning-undefined-at-zero" class="section level4 unnumbered">
<h4>Warning: undefined at zero!</h4>
<p>The above mapping from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(S^n\)</span> is not defined at zero.
While this point outcome has measure zero during sampling, and may
thus be ignored, it is the default initialization point and thus unit
vector parameters cannot be initialized at zero. A simple workaround
is to initialize from a very small interval around zero, which is an
option built into all of the Stan interfaces.</p>
</div>
</div>
<div id="absolute-jacobian-determinant-of-the-unit-vector-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of the Unit Vector Inverse Transform</h3>
<p>The Jacobian matrix relating the input vector <span class="math inline">\(y\)</span> to the output vector
<span class="math inline">\(x\)</span> is singular because <span class="math inline">\(x^\top x = 1\)</span> for any non-zero input vector
<span class="math inline">\(y\)</span>. Thus, there technically is no unique transformation from <span class="math inline">\(x\)</span> to
<span class="math inline">\(y\)</span>. To circumvent this issue, let <span class="math inline">\(r = \sqrt{y^\top y}\)</span> so that <span class="math inline">\(y = r x\)</span>. The transformation from <span class="math inline">\(\left(r, x_{-n}\right)\)</span> to <span class="math inline">\(y\)</span> is
well-defined but <span class="math inline">\(r\)</span> is arbitrary, so we set <span class="math inline">\(r = 1\)</span>. In this case,
the determinant of the Jacobian is proportional to <span class="math inline">\(-\frac{1}{2} y^\top y\)</span>,
which is the kernel of a standard multivariate normal distribution
with <span class="math inline">\(n\)</span> independent dimensions.</p>
</div>
</div>
<div id="correlation-matrix-transform.section" class="section level2">
<h2><span class="header-section-number">10.9</span> Correlation Matrices</h2>
<p>A <span class="math inline">\(K \times K\)</span> correlation matrix <span class="math inline">\(x\)</span> must be is a symmetric, so that</p>
<p><span class="math display">\[
x_{k,k&#39;} = x_{k&#39;,k}
\]</span></p>
<p>for all <span class="math inline">\(k,k&#39; \in \{ 1, \ldots, K \}\)</span>, it must have a unit diagonal,
so that</p>
<p><span class="math display">\[
x_{k,k} = 1
\]</span></p>
<p>for all <span class="math inline">\(k \in \{ 1, \ldots, K \}\)</span>, and it must be positive
definite, so that for every non-zero <span class="math inline">\(K\)</span>-vector <span class="math inline">\(a\)</span>,</p>
<p><span class="math display">\[
a^{\top} x a &gt; 0.
\]</span></p>
<p>The number of free parameters required to specify a <span class="math inline">\(K \times K\)</span>
correlation matrix is <span class="math inline">\(\binom{K}{2}\)</span>.</p>
<p>There is more than one way to map from <span class="math inline">\(\binom{K}{2}\)</span> unconstrained
parameters to a <span class="math inline">\(K \times K\)</span> correlation matrix. Stan implements the
Lewandowski-Kurowicka-Joe (LKJ) transform
<span class="citation">Lewandowski, Kurowicka, and Joe (<a href="#ref-LewandowskiKurowickaJoe:2009">2009</a>)</span>.</p>
<div id="correlation-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Correlation Matrix Inverse Transform</h3>
<p>It is easiest to specify the inverse, going from its <span class="math inline">\(\binom{K}{2}\)</span>
parameter basis to a correlation matrix. The basis will actually be
broken down into two steps. To start, suppose <span class="math inline">\(y\)</span> is a vector
containing <span class="math inline">\(\binom{K}{2}\)</span> unconstrained values. These are first
transformed via the bijective function <span class="math inline">\(\tanh : \mathbb{R} \rightarrow (-1, 1)\)</span></p>
<p><span class="math display">\[
\tanh x = \frac{\exp(2x) - 1}{\exp(2x) + 1}.
\]</span></p>
<p>Then, define a <span class="math inline">\(K \times K\)</span> matrix <span class="math inline">\(z\)</span>, the upper triangular values of
which are filled by row with the transformed values. For example, in
the <span class="math inline">\(4 \times 4\)</span> case, there are <span class="math inline">\(\binom{4}{2}\)</span> values arranged as</p>
<p><span class="math display">\[
z
=
\left[
\begin{array}{cccc}
0 &amp; \tanh y_1 &amp; \tanh y_2 &amp; \tanh y_4
\\
0 &amp; 0 &amp; \tanh y_3 &amp; \tanh y_5
\\
0 &amp; 0 &amp; 0 &amp; \tanh y_6
\\
0 &amp; 0 &amp; 0 &amp; 0
\end{array}
\right]
.
\]</span></p>
<p>Lewandowski, Kurowicka and Joe (LKJ) show how to bijectively map the
array <span class="math inline">\(z\)</span> to a correlation matrix <span class="math inline">\(x\)</span>. The entry <span class="math inline">\(z_{i,j}\)</span> for <span class="math inline">\(i &lt; j\)</span> is interpreted as the canonical partial correlation (CPC) between
<span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, which is the correlation between <span class="math inline">\(i\)</span>’s residuals and
<span class="math inline">\(j\)</span>’s residuals when both <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are regressed on all variables
<span class="math inline">\(i&#39;\)</span> such that <span class="math inline">\(i&#39;&lt; i\)</span>. In the case of <span class="math inline">\(i=1\)</span>, there are no earlier
variables, so <span class="math inline">\(z_{1,j}\)</span> is just the Pearson correlation between <span class="math inline">\(i\)</span>
and <span class="math inline">\(j\)</span>.</p>
<p>In Stan, the LKJ transform is reformulated in terms of a Cholesky factor <span class="math inline">\(w\)</span>
of the final correlation matrix, defined for <span class="math inline">\(1 \leq i,j \leq K\)</span> by</p>
<p><span class="math display">\[
w_{i,j} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } i &gt; j,
\\
1 &amp; \mbox{if } 1 = i = j,
\\
\prod_{i&#39;=1}^{i - 1} \left( 1 - z_{i&#39;\!,\,j}^2 \right)^{1/2}
&amp; \mbox{if } 1 &lt; i = j,
\\
z_{i,j} &amp; \mbox{if } 1 = i &lt; j, \mbox{ and}
\\\
z_{i,j} \, \prod_{i&#39;=1}^{i-1} \left( 1 - z_{i&#39;\!,\,j}^2 \right)^{1/2}
&amp; \mbox{ if } 1 &lt; i &lt; j.
\end{array}
\right.
\]</span></p>
<p>This does not require as much computation per matrix entry as it may appear;
calculating the rows in terms of earlier rows yields the more
manageable expression</p>
<p><span class="math display">\[
w_{i,j} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } i &gt; j,
\\
1 &amp; \mbox{if } 1 = i = j,
\\
z_{i,j} &amp; \mbox{if } 1 = i &lt; j, \mbox{ and}
\\
z_{i,j} \ w_{i-1,j} \left( 1 - z_{i-1,j}^2 \right)^{1/2}
&amp; \mbox{ if } 1 &lt; i \leq j.
\end{array}
\right.
\]</span></p>
<p>Given the upper-triangular Cholesky factor <span class="math inline">\(w\)</span>, the final correlation
matrix is</p>
<p><span class="math display">\[
x = w^{\top} w.
\]</span></p>
<p><span class="citation">Lewandowski, Kurowicka, and Joe (<a href="#ref-LewandowskiKurowickaJoe:2009">2009</a>)</span> show that the determinant of the correlation
matrix can be defined in terms of the canonical partial correlations
as</p>
<p><span class="math display">\[
\mbox{det} \, x = \prod_{i=1}^{K-1} \ \prod_{j=i+1}^K \ (1 - z_{i,j}^2)
 = \prod_{1 \leq i &lt; j \leq K} (1 - z_{i,j}^2),
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-the-correlation-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of the Correlation Matrix Inverse Transform</h3>
<p>From the inverse of equation 11 in <span class="citation">(Lewandowski, Kurowicka, and Joe <a href="#ref-LewandowskiKurowickaJoe:2009">2009</a>)</span>,
the absolute Jacobian determinant is</p>
<p><span class="math display">\[
\sqrt{\prod_{i=1}^{K-1}\prod_{j=i+1}^K \left(1-z_{i,j}^2\right)^{K-i-1}} \
\times \prod_{i=1}^{K-1}\prod_{j=i+1}^K
\frac{\partial z_{i,j}}{\partial y_{i,j}}
\]</span></p>
</div>
<div id="correlation-matrix-transform" class="section level3 unnumbered">
<h3>Correlation Matrix Transform</h3>
<p>The correlation transform is defined by reversing the steps of the
inverse transform defined in the previous section.</p>
<p>Starting with a correlation matrix <span class="math inline">\(x\)</span>, the first step is to find the
unique upper triangular <span class="math inline">\(w\)</span> such that <span class="math inline">\(x = w w^{\top}\)</span>. Because <span class="math inline">\(x\)</span>
is positive definite, this can be done by applying the Cholesky
decomposition,</p>
<p><span class="math display">\[
w = \mbox{chol}(x).
\]</span></p>
<p>The next step from the Cholesky factor <span class="math inline">\(w\)</span> back to the array <span class="math inline">\(z\)</span> of
canonical partial correlations (CPCs) is simplified by the ordering of the elements in the
definition of <span class="math inline">\(w\)</span>, which when inverted yields</p>
<p><span class="math display">\[
z_{i,j} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } i \leq j,
\\
w_{i,j} &amp; \mbox{if } 1 = i &lt; j, \mbox{ and}
\\
{w_{i,j}}
\
\prod_{i&#39;=1}^{i-1} \left( 1 - z_{i&#39;\!,j}^2 \right)^{-1/2}
&amp; \mbox{if } 1 &lt; i &lt; j.
\end{array}
\right.
\]</span></p>
<p>The final stage of the transform reverses the hyperbolic tangent
transform, which is defined by</p>
<p><span class="math display">\[
\tanh^{-1} v = \frac{1}{2} \log \left( \frac{1 + v}{1 - v} \right).
\]</span></p>
<p>The inverse hyperbolic tangent function, <span class="math inline">\(\tanh^{-1}\)</span>, is also called
the Fisher transformation.</p>
</div>
</div>
<div id="covariance-matrices-1" class="section level2">
<h2><span class="header-section-number">10.10</span> Covariance Matrices</h2>
<p>A <span class="math inline">\(K \times K\)</span> matrix is a covariance matrix if it is symmetric and
positive definite (see the previous section for definitions). It
requires <span class="math inline">\(K + \binom{K}{2}\)</span> free parameters to specify a <span class="math inline">\(K \times K\)</span>
covariance matrix.</p>
<div id="covariance-matrix-transform" class="section level3 unnumbered">
<h3>Covariance Matrix Transform</h3>
<p>Stan’s covariance transform is based on a Cholesky decomposition
composed with a log transform of the positive-constrained diagonal
elements.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<p>If <span class="math inline">\(x\)</span> is a covariance matrix (i.e., a symmetric, positive definite
matrix), then there is a unique lower-triangular matrix <span class="math inline">\(z = \mathrm{chol}(x)\)</span> with positive diagonal entries, called a Cholesky
factor, such that</p>
<p><span class="math display">\[
x = z \, z^{\top}.
\]</span></p>
<p>The off-diagonal entries of the Cholesky factor <span class="math inline">\(z\)</span> are unconstrained,
but the diagonal entries <span class="math inline">\(z_{k,k}\)</span> must be positive for <span class="math inline">\(1 \leq k \leq K\)</span>.</p>
<p>To complete the transform, the diagonal is log-transformed to produce
a fully unconstrained lower-triangular matrix <span class="math inline">\(y\)</span> defined by</p>
<p><span class="math display">\[
y_{m,n} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } m &lt; n,
\\
\log z_{m,m} &amp; \mbox{if } m = n, \mbox{ and}
\\
z_{m,n} &amp; \mbox{if } m &gt; n.
\end{array}
\right.
\]</span></p>
</div>
<div id="covariance-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Covariance Matrix Inverse Transform</h3>
<p>The inverse transform reverses the two steps of the transform.
Given an unconstrained lower-triangular <span class="math inline">\(K \times K\)</span> matrix <span class="math inline">\(y\)</span>, the
first step is to recover the intermediate matrix <span class="math inline">\(z\)</span> by reversing the
log transform,</p>
<p><span class="math display">\[
z_{m,n} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } m &lt; n,
\\
\exp(y_{m,m}) &amp; \mbox{if } m = n, \mbox{ and}
\\
y_{m,n} &amp; \mbox{if } m &gt; n.
\end{array}
\right.
\]</span></p>
<p>The covariance matrix <span class="math inline">\(x\)</span> is recovered from its Cholesky factor <span class="math inline">\(z\)</span> by
taking</p>
<p><span class="math display">\[
x = z \, z^{\top}.
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-the-covariance-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of the Covariance Matrix Inverse Transform</h3>
<p>The Jacobian is the product of the Jacobians of the exponential
transform from the unconstrained lower-triangular matrix <span class="math inline">\(y\)</span> to matrix
<span class="math inline">\(z\)</span> with positive diagonals and the product transform from the
Cholesky factor <span class="math inline">\(z\)</span> to <span class="math inline">\(x\)</span>.</p>
<p>The transform from unconstrained <span class="math inline">\(y\)</span> to Cholesky factor <span class="math inline">\(z\)</span> has a
diagonal Jacobian matrix, the absolute determinant of which is thus</p>
<p><span class="math display">\[
\prod_{k=1}^K  \frac{\partial}{\partial_{y_{k,k}}} \, \exp(y_{k,k})
\ = \
\prod_{k=1}^K \exp(y_{k,k})
\ = \
\prod_{k=1}^K z_{k,k}.
\]</span></p>
<p>The Jacobian matrix of the second transform from the Cholesky factor <span class="math inline">\(z\)</span> to
the covariance matrix <span class="math inline">\(x\)</span> is also triangular, with diagonal entries
corresponding to pairs <span class="math inline">\((m,n)\)</span> with <span class="math inline">\(m \geq n\)</span>, defined by</p>
<p><span class="math display">\[
\frac{\partial}{\partial z_{m,n}}
\left( z \, z^{\top} \right)_{m,n}
\ = \
\frac{\partial}{\partial z_{m,n}}
\left( \sum_{k=1}^K z_{m,k} \, z_{n,k} \right)
\ = \
\left\{
\begin{array}{cl}
2 \, z_{n,n} &amp; \mbox{if } m = n \mbox{ and }
\\
z_{n,n} &amp; \mbox{if } m &gt; n.
\end{array}
\right.
\]</span></p>
<p>The absolute Jacobian determinant of the second transform is thus</p>
<p><span class="math display">\[
2^{K} \ \prod_{m = 1}^{K} \ \prod_{n=1}^{m} z_{n,n}
\ = \
\prod_{n=1}^K \ \prod_{m=n}^K z_{n,n}
\ = \
2^{K} \ \prod_{k=1}^K z_{k,k}^{K - k + 1}.
\]</span></p>
<p>Finally, the full absolute Jacobian determinant of the inverse
of the covariance matrix transform from the unconstrained lower-triangular
<span class="math inline">\(y\)</span> to a symmetric, positive definite matrix <span class="math inline">\(x\)</span> is the product of the
Jacobian determinants of the exponentiation and product transforms,</p>
<p><span class="math display">\[
\left( \prod_{k=1}^K z_{k,k} \right)
\left(
2^{K} \ \prod_{k=1}^K z_{k,k}^{K - k + 1}
\right)
\ = \
2^K
\, \prod_{k=1}^K z_{k,k}^{K-k+2}.
\]</span></p>
<p>Let <span class="math inline">\(f^{-1}\)</span> be the inverse transform from a <span class="math inline">\(K + \binom{K}{2}\)</span>-vector
<span class="math inline">\(y\)</span> to the <span class="math inline">\(K \times K\)</span> covariance matrix <span class="math inline">\(x\)</span>. A density function
<span class="math inline">\(p_X(x)\)</span> defined on <span class="math inline">\(K \times K\)</span> covariance matrices is transformed to
the density <span class="math inline">\(p_Y(y)\)</span> over <span class="math inline">\(K + \binom{K}{2}\)</span> vectors <span class="math inline">\(y\)</span> by</p>
<p><span class="math display">\[
p_Y(y) = p_X(f^{-1}(y)) \ 2^K \ \prod_{k=1}^K z_{k,k}^{K-k+2}.
\]</span></p>
</div>
</div>
<div id="cholesky-factors-of-covariance-matrices-1" class="section level2">
<h2><span class="header-section-number">10.11</span> Cholesky Factors of Covariance Matrices</h2>
<p>An <span class="math inline">\(M \times M\)</span> covariance matrix <span class="math inline">\(\Sigma\)</span> can be Cholesky factored to
a lower triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(L\,L^{\top} = \Sigma\)</span>. If
<span class="math inline">\(\Sigma\)</span> is positive definite, then <span class="math inline">\(L\)</span> will be <span class="math inline">\(M \times M\)</span>. If
<span class="math inline">\(\Sigma\)</span> is only positive semi-definite, then <span class="math inline">\(L\)</span> will be <span class="math inline">\(M \times N\)</span>,
with <span class="math inline">\(N &lt; M\)</span>.</p>
<p>A matrix is a Cholesky factor for a covariance matrix if and only if
it is lower triangular, the diagonal entries are positive, and <span class="math inline">\(M \geq N\)</span>. A matrix satisfying these conditions ensures that <span class="math inline">\(L \, L^{\top}\)</span> is positive semi-definite if <span class="math inline">\(M &gt; N\)</span> and positive definite
if <span class="math inline">\(M = N\)</span>.</p>
<p>A Cholesky factor of a covariance matrix requires <span class="math inline">\(N + \binom{N}{2} + (M - N)N\)</span> unconstrained parameters.</p>
<div id="cholesky-factor-of-covariance-matrix-transform" class="section level3 unnumbered">
<h3>Cholesky Factor of Covariance Matrix Transform</h3>
<p>Stan’s Cholesky factor transform only requires the first step of the
covariance matrix transform, namely log transforming the positive
diagonal elements. Suppose <span class="math inline">\(x\)</span> is an <span class="math inline">\(M \times N\)</span> Cholesky factor.
The above-diagonal entries are zero, the diagonal entries are
positive, and the below-diagonal entries are unconstrained. The
transform required is thus</p>
<p><span class="math display">\[
y_{m,n} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } m &lt; n,
\\
\log x_{m,m} &amp; \mbox{if } m = n, \mbox{ and}
\\
x_{m,n} &amp; \mbox{if } m &gt; n.
\end{array}
\right.
\]</span></p>
</div>
<div id="cholesky-factor-of-covariance-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Cholesky Factor of Covariance Matrix Inverse Transform</h3>
<p>The inverse transform need only invert the logarithm with an
exponentiation. If <span class="math inline">\(y\)</span> is the unconstrained matrix representation,
then the elements of the constrained matrix <span class="math inline">\(x\)</span> is defined by</p>
<p><span class="math display">\[
x_{m,n} =
\left\{
\begin{array}{cl}
0 &amp; \mbox{if } m &lt; n,
\\
\exp(y_{m,m}) &amp; \mbox{if } m = n, \mbox{ and}
\\
y_{m,n} &amp; \mbox{if } m &gt; n.
\end{array}
\right.
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-cholesky-factor-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of Cholesky Factor Inverse Transform</h3>
<p>The transform has a diagonal Jacobian matrix, the absolute determinant
of which is</p>
<p><span class="math display">\[
\prod_{n=1}^N  \frac{\partial}{\partial_{y_{n,n}}} \, \exp(y_{n,n})
\ = \
\prod_{n=1}^N \exp(y_{n,n})
\ = \
\prod_{n=1}^N x_{n,n}.
\]</span></p>
<p>Let <span class="math inline">\(x = f^{-1}(y)\)</span> be the inverse transform from a <span class="math inline">\(N + \binom{N}{2} + (M - N)N\)</span> vector to an <span class="math inline">\(M \times N\)</span> Cholesky factor for a covariance
matrix <span class="math inline">\(x\)</span> defined in the previous section. A density function
<span class="math inline">\(p_X(x)\)</span> defined on <span class="math inline">\(M \times N\)</span> Cholesky factors of covariance
matrices is transformed to the density <span class="math inline">\(p_Y(y)\)</span> over <span class="math inline">\(N + \binom{N}{2} + (M - N)N\)</span> vectors <span class="math inline">\(y\)</span> by</p>
<p><span class="math display">\[
p_Y(y) = p_X(f^{-1}(y)) \prod_{N=1}^N x_{n,n}.
\]</span></p>
</div>
</div>
<div id="cholesky-factors-of-correlation-matrices-1" class="section level2">
<h2><span class="header-section-number">10.12</span> Cholesky Factors of Correlation Matrices</h2>
<p>A <span class="math inline">\(K \times K\)</span> correlation matrix <span class="math inline">\(\Omega\)</span> is positive definite and
has a unit diagonal. Because it is positive definite, it can be
Cholesky factored to a <span class="math inline">\(K \times K\)</span> lower-triangular matrix <span class="math inline">\(L\)</span> with
positive diagonal elements such that <span class="math inline">\(\Omega = L\,L^{\top}\)</span>. Because
the correlation matrix has a unit diagonal,</p>
<p><span class="math display">\[
\Omega_{k,k} = L_k\,L_k^{\top} = 1,
\]</span></p>
<p>each row vector <span class="math inline">\(L_k\)</span> of the Cholesky factor is of unit length. The
length and positivity constraint allow the diagonal elements of <span class="math inline">\(L\)</span> to
be calculated from the off-diagonal elements, so that a Cholesky
factor for a <span class="math inline">\(K \times K\)</span> correlation matrix requires only
<span class="math inline">\(\binom{K}{2}\)</span> unconstrained parameters.</p>
<div id="cholesky-factor-of-correlation-matrix-inverse-transform" class="section level3 unnumbered">
<h3>Cholesky Factor of Correlation Matrix Inverse Transform</h3>
<p>It is easiest to start with the inverse transform from the <span class="math inline">\(\binom{K}{2}\)</span> unconstrained parameters <span class="math inline">\(y\)</span> to the <span class="math inline">\(K \times K\)</span> lower-triangular Cholesky factor <span class="math inline">\(x\)</span>. The inverse transform is based on the hyperbolic tangent function, <span class="math inline">\(\tanh\)</span>, which satisfies <span class="math inline">\(\tanh(x) \in (-1,1)\)</span>. Here it will function like an inverse logit with a sign to pick out the direction of an underlying canonical partial correlation; see the <a href="variable-transforms-chapter.html#correlation-matrix-transform.section">section on correlation matrix transforms</a> for more information on the relation between canonical partial correlations and the Cholesky factors of correlation matrices.</p>
<p>Suppose <span class="math inline">\(y\)</span> is a vector of <span class="math inline">\(\binom{K}{2}\)</span> unconstrained values. Let
<span class="math inline">\(z\)</span> be a lower-triangular matrix with zero diagonal and below
diagonal entries filled by row. For example, in the <span class="math inline">\(3 \times 3\)</span>
case,</p>
<p><span class="math display">\[
z =
\left[
\begin{array}{ccc}
0 &amp; 0 &amp; 0
\\
\tanh y_1 &amp; 0 &amp; 0
\\
\tanh y_2 &amp; \tanh y_3 &amp; 0
\end{array}
\right]
\]</span></p>
<p>The matrix <span class="math inline">\(z\)</span>, with entries in the range <span class="math inline">\((-1, 1)\)</span>, is then
transformed to the Cholesky factor <span class="math inline">\(x\)</span>, by taking<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
<p><span class="math display">\[
x_{i,j}
=
\left\{
\begin{array}{lll}
0 &amp; \mbox{ if } i &lt; j &amp; \mbox{ [above diagonal]}
\\
\sqrt{1 - \sum_{j&#39; &lt; j} x_{i,j&#39;}^2}
  &amp; \mbox{ if } i = j &amp; \mbox{ [on diagonal]}
\\
z_{i,j} \ \sqrt{1 - \sum_{j&#39; &lt; j} x_{i,j&#39;}^2}
  &amp; \mbox{ if } i &gt; j &amp; \mbox{ [below diagonal]}
\end{array}
\right.
\]</span></p>
<p>In the <span class="math inline">\(3 \times 3\)</span> case, this yields</p>
<p><span class="math display">\[
x =
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0
\\
z_{2,1} &amp; \sqrt{1 - x_{2,1}^2} &amp; 0
\\
z_{3,1} &amp; z_{3,2} \sqrt{1 - x_{3,1}^2}
        &amp; \sqrt{1 - (x_{3,1}^2 + x_{3,2}^2)}
\end{array}
\right],
\]</span></p>
<p>where the <span class="math inline">\(z_{i,j} \in (-1,1)\)</span> are the <span class="math inline">\(\tanh\)</span>-transformed <span class="math inline">\(y\)</span>.</p>
<p>The approach is a signed stick-breaking process on the quadratic
(Euclidean length) scale. Starting from length 1 at <span class="math inline">\(j=1\)</span>, each
below-diagonal entry <span class="math inline">\(x_{i,j}\)</span> is determined by the (signed) fraction
<span class="math inline">\(z_{i,j}\)</span> of the remaining length for the row that it consumes. The
diagonal entries <span class="math inline">\(x_{i,i}\)</span> get any leftover length from earlier
entries in their row. The above-diagonal entries are zero.</p>
</div>
<div id="cholesky-factor-of-correlation-matrix-transform" class="section level3 unnumbered">
<h3>Cholesky Factor of Correlation Matrix Transform</h3>
<p>Suppose <span class="math inline">\(x\)</span> is a <span class="math inline">\(K \times K\)</span> Cholesky factor for some correlation
matrix. The first step of the transform reconstructs the intermediate
values <span class="math inline">\(z\)</span> from <span class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[
z_{i,j} = \frac{x_{i,j}}{\sqrt{1 - \sum_{j&#39; &lt; j}x_{i,j&#39;}^2}}.
\]</span></p>
<p>The mapping from the resulting <span class="math inline">\(z\)</span> to <span class="math inline">\(y\)</span> inverts <span class="math inline">\(\tanh\)</span>,</p>
<p><span class="math display">\[
y
\ = \
\tanh^{-1} z
\ = \
\frac{1}{2} \left( \log (1 + z) - \log (1 - z) \right).
\]</span></p>
</div>
<div id="absolute-jacobian-determinant-of-inverse-transform" class="section level3 unnumbered">
<h3>Absolute Jacobian Determinant of Inverse Transform</h3>
<p>The Jacobian of the full transform is the product of the Jacobians of
its component transforms.</p>
<p>First, for the inverse transform <span class="math inline">\(z = \tanh y\)</span>, the derivative is</p>
<p><span class="math display">\[
\frac{d}{dy} \tanh y = \frac{1}{(\cosh y)^2}.
\]</span></p>
<p>Second, for the inverse transform of <span class="math inline">\(z\)</span> to <span class="math inline">\(x\)</span>, the resulting
Jacobian matrix <span class="math inline">\(J\)</span> is of dimension <span class="math inline">\(\binom{K}{2} \times \binom{K}{2}\)</span>, with indexes <span class="math inline">\((i,j)\)</span> for <span class="math inline">\((i &gt; j)\)</span>. The Jacobian
matrix is lower triangular, so that its determinant is the product of
its diagonal entries, of which there is one for each <span class="math inline">\((i,j)\)</span> pair,</p>
<p><span class="math display">\[
\left| \, \mbox{det} \, J \, \right|
  \ = \ \prod_{i &gt; j} \left| \frac{d}{dz_{i,j}} x_{i,j} \right|,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\frac{d}{dz_{i,j}} x_{i,j}
= \sqrt{1 - \sum_{j&#39; &lt; j} x^2_{i,j&#39;}}.
\]</span></p>
<p>So the combined density for unconstrained <span class="math inline">\(y\)</span> is</p>
<p><span class="math display">\[
p_Y(y)
= p_X(f^{-1}(y))
  \ \
  \prod_{n &lt; \binom{K}{2}} \frac{1}{(\cosh y)^2}
  \ \
  \prod_{i &gt; j} \left( 1 - \sum_{j&#39; &lt; j} x_{i,j&#39;}^2
  \right)^{1/2},
\]</span></p>
<p>where <span class="math inline">\(x = f^{-1}(y)\)</span> is used for notational convenience. The log
Jacobian determinant of the complete inverse transform <span class="math inline">\(x = f^{-1}(y)\)</span>
is given by</p>
<p><span class="math display">\[
\log \left| \, \det J \, \right|
=
-2 \sum_{n \leq \binom{K}{2}}
\log \cosh y
\
+
\
\frac{1}{2} \
\sum_{i &gt; j}
\log \left( 1 - \sum_{j&#39; &lt; j} x_{i,j&#39;}^2 \right)
.
\]</span></p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Marsaglia:1972">
<p>Marsaglia, George. 1972. “Choosing a Point from the Surface of a Sphere.” <em>The Annals of Mathematical Statistics</em> 43 (2): 645–46.</p>
</div>
<div id="ref-LewandowskiKurowickaJoe:2009">
<p>Lewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. “Generating Random Correlation Matrices Based on Vines and Extended Onion Method.” <em>Journal of Multivariate Analysis</em> 100: 1989–2001.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>For an alternative derivation of the same transform using hyperspherical coordinates, see <span class="citation">(Betancourt <a href="#ref-Betancourt:2010">2010</a>)</span>.<a href="variable-transforms-chapter.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>An alternative to the transform in this section, which can be coded directly in Stan, is to parameterize a covariance matrix as a scaled correlation matrix. An arbitrary <span class="math inline">\(K \times K\)</span> covariance matrix <span class="math inline">\(\Sigma\)</span> can be expressed in terms of a <span class="math inline">\(K\)</span>-vector <span class="math inline">\(\sigma\)</span> and correlation matrix <span class="math inline">\(\Omega\)</span> as <span class="math display">\[\Sigma = \mbox{diag}(\sigma) \times \Omega \times \mbox{diag}(\sigma),\]</span> so that each entry is just a deviation-scaled correlation, <span class="math display">\[\Sigma_{m,n} = \sigma_m \times \sigma_n \times \Omega_{m,n}.\]</span><a href="variable-transforms-chapter.html#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>For convenience, a summation with no terms, such as <span class="math inline">\(\sum_{j&#39; &lt; 1} x_{i,j&#39;}\)</span>, is defined to be 0. This implies <span class="math inline">\(x_{1,1} = 1\)</span> and that <span class="math inline">\(x_{i,1} = z_{i,1}\)</span> for <span class="math inline">\(i &gt; 1\)</span>.<a href="variable-transforms-chapter.html#fnref17" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="functions-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="language-syntax.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": false,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
