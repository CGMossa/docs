<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="custom-probability-functions-chapter.html">
<link rel="next" href="change-of-variables-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="7" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression Models</a></li>
<li class="chapter" data-level="8" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>8</b> Time-Series Models</a></li>
<li class="chapter" data-level="9" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>9</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="10" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>10</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="11" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>11</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="12" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>12</b> Finite Mixtures</a></li>
<li class="chapter" data-level="13" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>13</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="14" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>14</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="15" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>15</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="16" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>16</b> Clustering Models</a></li>
<li class="chapter" data-level="17" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>17</b> Gaussian Processes</a></li>
<li class="chapter" data-level="18" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>18</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="19" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>19</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="20" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>20</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="21" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>21</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="22" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>22</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="23" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>23</b> User-Defined Functions</a></li>
<li class="chapter" data-level="24" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>24</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="25" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>25</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="26" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>26</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="27" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>27</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="28" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>28</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="29" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>29</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="30" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>30</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>31</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="32" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>32</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="problematic-posteriors.chapter" class="section level1">
<h1><span class="header-section-number">25</span> Problematic Posteriors</h1>
<p>Mathematically speaking, with a proper posterior, one can do Bayesian
inference and that’s that. There is not even a need to require a
finite variance or even a finite mean—all that’s needed is a finite
integral. Nevertheless, modeling is a tricky business and even
experienced modelers sometimes code models that lead to improper
priors. Furthermore, some posteriors are mathematically sound, but
ill-behaved in practice. This chapter discusses issues in models that
create problematic posterior inferences, either in general for
Bayesian inference or in practice for Stan.</p>
<div id="collinearity.section" class="section level2">
<h2><span class="header-section-number">25.1</span> Collinearity of Predictors in Regressions</h2>
<p>This section discusses problems related to the classical notion of
identifiability, which lead to ridges in the posterior density and
wreak havoc with both sampling and inference.</p>
<div id="examples-of-collinearity" class="section level3 unnumbered">
<h3>Examples of Collinearity</h3>
<div id="redundant-intercepts" class="section level4 unnumbered">
<h4>Redundant Intercepts</h4>
<p>The first example of collinearity is an artificial example involving
redundant intercept parameters.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<p>Suppose there are observations <span class="math inline">\(y_n\)</span> for <span class="math inline">\(n \in 1{:}N\)</span>,
two intercept parameters <span class="math inline">\(\lambda_1\)</span> and
<span class="math inline">\(\lambda_2\)</span>, a scale parameter <span class="math inline">\(\sigma &gt; 0\)</span>, and the sampling distribution</p>
<p><span class="math display">\[
y_n \sim \mathsf{normal}(\lambda_1 + \lambda_2, \sigma).
\]</span></p>
<p>For any constant <span class="math inline">\(q\)</span>, the sampling density for <span class="math inline">\(y\)</span> does not change if
we add <span class="math inline">\(q\)</span> to <span class="math inline">\(\lambda_1\)</span> and subtract it from <span class="math inline">\(\lambda_2\)</span>, i.e.,</p>
<p><span class="math display">\[
p(y | \lambda_1, \lambda_2,\sigma)
=
p(y | \lambda_1 + q, \lambda_2 - q, \sigma).
\]</span></p>
<p>The consequence is that an improper uniform prior <span class="math inline">\(p(\mu,\sigma) \propto 1\)</span> leads to an improper posterior. This impropriety arises
because the neighborhoods around <span class="math inline">\(\lambda_1 + q, \lambda_2 - q\)</span> have
the same mass no matter what <span class="math inline">\(q\)</span> is. Therefore, a sampler would need
to spend as much time in the neighborhood of <span class="math inline">\(\lambda_1=1000000000\)</span>
and <span class="math inline">\(\lambda_2=-1000000000\)</span> as it does in the neighborhood of
<span class="math inline">\(\lambda_1=0\)</span> and <span class="math inline">\(\lambda_2=0\)</span>, and so on for ever more far-ranging
values.</p>
<p>The marginal posterior <span class="math inline">\(p(\lambda_1,\lambda_2|y)\)</span> for this model is
thus improper.<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a></p>
<p>The impropriety shows up visually as a ridge in the posterior density,
as illustrated in the left-hand plot. The ridge for this model is
along the line where <span class="math inline">\(\lambda_2 = \lambda_1 + c\)</span> for some constant
<span class="math inline">\(c\)</span>.</p>
<p>Contrast this model with a simple regression with a single intercept
parameter <span class="math inline">\(\mu\)</span> and sampling distribution
<span class="math display">\[
y_n \sim \mathsf{normal}(\mu,\sigma).
\]</span>
Even with an improper prior, the posterior is proper as long as there
are at least two data points <span class="math inline">\(y_n\)</span> with distinct values.</p>
</div>
<div id="ability-and-difficulty-in-irt-models" class="section level4 unnumbered">
<h4>Ability and Difficulty in IRT Models</h4>
<p>Consider an item-response theory model for students <span class="math inline">\(j \in 1{:}J\)</span> with
abilities <span class="math inline">\(\alpha_j\)</span> and test items <span class="math inline">\(i \in 1{:}I\)</span> with difficulties
<span class="math inline">\(\beta_i\)</span>. The observed data are an <span class="math inline">\(I \times J\)</span> array with entries
<span class="math inline">\(y_{i, j} \in \{ 0, 1 \}\)</span> coded such that <span class="math inline">\(y_{i, j} = 1\)</span> indicates that
student <span class="math inline">\(j\)</span> answered question <span class="math inline">\(i\)</span> correctly. The sampling
distribution for the data is</p>
<p><span class="math display">\[
y_{i, j} \sim \mathsf{Bernoulli}(\mbox{logit}^{-1}(\alpha_j - \beta_i)).
\]</span></p>
<p>For any constant <span class="math inline">\(c\)</span>, the probability of <span class="math inline">\(y\)</span> is unchanged by adding a
constant <span class="math inline">\(c\)</span> to all the abilities and subtracting it from all the
difficulties, i.e.,</p>
<p><span class="math display">\[
p(y | \alpha, \beta)
=
p(y | \alpha + c, \beta - c).
\]</span></p>
<p>This leads to a multivariate version of the ridge displayed by the
regression with two intercepts discussed above.</p>
</div>
<div id="general-collinear-regression-predictors" class="section level4 unnumbered">
<h4>General Collinear Regression Predictors</h4>
<p>The general form of the collinearity problem arises when predictors
for a regression are collinear. For example, consider a linear
regression sampling distribution
<span class="math display">\[
y_n \sim \mathsf{normal}(x_n \beta, \sigma)
\]</span>
for an <span class="math inline">\(N\)</span>-dimensional observation vector <span class="math inline">\(y\)</span>, an <span class="math inline">\(N \times K\)</span> predictor
matrix <span class="math inline">\(x\)</span>, and a <span class="math inline">\(K\)</span>-dimensional coefficient vector <span class="math inline">\(\beta\)</span>.</p>
<p>Now suppose that column <span class="math inline">\(k\)</span> of the predictor matrix is a multiple of
column <span class="math inline">\(k&#39;\)</span>, i.e., there is some constant <span class="math inline">\(c\)</span> such that <span class="math inline">\(x_{n,k} = c \, x_{n,k&#39;}\)</span> for all <span class="math inline">\(n\)</span>. In this case, the coefficients <span class="math inline">\(\beta_k\)</span>
and <span class="math inline">\(\beta_{k&#39;}\)</span> can covary without changing the predictions, so that
for any <span class="math inline">\(d \neq 0\)</span>,</p>
<p><span class="math display">\[
p(y | \ldots, \beta_k, \ldots, \beta_{k&#39;}, \ldots, \sigma)
=
p(y | \ldots, d  \beta_k, \ldots, \frac{d}{c} \, \beta_{k&#39;}, \ldots,
\sigma).
\]</span></p>
<p>Even if columns of the predictor matrix are not exactly collinear as
discussed above, they cause similar problems for inference if they are
nearly collinear.</p>
</div>
<div id="multiplicative-issues-with-discrimination-in-irt" class="section level4 unnumbered">
<h4>Multiplicative Issues with Discrimination in IRT</h4>
<p>Consider adding a discrimination parameter <span class="math inline">\(\delta_i\)</span> for each
question in an IRT model, with data sampling model
<span class="math display">\[
y_{i, j} \sim \mathsf{Bernoulli}(\mbox{logit}^{-1}(\delta_i(\alpha_j - \beta_i))).
\]</span>
For any constant <span class="math inline">\(c \neq 0\)</span>, multiplying <span class="math inline">\(\delta\)</span> by <span class="math inline">\(c\)</span> and dividing
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by <span class="math inline">\(c\)</span> produces the same likelihood,
<span class="math display">\[
p(y|\delta,\alpha,\beta)
= p(y|c \delta, \, \frac{1}{c}\alpha, \, \frac{1}{c}\beta).
\]</span>
If <span class="math inline">\(c &lt; 0\)</span>, this switches the signs of every component in <span class="math inline">\(\alpha\)</span>,
<span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\delta\)</span> without changing the density.</p>
</div>
<div id="softmax-with-k-vs.-k-1-parameters" class="section level4 unnumbered">
<h4>Softmax with <span class="math inline">\(K\)</span> vs. <span class="math inline">\(K-1\)</span> Parameters</h4>
<p>In order to parameterize a <span class="math inline">\(K\)</span>-simplex (i.e., a <span class="math inline">\(K\)</span>-vector with
non-negative values that sum to one), only <span class="math inline">\(K - 1\)</span> parameters are
necessary because the <span class="math inline">\(K\)</span>th is just one minus the sum of the first <span class="math inline">\(K - 1\)</span> parameters, so that if <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(K\)</span>-simplex,</p>
<p><span class="math display">\[
\theta_K = 1 - \sum_{k=1}^{K-1} \theta_k.
\]</span></p>
<p>The softmax function maps a <span class="math inline">\(K\)</span>-vector <span class="math inline">\(\alpha\)</span> of linear predictors
to a <span class="math inline">\(K\)</span>-simplex <span class="math inline">\(\theta = \mbox{softmax}(\alpha)\)</span> by defining</p>
<p><span class="math display">\[
\theta_k = \frac{\exp(\alpha_k)}{\sum_{k&#39;=1}^K \exp(\alpha_k&#39;)}.
\]</span></p>
<p>The softmax function is many-to-one, which leads to a lack of
identifiability of the unconstrained parameters <span class="math inline">\(\alpha\)</span>. In
particular, adding or subtracting a constant from each <span class="math inline">\(\alpha_k\)</span>
produces the same simplex <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="mitigating-the-invariances" class="section level3 unnumbered">
<h3>Mitigating the Invariances</h3>
<p>All of the examples discussed in the previous section allow
translation or scaling of parameters while leaving the data
probability density invariant. These problems can be mitigated in
several ways.</p>
<div id="removing-redundant-parameters-or-predictors" class="section level4 unnumbered">
<h4>Removing Redundant Parameters or Predictors</h4>
<p>In the case of the multiple intercepts, <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>,
the simplest solution is to remove the redundant intercept, resulting
in a model with a single intercept parameter <span class="math inline">\(\mu\)</span> and sampling
distribution <span class="math inline">\(y_n \sim \mathsf{normal}(\mu, \sigma)\)</span>. The same
solution works for solving the problem with collinearity—just remove
one of the columns of the predictor matrix <span class="math inline">\(x\)</span>.</p>
</div>
<div id="pinning-parameters" class="section level4 unnumbered">
<h4>Pinning Parameters</h4>
<p>The IRT model without a discrimination parameter can be fixed by
pinning one of its parameters to a fixed value, typically 0. For
example, the first student ability <span class="math inline">\(\alpha_1\)</span> can be fixed to 0. Now
all other student ability parameters can be interpreted as being
relative to student 1. Similarly, the difficulty parameters are
interpretable relative to student 1’s ability to answer them.</p>
<p>This solution is not sufficient to deal with the multiplicative
invariance introduced by the question discrimination parameters
<span class="math inline">\(\delta_i\)</span>. To solve this problem, one of the difficulty parameters,
say <span class="math inline">\(\delta_1\)</span>, must also be constrained. Because it’s a
multiplicative and not an additive invariance, it must be constrained
to a non-zero value, with 1 being a convenient choice. Now all of the
discrimination parameters may be interpreted relative to item 1’s
discrimination.</p>
<p>The many-to-one nature of <span class="math inline">\(\mbox{softmax}(\alpha)\)</span> is typically
mitigated by pinning a component of <span class="math inline">\(\alpha\)</span>, for instance fixing
<span class="math inline">\(\alpha_K = 0\)</span>. The resulting mapping is one-to-one from <span class="math inline">\(K-1\)</span>
unconstrained parameters to a <span class="math inline">\(K\)</span>-simplex. This is roughly how
simplex-constrained parameters are defined in Stan; see the reference
manual chapter on constrained parameter transforms for a precise
definition. The Stan code for creating a simplex from a <span class="math inline">\(K-1\)</span>-vector
can be written as</p>
<pre><code>vector softmax_id(vector alpha) {
  vector[num_elements(alpha) + 1] alphac1;
  for (k in 1:num_elements(alpha))
    alphac1[k] = alpha[k];
  alphac1[num_elements(alphac1)] = 0;
  return softmax(alphac1);
}</code></pre>
</div>
<div id="adding-priors" class="section level4 unnumbered">
<h4>Adding Priors</h4>
<p>So far, the models have been discussed as if the priors on the
parameters were improper uniform priors.</p>
<p>A more general Bayesian solution to these invariance problems is to
impose proper priors on the parameters. This approach can be used to
solve problems arising from either additive or multiplicative
invariance.</p>
<p>For example, normal priors on the multiple intercepts,
<span class="math display">\[
\lambda_1, \lambda_2 \sim \mathsf{normal}(0,\tau),
\]</span>
with a constant scale <span class="math inline">\(\tau\)</span>, ensure that the posterior mode is
located at a point where <span class="math inline">\(\lambda_1 = \lambda_2\)</span>, because this
minimizes <span class="math inline">\(\log \mathsf{normal}(\lambda_1|0,\tau) + \log \mathsf{normal}(\lambda_2|0,\tau)\)</span>.<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a></p>
<p>The following plots show the posteriors for two intercept
parameterization without prior, two intercept parameterization with
standard normal prior, and one intercept reparameterization without
prior. For all three cases, the posterior is plotted for 100 data
points drawn from a standard normal.</p>
<p>The two intercept parameterization leads to an improper prior with a ridge extending infinitely to the northwest and southeast.</p>
<div class="figure">
<img src="img/non-identified.png" alt="Two intercepts with improper prior" />
<p class="caption">Two intercepts with improper prior</p>
</div>
<p>Adding a standard normal prior for the intercepts results in a proper posterior.</p>
<div class="figure">
<img src="img/non-identified-plus-prior.png" alt="Two intercepts with proper prior" />
<p class="caption">Two intercepts with proper prior</p>
</div>
<p>The single intercept parameterization with no prior also has a proper posterior.</p>
<div class="figure">
<img src="img/one-param-identified.png" alt="Single intercepts with improper prior" />
<p class="caption">Single intercepts with improper prior</p>
</div>
<p>id:non-identifiable-density.figure</p>
<p>The addition of a prior to the two intercepts model is shown in the
second plot; the final plot shows the result of reparameterizing to a
single intercept.</p>
<p>An alternative strategy for identifying a <span class="math inline">\(K\)</span>-simplex parameterization
<span class="math inline">\(\theta = \mbox{softmax}(\alpha)\)</span> in terms of an unconstrained
<span class="math inline">\(K\)</span>-vector <span class="math inline">\(\alpha\)</span> is to place a prior on the components of <span class="math inline">\(\alpha\)</span>
with a fixed location (that is, specifically avoid hierarchical priors
with varying location). Unlike the approaching of pinning <span class="math inline">\(\alpha_K = 0\)</span>, the prior-based approach models the <span class="math inline">\(K\)</span> outcomes symmetrically
rather than modeling <span class="math inline">\(K-1\)</span> outcomes relative to the <span class="math inline">\(K\)</span>-th. The
pinned parameterization, on the other hand, is usually more efficient
statistically because it does not have the extra degree of (prior
constrained) wiggle room.</p>
</div>
<div id="vague-strongly-informative-and-weakly-informative-priors" class="section level4 unnumbered">
<h4>Vague, Strongly Informative, and Weakly Informative Priors</h4>
<p>Care must be used when adding a prior to resolve invariances. If the
prior is taken to be too broad (i.e., too vague), the resolution is in
theory only, and samplers will still struggle.</p>
<p>Ideally, a realistic prior will be formulated based on substantive
knowledge of the problem being modeled. Such a prior can be chosen to
have the appropriate strength based on prior knowledge. A strongly
informative prior makes sense if there is strong prior information.</p>
<p>When there is not strong prior information, a weakly informative prior
strikes the proper balance between controlling computational inference
without dominating the data in the posterior. In most problems, the
modeler will have at least some notion of the expected scale of the
estimates and be able to choose a prior for identification purposes
that does not dominate the data, but provides sufficient computational
control on the posterior.</p>
<p>Priors can also be used in the same way to control the additive
invariance of the IRT model. A typical approach is to place a strong
prior on student ability parameters <span class="math inline">\(\alpha\)</span> to control scale simply
to control the additive invariance of the basic IRT model and the
multiplicative invariance of the model extended with a item
discrimination parameters; such a prior does not add any prior
knowledge to the problem. Then a prior on item difficulty can be
chosen that is either informative or weakly informative based on prior
knowledge of the problem.</p>
</div>
</div>
</div>
<div id="label-switching-problematic.section" class="section level2">
<h2><span class="header-section-number">25.2</span> Label Switching in Mixture Models</h2>
<p>Where collinearity in regression models can lead to infinitely many
posterior maxima, swapping components in a mixture model leads to
finitely many posterior maxima.</p>
<div id="mixture-models" class="section level3 unnumbered">
<h3>Mixture Models</h3>
<p>Consider a normal mixture model with two location parameters <span class="math inline">\(\mu_1\)</span>
and <span class="math inline">\(\mu_2\)</span>, a shared scale <span class="math inline">\(\sigma &gt; 0\)</span>, a mixture ratio <span class="math inline">\(\theta \in [0,1]\)</span>, and likelihood
<span class="math display">\[
p(y|\theta,\mu_1,\mu_2,\sigma)
= \prod_{n=1}^N \big( \theta \, \mathsf{normal}(y_n|\mu_1,\sigma)
                       + (1 - \theta) \, \mathsf{normal}(y_n|\mu_2,\sigma) \big).
\]</span>
The issue here is exchangeability of the mixture components, because
<span class="math display">\[
p(\theta,\mu_1,\mu_2,\sigma|y) = p((1-\theta),\mu_2,\mu_1,\sigma|y).
\]</span>
The problem is exacerbated as the number of mixture components <span class="math inline">\(K\)</span>
grows, as in clustering models, leading to <span class="math inline">\(K!\)</span> identical posterior
maxima.</p>
</div>
<div id="convergence-monitoring-and-effective-sample-size" class="section level3 unnumbered">
<h3>Convergence Monitoring and Effective Sample Size</h3>
<p>The analysis of posterior convergence and effective sample size is
also difficult for mixture models. For example, the <span class="math inline">\(\hat{R}\)</span>
convergence statistic reported by Stan and the computation of
effective sample size are both compromised by label switching. The
problem is that the posterior mean, a key ingredient in these
computations, is affected by label switching, resulting in a posterior
mean for <span class="math inline">\(\mu_1\)</span> that is equal to that of <span class="math inline">\(\mu_2\)</span>, and a posterior
mean for <span class="math inline">\(\theta\)</span> that is always 1/2, no matter what the data are.</p>
</div>
<div id="some-inferences-are-invariant" class="section level3 unnumbered">
<h3>Some Inferences are Invariant</h3>
<p>In some sense, the index (or label) of a mixture component is
irrelevant. Posterior predictive inferences can still be carried out
without identifying mixture components. For example, the log
probability of a new observation does not depend on the identities of
the mixture components. The only sound Bayesian inferences in such
models are those that are invariant to label switching. Posterior
means for the parameters are meaningless because they are not
invariant to label switching; for example, the posterior mean for
<span class="math inline">\(\theta\)</span> in the two component mixture model will always be 1/2.</p>
</div>
<div id="highly-multimodal-posteriors" class="section level3 unnumbered">
<h3>Highly Multimodal Posteriors</h3>
<p>Theoretically, this should not present a problem for inference because
all of the integrals involved in posterior predictive inference will
be well behaved. The problem in practice is computation.</p>
<p>Being able to carry out such invariant inferences in practice is an
altogether different matter. It is almost always intractable to find
even a single posterior mode, much less balance the exploration of the
neighborhoods of multiple local maxima according to the probability
masses. In Gibbs sampling, it is unlikely for <span class="math inline">\(\mu_1\)</span>
to move to a new mode when sampled conditioned on the current values
of <span class="math inline">\(\mu_2\)</span> and <span class="math inline">\(\theta\)</span>. For HMC and NUTS, the problem is that the
sampler gets stuck in one of the two “bowls” around the modes and
cannot gather enough energy from random momentum assignment to move
from one mode to another.</p>
<p>Even with a proper posterior, all known sampling and inference
techniques are notoriously ineffective when the number of modes grows
super-exponentially as it does for mixture models with increasing
numbers of components.</p>
</div>
<div id="hacks-as-fixes" class="section level3 unnumbered">
<h3>Hacks as Fixes</h3>
<p>Several hacks (i.e., “tricks”) have been suggested and employed to
deal with the problems posed by label switching in practice.</p>
<div id="parameter-ordering-constraints" class="section level4 unnumbered">
<h4>Parameter Ordering Constraints</h4>
<p>One common strategy is to impose a constraint on the parameters that
identifies the components. For instance, we might consider
constraining <span class="math inline">\(\mu_1 &lt; \mu_2\)</span> in the two-component normal mixture model
discussed above. A problem that can arise from such an approach is
when there is substantial probability mass for the opposite ordering
<span class="math inline">\(\mu_1 &gt; \mu_2\)</span>. In these cases, the posteriors are affected by
the constraint and true posterior uncertainty in <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>
is not captured by the model with the constraint. In addition,
standard approaches to posterior inference for event probabilities is
compromised. For instance, attempting to use <span class="math inline">\(M\)</span> posterior samples to
estimate <span class="math inline">\(\mbox{Pr}[\mu_1 &gt; \mu_2]\)</span>, will fail, because the estimator
<span class="math display">\[
\mbox{Pr}[\mu_1 &gt; \mu_2]
\approx
\sum_{m=1}^M \mbox{I}(\mu_1^{(m)} &gt; \mu_2^{(m)})
\]</span>
will result in an estimate of 0 because the posterior respects the
constraint in the model.</p>
</div>
<div id="initialization-around-a-single-mode" class="section level4 unnumbered">
<h4>Initialization around a Single Mode</h4>
<p>Another common approach is to run a single chain or to initialize the
parameters near realistic values.<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<p>This can work better than the hard constraint approach if reasonable
initial values can be found and the labels do not switch within a
Markov chain. The result is that all chains are glued to a
neighborhood of a particular mode in the posterior.</p>
</div>
</div>
</div>
<div id="component-collapsing-in-mixture-models" class="section level2">
<h2><span class="header-section-number">25.3</span> Component Collapsing in Mixture Models</h2>
<p>It is possible for two mixture components in a mixture model to
collapse to the same values during sampling or optimization. For
example, a mixture of <span class="math inline">\(K\)</span> normals might devolve to have <span class="math inline">\(\mu_i = \mu_j\)</span> and <span class="math inline">\(\sigma_i = \sigma_j\)</span> for <span class="math inline">\(i \neq j\)</span>.</p>
<p>This will typically happen early in sampling due to initialization in
MCMC or optimization or arise from random movement during MCMC. Once
the parameters match for a given draw <span class="math inline">\((m)\)</span>, it can become hard to
escape because there can be a trough of low-density mass between the
current parameter values and the ones without collapsed components.</p>
<p>It may help to use a smaller step size during warmup, a stronger prior
on each mixture component’s membership responsibility. A more extreme
measure is to include additional mixture components to deal with the
possibility that some of them may collapse.</p>
<p>In general, it is difficult to recover exactly the right <span class="math inline">\(K\)</span>
mixture components in a mixture model as <span class="math inline">\(K\)</span> increases beyond one
(yes, even a two-component mixture can have this problem).</p>
</div>
<div id="posteriors-with-unbounded-densities" class="section level2">
<h2><span class="header-section-number">25.4</span> Posteriors with Unbounded Densities</h2>
<p>In some cases, the posterior density grows without bounds as
parameters approach certain poles or boundaries. In such, there
are no posterior modes and numerical stability issues can arise as
sampled parameters approach constraint boundaries.</p>
<div id="mixture-models-with-varying-scales" class="section level3 unnumbered">
<h3>Mixture Models with Varying Scales</h3>
<p>One such example is a binary mixture model with scales varying by
component, <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> for locations <span class="math inline">\(\mu_1\)</span> and
<span class="math inline">\(\mu_2\)</span>. In this situation, the density grows without bound as
<span class="math inline">\(\sigma_1 \rightarrow 0\)</span> and <span class="math inline">\(\mu_1 \rightarrow y_n\)</span> for some <span class="math inline">\(n\)</span>;
that is, one of the mixture components concentrates all of its mass
around a single data item <span class="math inline">\(y_n\)</span>.</p>
</div>
<div id="beta-binomial-models-with-skewed-data-and-weak-priors" class="section level3 unnumbered">
<h3>Beta-Binomial Models with Skewed Data and Weak Priors</h3>
<p>Another example of unbounded densities arises with a posterior such as
<span class="math inline">\(\mathsf{Beta}(\phi|0.5,0.5)\)</span>, which can arise if seemingly weak beta
priors are used for groups that have no data. This density is
unbounded as <span class="math inline">\(\phi \rightarrow 0\)</span> and <span class="math inline">\(\phi \rightarrow 1\)</span>. Similarly,
a Bernoulli likelihood model coupled with a “weak” beta prior, leads
to a posterior</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(\phi|y)
&amp; \propto &amp; \textstyle
\mathsf{Beta}(\phi|0.5,0.5) * \prod_{n=1}^N \mathsf{Bernoulli}(y_n|\phi)
\\[4pt]
&amp; = &amp;\textstyle
\mathsf{Beta}(\phi \, | \, 0.5 + \sum_{n=1}^N y_n, \ \ 0.5 + N - \sum_{n=1}^N y_n).
\end{array}
\]</span></p>
<p>If <span class="math inline">\(N = 9\)</span> and each <span class="math inline">\(y_n = 1\)</span>, the posterior is
<span class="math inline">\(\mathsf{Beta}(\phi|9.5,0,5)\)</span>. This posterior is unbounded as <span class="math inline">\(\phi \rightarrow 1\)</span>. Nevertheless, the posterior is proper, and although
there is no posterior mode, the posterior mean is well-defined with a
value of exactly 0.95.</p>
<div id="constrained-vs.unconstrained-scales" class="section level4 unnumbered">
<h4>Constrained vs. Unconstrained Scales</h4>
<p>Stan does not sample directly on the constrained <span class="math inline">\((0,1)\)</span> space for
this problem, so it doesn’t directly deal with unconstrained density
values. Rather, the probability values <span class="math inline">\(\phi\)</span> are logit-transformed
to <span class="math inline">\((-\infty,\infty)\)</span>. The boundaries at 0 and 1 are pushed out to
<span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span> respectively. The Jacobian adjustment that
Stan automatically applies ensures the unconstrained density is
proper. The adjustment for the particular case of <span class="math inline">\((0,1)\)</span> is <span class="math inline">\(\log \mbox{logit}^{-1}(\phi) + \log \mbox{logit}(1 - \phi)\)</span>.</p>
<p>There are two problems that still arise, though. The first is that if
the posterior mass for <span class="math inline">\(\phi\)</span> is near one of the boundaries, the
logit-transformed parameter will have to sweep out long paths and
thus can dominate the U-turn condition imposed by the no-U-turn
sampler (NUTS). The second issue is that the inverse transform from
the unconstrained space to the constrained space can underflow to 0 or
overflow to 1, even when the unconstrained parameter is not infinite.
Similar problems arise for the expectation terms in logistic
regression, which is why the logit-scale parameterizations of the
Bernoulli and binomial distributions are more stable.</p>
</div>
</div>
</div>
<div id="posteriors-with-unbounded-parameters" class="section level2">
<h2><span class="header-section-number">25.5</span> Posteriors with Unbounded Parameters</h2>
<p>In some cases, the posterior density will not grow without bound, but
parameters will grow without bound with gradually increasing density
values. Like the models discussed in the previous section that have
densities that grow without bound, such models also have no posterior
modes.</p>
<div id="separability-in-logistic-regression" class="section level3 unnumbered">
<h3>Separability in Logistic Regression</h3>
<p>Consider a logistic regression model with <span class="math inline">\(N\)</span> observed outcomes <span class="math inline">\(y_n \in \{ 0, 1 \}\)</span>, an <span class="math inline">\(N \times K\)</span> matrix <span class="math inline">\(x\)</span> of predictors, a
<span class="math inline">\(K\)</span>-dimensional coefficient vector <span class="math inline">\(\beta\)</span>, and sampling distribution
<span class="math display">\[
y_n \sim \mathsf{Bernoulli}(\mbox{logit}^{-1}(x_n \beta)).
\]</span>
Now suppose that column <span class="math inline">\(k\)</span> of the predictor matrix is such that
<span class="math inline">\(x_{n,k} &gt; 0\)</span> if and only if <span class="math inline">\(y_n = 1\)</span>, a condition known as
``separability.&quot; In this case, predictive accuracy on the observed data
continue to improve as <span class="math inline">\(\beta_k \rightarrow \infty\)</span>, because for cases
with <span class="math inline">\(y_n = 1\)</span>, <span class="math inline">\(x_n \beta \rightarrow \infty\)</span> and hence
<span class="math inline">\(\mbox{logit}^{-1}(x_n \beta) \rightarrow 1\)</span>.</p>
<p>With separability, there is no maximum to the likelihood and hence no
maximum likelihood estimate. From the Bayesian perspective, the
posterior is improper and therefore the marginal posterior mean for
<span class="math inline">\(\beta_k\)</span> is also not defined. The usual solution to this problem in
Bayesian models is to include a proper prior for <span class="math inline">\(\beta\)</span>, which
ensures a proper posterior.</p>
</div>
</div>
<div id="uniform-posteriors" class="section level2">
<h2><span class="header-section-number">25.6</span> Uniform Posteriors</h2>
<p>Suppose your model includes a parameter <span class="math inline">\(\psi\)</span> that is defined on
<span class="math inline">\([0,1]\)</span> and is given a flat prior <span class="math inline">\(\mathsf{Uniform}(\psi|0,1)\)</span>. Now if
the data don’t tell us anything about <span class="math inline">\(\psi\)</span>, the posterior is also
<span class="math inline">\(\mathsf{Uniform}(\psi|0,1)\)</span>.</p>
<p>Although there is no maximum likelihood estimate for <span class="math inline">\(\psi\)</span>, the
posterior is uniform over a closed interval and hence proper. In the
case of a uniform posterior on <span class="math inline">\([0,1]\)</span>, the posterior mean for <span class="math inline">\(\psi\)</span>
is well-defined with value <span class="math inline">\(1/2\)</span>. Although there is no posterior
mode, posterior predictive inference may nevertheless do the right
thing by simply integrating (i.e., averaging) over the predictions for
<span class="math inline">\(\psi\)</span> at all points in <span class="math inline">\([0,1]\)</span>.</p>
</div>
<div id="sampling-difficulties-with-problematic-priors" class="section level2">
<h2><span class="header-section-number">25.7</span> Sampling Difficulties with Problematic Priors</h2>
<p>With an improper posterior, it is theoretically impossible to properly
explore the posterior. However, Gibbs sampling as performed by BUGS
and JAGS, although still unable to properly sample from such an
improper posterior, behaves differently in practice than the
Hamiltonian Monte Carlo sampling performed by Stan when faced with an
example such as the two intercept model discussed in
the <a href="problematic-posteriors-chapter.html#collinearity.section">collinearity section</a> and illustrated in
the non-identifiable density plot.</p>
<div id="gibbs-sampling" class="section level3 unnumbered">
<h3>Gibbs Sampling</h3>
<p>Gibbs sampling, as performed by BUGS and JAGS, may appear to be
efficient and well behaved for this unidentified model, but as
discussed in the previous subsection, will not actually explore the
posterior properly.</p>
<p>Consider what happens with initial values <span class="math inline">\(\lambda_1^{(0)}, \lambda_2^{(0)}\)</span>.
Gibbs sampling proceeds in iteration <span class="math inline">\(m\)</span> by drawing</p>
<p><span class="math display">\[
\begin{array}{rcl}
\lambda_1^{(m)}
&amp; \sim &amp; p(\lambda_1 \, | \, \lambda_2^{(m-1)}, \, \sigma^{(m-1)}, \, y)
\\[6pt]
\lambda_2^{(m)}
&amp; \sim &amp; p(\lambda_2 \, | \, \lambda_1^{(m)}, \, \sigma^{(m-1)}, \, y)
\\[6pt]
\sigma^{(m)}
&amp; \sim &amp; p(\sigma \, | \, \lambda_1^{(m)}, \, \lambda_2^{(m)}, \, y).
\end{array}
\]</span></p>
<p>Now consider the draw for <span class="math inline">\(\lambda_1\)</span> (the draw for <span class="math inline">\(\lambda_2\)</span> is
symmetric), which is conjugate in this model and thus can be done
efficiently. In this model, the range from which the next <span class="math inline">\(\lambda_1\)</span>
can be drawn is highly constrained by the current values of
<span class="math inline">\(\lambda_2\)</span> and <span class="math inline">\(\sigma\)</span>. Gibbs will run quickly and provide
seemingly reasonable inferences for <span class="math inline">\(\lambda_1 + \lambda_2\)</span>. But it
will not explore the full range of the posterior; it will merely take
a slow random walk from the initial values. This random walk behavior
is typical of Gibbs sampling when posteriors are highly correlated and
the primary reason to prefer Hamiltonian Monte Carlo to Gibbs sampling
for models with parameters correlated in the posterior.</p>
</div>
<div id="hamiltonian-monte-carlo-sampling" class="section level3 unnumbered">
<h3>Hamiltonian Monte Carlo Sampling</h3>
<p>Hamiltonian Monte Carlo (HMC), as performed by Stan, is much more
efficient at exploring posteriors in models where parameters are
correlated in the posterior. In this particular example, the
Hamiltonian dynamics (i.e., the motion of a fictitious particle given
random momentum in the field defined by the negative log posterior) is
going to run up and down along the valley defined by the potential
energy (ridges in log posteriors correspond to valleys in potential
energy). In practice, even with a random momentum for <span class="math inline">\(\lambda_1\)</span> and
<span class="math inline">\(\lambda_2\)</span>, the gradient of the log posterior is going to adjust for
the correlation and the simulation will run <span class="math inline">\(\lambda_1\)</span> and
<span class="math inline">\(\lambda_2\)</span> in opposite directions along the valley corresponding to
the ridge in the posterior log density.</p>
</div>
<div id="no-u-turn-sampling" class="section level3 unnumbered">
<h3>No-U-Turn Sampling</h3>
<p>Stan’s default no-U-turn sampler (NUTS), is even more efficient at
exploring the posterior; see
<span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2011">2011</a>)</span> and <span class="citation">Hoffman and Gelman (<a href="#ref-Hoffman-Gelman:2014">2014</a>)</span>. NUTS simulates the
motion of the fictitious particle representing the parameter values
until it makes a U-turn, it will be defeated in most cases, as it will
just move down the potential energy valley indefinitely without making
a U-turn. What happens in practice is that the maximum number of
leapfrog steps in the simulation will be hit in many of the
iterations, causing a large number of log probability and
gradient evaluations (1000 if the max tree depth is set to 10, as in
the default). Thus sampling will appear to be slow. This is
indicative of an improper posterior, not a bug in the NUTS algorithm
or its implementation. It is simply not possible to sample from an
improper posterior! Thus the behavior of HMC in general and NUTS
in particular should be reassuring in that it will clearly fail in
cases of improper posteriors, resulting in a clean diagnostic of
sweeping out large paths in the posterior.</p>
<p>Here are results of Stan runs with default parameters fit to
<span class="math inline">\(N=100\)</span> data points generated from <span class="math inline">\(y_n \sim \mathsf{normal}(0,1)\)</span>:</p>
<p><em>Two Scale Parameters, Improper Prior</em></p>
<pre><code>Inference for Stan model: improper_stan
Warmup took (2.7, 2.6, 2.9, 2.9) seconds, 11 seconds total
Sampling took (3.4, 3.7, 3.6, 3.4) seconds, 14 seconds total

                  Mean     MCSE   StdDev        5%       95%  N_Eff  N_Eff/s  R_hat
lp__          -5.3e+01  7.0e-02  8.5e-01  -5.5e+01  -5.3e+01    150       11    1.0
n_leapfrog__   1.4e+03  1.7e+01  9.2e+02   3.0e+00   2.0e+03   2987      212    1.0
lambda1        1.3e+03  1.9e+03  2.7e+03  -2.3e+03   6.0e+03    2.1     0.15    5.2
lambda2       -1.3e+03  1.9e+03  2.7e+03  -6.0e+03   2.3e+03    2.1     0.15    5.2
sigma          1.0e+00  8.5e-03  6.2e-02   9.5e-01   1.2e+00     54      3.9    1.1
mu             1.6e-01  1.9e-03  1.0e-01  -8.3e-03   3.3e-01   2966      211    1.0</code></pre>
<p><em>Two Scale Parameters, Weak Prior</em></p>
<pre><code>Warmup took (0.40, 0.44, 0.40, 0.36) seconds, 1.6 seconds total
Sampling took (0.47, 0.40, 0.47, 0.39) seconds, 1.7 seconds total

                 Mean     MCSE   StdDev        5%    95%  N_Eff  N_Eff/s  R_hat
lp__              -54  4.9e-02  1.3e+00  -5.7e+01    -53    728      421    1.0
n_leapfrog__      157  2.8e+00  1.5e+02   3.0e+00    511   3085     1784    1.0
lambda1          0.31  2.8e-01  7.1e+00  -1.2e+01     12    638      369    1.0
lambda2         -0.14  2.8e-01  7.1e+00  -1.2e+01     12    638      369    1.0
sigma             1.0  2.6e-03  8.0e-02   9.2e-01    1.2    939      543    1.0
mu               0.16  1.8e-03  1.0e-01  -8.1e-03   0.33   3289     1902    1.0</code></pre>
<p><em>One Scale Parameter, Improper Prior</em></p>
<pre><code>Warmup took (0.011, 0.012, 0.011, 0.011) seconds, 0.044 seconds total
Sampling took (0.017, 0.020, 0.020, 0.019) seconds, 0.077 seconds total

                Mean     MCSE  StdDev        5%   50%   95%  N_Eff  N_Eff/s  R_hat
lp__             -54  2.5e-02    0.91  -5.5e+01   -53   -53   1318    17198    1.0
n_leapfrog__     3.2  2.7e-01     1.7   1.0e+00   3.0   7.0     39      507    1.0
mu              0.17  2.1e-03    0.10  -3.8e-03  0.17  0.33   2408    31417    1.0
sigma            1.0  1.6e-03   0.071   9.3e-01   1.0   1.2   2094    27321    1.0</code></pre>
<p>On the top is the non-identified model with improper uniform priors
and likelihood <span class="math inline">\(y_n \sim \mathsf{normal}(\lambda_1 + \lambda_2,  \sigma)\)</span>.</p>
<p>In the middle is the same likelihood as the middle plus priors
<span class="math inline">\(\lambda_k \sim \mathsf{normal}(0,10)\)</span>.</p>
<p>On the bottom is an identified model with an improper prior, with
likelihood <span class="math inline">\(y_n \sim \mathsf{normal}(\mu,\sigma)\)</span>. All models
estimate <span class="math inline">\(\mu\)</span> at roughly 0.16 with low Monte Carlo standard
error, but a high posterior standard deviation of 0.1; the true
value <span class="math inline">\(\mu=0\)</span> is within the 90% posterior intervals in all three models.</p>
<p>id:non-identified-stan-fits.figure</p>
</div>
<div id="examples-fits-in-stan" class="section level3 unnumbered">
<h3>Examples: Fits in Stan</h3>
<p>To illustrate the issues with sampling from non-identified and only
weakly identified models, we fit three models with increasing degrees
of identification of their parameters. The posteriors for these
models is illustrated in the non-identifiable density plot. The
first model is the unidentified model with two location parameters and
no priors discussed in the <a href="problematic-posteriors-chapter.html#collinearity.section">collinearity section</a>.</p>
<pre><code>data {
  int N;
  real y[N];
}
parameters {
  real lambda1;
  real lambda2;
  real&lt;lower=0&gt; sigma;
}
transformed parameters {
  real mu;
  mu = lambda1 + lambda2;
}
model {
  y ~ normal(mu, sigma);
}</code></pre>
<p>The second adds priors to the model block for <code>lambda1</code> and
<code>lambda2</code> to the previous model.</p>
<pre><code>  lambda1 ~ normal(0, 10);
  lambda2 ~ normal(0, 10);</code></pre>
<p>The third involves a single location parameter, but no priors.</p>
<pre><code>data {
  int N;
  real y[N];
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(mu, sigma);
}</code></pre>
<p>All three of the example models were fit in Stan 2.1.0 with default
parameters (1000 warmup iterations, 1000 sampling iterations, NUTS
sampler with max tree depth of 10). The results are shown in the
non-identified fits figure. The key statistics from these outputs are
the following.</p>
<ul>
<li><p>As indicated by <code>R_hat</code> column, all parameters have
converged other than <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> in the
non-identified model.</p></li>
<li><p>The average number of leapfrog steps is roughly 3 in
the identified model, 150 in the model identified by a weak prior, and
1400 in the non-identified model.</p></li>
<li><p>The number of effective samples per
second for <span class="math inline">\(\mu\)</span> is roughly 31,000 in the identified model, 1900 in the model
identified with weakly informative priors, and 200 in the
non-identified model; the results are similar for <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>In the non-identified model, the 95% interval for <span class="math inline">\(\lambda_1\)</span> is
(-2300,6000), whereas it is only (-12,12) in the model identified with
weakly informative priors.</p></li>
<li><p>In all three models, the simulated value of <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>
are well within the posterior 90% intervals.</p></li>
</ul>
<p>The first two points, lack of convergence and hitting the maximum
number of leapfrog steps (equivalently maximum tree depth) are
indicative of improper posteriors. Thus rather than covering up the
problem with poor sampling as may be done with Gibbs samplers,
Hamiltonian Monte Carlo tries to explore the posterior and its failure
is a clear indication that something is amiss in the model.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Hoffman-Gelman:2011">
<p>Hoffman, Matthew D., and Andrew Gelman. 2011. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>arXiv</em> 1111.4246. <a href="http://arxiv.org/abs/1111.4246" class="uri">http://arxiv.org/abs/1111.4246</a>.</p>
</div>
<div id="ref-Hoffman-Gelman:2014">
<p>Hoffman, Matthew D., and Andrew Gelman. 2011. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>arXiv</em> 1111.4246. <a href="http://arxiv.org/abs/1111.4246" class="uri">http://arxiv.org/abs/1111.4246</a>.</p> 2014. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>Journal of Machine Learning Research</em> 15: 1593–1623. <a href="http://jmlr.org/papers/v15/hoffman14a.html" class="uri">http://jmlr.org/papers/v15/hoffman14a.html</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="39">
<li id="fn39"><p>This example was raised by Richard McElreath on the Stan users group in a query about the difference in behavior between Gibbs sampling as used in BUGS and JAGS and the Hamiltonian Monte Carlo (HMC) and no-U-turn samplers (NUTS) used by Stan.<a href="problematic-posteriors-chapter.html#fnref39" class="footnote-back">↩</a></p></li>
<li id="fn40"><p>The marginal posterior <span class="math inline">\(p(\sigma|y)\)</span> for <span class="math inline">\(\sigma\)</span> is proper here as long as there are at least two distinct data points.<a href="problematic-posteriors-chapter.html#fnref40" class="footnote-back">↩</a></p></li>
<li id="fn41"><p>A Laplace prior (or an L1 regularizer for penalized maximum likelihood estimation) is not sufficient to remove this additive invariance. It provides shrinkage, but does not in and of itself identify the parameters because adding a constant to <span class="math inline">\(\lambda_1\)</span> and subtracting it from <span class="math inline">\(\lambda_2\)</span> results in the same value for the prior density.<a href="problematic-posteriors-chapter.html#fnref41" class="footnote-back">↩</a></p></li>
<li id="fn42"><p>Tempering methods may be viewed as automated ways to carry
out such a search for modes, though most MCMC tempering methods
continue to search for modes on an ongoing basis; see
<span class="citation">(Swendsen and Wang <a href="#ref-SwendsenWang:1986">1986</a>; Neal <a href="#ref-Neal:1996b">1996</a><a href="#ref-Neal:1996b">b</a>)</span>.<a href="problematic-posteriors-chapter.html#fnref42" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="custom-probability-functions-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="change-of-variables-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
