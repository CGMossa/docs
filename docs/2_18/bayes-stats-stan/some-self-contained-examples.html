<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="prior-distributions-and-models-for-data.html">
<link rel="next" href="workflow-in-action.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="7" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression Models</a></li>
<li class="chapter" data-level="8" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>8</b> Time-Series Models</a></li>
<li class="chapter" data-level="9" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>9</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="10" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>10</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="11" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>11</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="12" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>12</b> Finite Mixtures</a></li>
<li class="chapter" data-level="13" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>13</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="14" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>14</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="15" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>15</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="16" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>16</b> Clustering Models</a></li>
<li class="chapter" data-level="17" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>17</b> Gaussian Processes</a></li>
<li class="chapter" data-level="18" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>18</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="19" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>19</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="20" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>20</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="21" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>21</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="22" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>22</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="23" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>23</b> User-Defined Functions</a></li>
<li class="chapter" data-level="24" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>24</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="25" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>25</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="26" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>26</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="27" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>27</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="28" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>28</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="29" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>29</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="30" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>30</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>31</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="32" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>32</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="some-self-contained-examples" class="section level1">
<h1><span class="header-section-number">4</span> Some Self-Contained Examples</h1>
<div id="what-is-in-this-chapter" class="section level2">
<h2><span class="header-section-number">4.1</span> What is in this chapter?</h2>
<p>We introduce Bayesian data analysis and Stan through a set of examples
that are small but have aspects of real problems. Here are the
examples we use in this chapter:</p>
<ul>
<li><p>Golf putting. A simple nonlinear regression model demonstrating the
flexibility of Stan to fit arbitrarily specified models, and
demonstrating the advantages of thoughtful modeling applied to real
data.</p></li>
<li><p>Movie ratings. A fake-data example of an item-response problem. We
build up the model in different ways and demonstrate how to use
fake-data simulation.</p></li>
<li><p>World Cup. A simple item-response model fit to real data with an
external predictor, thus two data sources to be combined. One
feature of this example is that the first model we fit had a bug
which took us some work to find out.</p></li>
<li><p>Sex ratio. A Bayesian analysis of a notorious example from the
biology literature in which headlines were obtained from what was
essentially pure noise. This is a good example of the importance of
prior information when data are weak, and because the problem is so
simple, with only one parameter of interest, we can show graphically
how the posterior distribution is a compromise between the prior
distribution and the likelihood.</p></li>
<li><p>Time series competition. A simple example in which we push through
all the steps from data processing, through Bayesian modeling and
inference, to decision analysis. This example also demonstrates how
to fit a mixture model in Stan.</p></li>
<li><p>Declining exponential. A nonlinear model that comes up in many
science and engineering applications, also illsutrating the use of
bounds in parameter estimation.</p></li>
<li><p>Sum of declining exponentials. An important class of nonlinear
models that can be surprisingly difficult to estimate from data.
This example illustrates how Stan can fail, and also the way in
which default prior information can add stability to an otherwise
intractable problem.</p></li>
</ul>
<p>We perform all these computations in R and Stan—processing data in
R, fitting models in Stan, and using R for postprocessing of
inferences and graphics—but we suppress the R code, as our focus is
on the Stan programming. You can go to the raw knitr document (the
.Rmd file) in order to see the R code and reproduce all these
analyses.</p>
</div>
<div id="golf-putting" class="section level2">
<h2><span class="header-section-number">4.2</span> Golf putting</h2>
<p>The following graph shows data on the proportion of successful golf
putts as a function of distance from the hole. Unsurprisingly, the
probability of making the shot declines as a function of distance:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The error bars associated with each point <span class="math inline">\(j\)</span> in the above graph are
simple binomial standard deviations,
<span class="math inline">\(\sqrt{\hat{p}_j(1-\hat{p}_j)/n_j}\)</span>, where <span class="math inline">\(\hat{p_j}=y_j/n_j\)</span> is the
success rate for putts taken at distance <span class="math inline">\(x_j\)</span>.</p>
<div id="logistic-regression" class="section level3 unnumbered">
<h3>Logistic regression</h3>
<p>Can we model the probability of success in golf putting as a function
of distance from the hole? Given usual statistical practice, the
natural starting point would be logistic regression:</p>
<p><span class="math display">\[
y_j\sim\mbox{binomial}(n_j, \mbox{logit}^{-1}(a + bx_j)),
\mbox{ for } j=1,\dots, J.
\]</span></p>
<p>In Stan, this is:</p>
<pre><code>data {
  int J;
  int n[J];
  vector[J] x;
  int y[J];
}
parameters {
  real a;
  real b;
}
model {
  y ~ binomial_logit(n, a + b*x);
}</code></pre>
<p>Here is the result of fitting this model to the data:</p>
<pre><code>Inference for Stan model: golf_logistic.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd     2.5%      25%      50%      75%      98%
a        2.23    0.00 0.06     2.12     2.19     2.23     2.27     2.35
b       -0.26    0.00 0.01    -0.27    -0.26    -0.26    -0.25    -0.24
lp__ -3021.18    0.03 1.03 -3023.96 -3021.57 -3020.87 -3020.45 -3020.18
     n_eff Rhat
a     1126    1
b     1112    1
lp__  1322    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:08:28 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Stan has computed the posterior means <span class="math inline">\(\pm\)</span> standard deviations of <span class="math inline">\(a\)</span>
and <span class="math inline">\(b\)</span> to be <span class="math inline">\(2.23\pm 0.06\)</span> and <span class="math inline">\(-0.26\pm 0.01\)</span>, respectively. The
Monte Carlo standard error of the mean of each of these parameters is
0 (to two decimal places), indicating that the simulations have run
long enough to estimate the posterior means precisely. The posterior
quantiles give a sense of the uncertainty in the parameters, with 50%
posterior intervals of <span class="math inline">\([2.19,2.27]\)</span> and <span class="math inline">\([-0.26,-0.25]\)</span> for <span class="math inline">\(a\)</span> and
<span class="math inline">\(b\)</span>, respectively. Finally, the values of <span class="math inline">\(\widehat{R}\)</span> near 1 tell
us that the simulations from Stan’s four simulated chains have mixed
well.</p>
<p>The following graph shows the fit plotted along with the data:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The thick line shows the fit corresponding to the posterior median
estimates of the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>; the light lines show 20 draws
from the posterior distribution.</p>
</div>
<div id="modeling-from-first-principles" class="section level3 unnumbered">
<h3>Modeling from first principles</h3>
<p>As an alternative to logistic regression, we shall build a model from
first principles and fit it to the data. The graph below shows a
simplified sketch of a golf shot. The dotted line represents the
angle within which the ball of radius <span class="math inline">\(r\)</span> must be hit so that it falls
within the hole of radius <span class="math inline">\(R\)</span>. This threshold angle is
<span class="math inline">\(\sin^{-1}((R-r)/x)\)</span>.</p>
<div class="figure">
<img src="img/golfpicture.png" alt="Golf diagram" />
<p class="caption">Golf diagram</p>
</div>
<p>The next step is to model human error. We assume that the golfer is
attempting to hit the ball completely straight but that many small
factors interfere with this goal, so that the actual angle follows a
normal distribution centered at 0 with some standard deviation
<span class="math inline">\(\sigma\)</span>.</p>
<p>The probability the ball goes in the hole is then the probability that
the angle is less than the threshold; that is,
<span class="math inline">\(2\Phi(\sin^{-1}((R-r)/x)) - 1\)</span>, where <span class="math inline">\(\Phi\)</span> is the cumulative normal
distribution function.</p>
<p>Our model then has two parts:
<span class="math display">\[y_j \sim \mbox{binomial}(n_j, p_j)\]</span>
<span class="math display">\[p_j = 2\Phi(\sin^{-1}((R-r)/x)) - 1 , \mbox{ for } j=1,\dots, J.\]</span></p>
<p>Here is the model in Stan:</p>
<pre><code>data {
  int J;
  int n[J];
  vector[J] x;
  int y[J];
  real r;
  real R;
}
parameters {
  real&lt;lower=0&gt; sigma;
}
model {
  vector[J] p;
  for (j in 1:J){
    p[j] = 2*Phi(asin((R-r)/x[j]) / sigma) - 1;
  }
  y ~ binomial(n, p);
}
generated quantities {
  real sigma_degrees;
  sigma_degrees = (180/pi())*sigma;
}</code></pre>
<p>The data <span class="math inline">\(J,n,x,y\)</span> have already been set up; we just need to define
<span class="math inline">\(r\)</span> and <span class="math inline">\(R\)</span> (the golf ball and hole have diameters 1.68 and 4.25
inches, respectively), and run the Stan model:</p>
<p>Here is the result:</p>
<pre><code>Inference for Stan model: golf1.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

                  mean se_mean   sd     2.5%      25%      50%      75%
sigma             0.03    0.00 0.00     0.03     0.03     0.03     0.03
sigma_degrees     1.53    0.00 0.02     1.48     1.51     1.53     1.54
lp__          -2926.77    0.02 0.72 -2928.87 -2926.93 -2926.48 -2926.31
                   98% n_eff Rhat
sigma             0.03  1320    1
sigma_degrees     1.57  1320    1
lp__          -2926.26  1600    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:09:05 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The model has a single parameter, <span class="math inline">\(\sigma\)</span>. From the output, we find
that Stan has computed the posterior mean of <span class="math inline">\(\sigma\)</span> to be 0.0267
(multiplying this by <span class="math inline">\(180/\pi\)</span>, this comes to 1.5 degrees). The Monte
Carlo standard error of the mean is 0 (to four decimal places),
indicating that the simulations have run long enough to estimate the
posterior mean precisely. The posterior standard deviation is
calculated at 0.0004 (that is, 0.02 degrees), indicating that <span class="math inline">\(\sigma\)</span>
itself has been estimated with high precision, which makes sense given
the large number of data points and the simplicity of the model. The
precise posterior distribution of <span class="math inline">\(\sigma\)</span> can also be seen from the
narrow range of the posterior quantiles. Finally, <span class="math inline">\(\widehat{R}\)</span> is
near 1, telling us that the simulations from Stan’s four simulated
chains have mixed well.</p>
<p>We next plot the data and the fitted model (here using the posterior
median of <span class="math inline">\(\sigma\)</span> but in this case the uncertainty is so narrow that
any reasonable posterior summary would give essentially the same
result), along with the logistic regression fitted earlier:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The custom nonlinear model fits the data much better. This is not to
say that the model is perfect—any experience of golf will reveal
that the angle is not the only factor determining whether the ball
goes in the hole—but it seems like a useful start, and it is good to
know that we can fit nonlinear models by just coding them up in Stan.</p>
</div>
</div>
<div id="movie-ratings" class="section level2">
<h2><span class="header-section-number">4.3</span> Movie ratings</h2>
<p>Consider the following scenario. You are considering which of two
movies to go see. Both have average online ratings of 4 out of 5
stars, but one is based on 2 ratings and the other is based on 100.
Which movie should you go see?</p>
<p>We will set this up as a statistics problem, making the assumption
that you would prefer to see the movie that is most preferred, on
average, by others. That is, we can imagine every movie having a
“true popularity” which would be the average rating, if everyone in
the population were to see this movie and rate it on a 0–5 scale. We
are thus implicitly assuming that these two movies are aimed at the
same target audience, which includes you.</p>
<p>We further assume that each of these movies has been rated by a random
sample of people from this general audience: thus, we can consider the
observed average rating of 4.0 in each case to be an estimate of the
true popularity of each movie. At the end of this example, we will
discuss how to relax these assumptions.</p>
<p>At first you might think that since both movies have average ratings
of 4.0, it’s a tossup which one to see.</p>
<p>But the estimate based on 2 ratings should be much less accurate than
the estimate based on 100 ratings. If you really want to see a 4-star
movie, the one with 100 ratings should be a safer bet.</p>
<p>We’ll now make this point using Bayesian inference, going through the
following steps:</p>
<ul>
<li><p>Constructing data fitting the above story and fitting a Bayesian
model to get inferences about the true popularity of the two movies.</p></li>
<li><p>Embedding this all in a larger problem with many movies being rated.</p></li>
<li><p>Setting up a model in which different people rate different movies,
with systematic differences between who rates which movies.</p></li>
<li><p>Espanding the model to allow for the possibility that more people
see, and rate, movies that are more popular.</p></li>
</ul>
<p>For each step, we set up a model, simulate fake data from that model,
and check that we can recover the underlying parameters to some level
of accuracy.</p>
<div id="model-for-2-movies" class="section level3 unnumbered">
<h3>Model for 2 movies</h3>
<p>Define <span class="math inline">\(\theta_j, j=1,2\)</span> to be the true popularities of the two
movies, with any randomly selected rating for movie <span class="math inline">\(j\)</span> being drawn
from a normal distribution with mean <span class="math inline">\(\theta_j\)</span> and standard deviation
<span class="math inline">\(\sigma\)</span>.</p>
<p>For simplicity, we’re pretending radings are continuous unbounded
numbers (rather than integers from 0 through 5) and that the
distributions of ratings for the teo movies differ only in their mean,
not their variance. We’re not allowing, for example, a polarizing
movie that you either love or hate. The ratings distribution for any
movie is assumed to be bell-shaped. Again, we can later go back and
expand our models to allow other possibilities.</p>
<p>In our initial example, we have 102 ratings: 2 for one movie and 100
for the other. We label the individual ratings as
<span class="math inline">\(y_i,i=1,\dots,N=102\)</span> and index the movies being rated as <span class="math inline">\(j[i]\)</span>,
where <span class="math inline">\(j[1]=j[2]=1\)</span> and <span class="math inline">\(j[3]=j[4]=\dots=j[102]=2\)</span>. We can then write
our model as, <span class="math display">\[y_i \sim \mbox{normal}(\theta_{j[i]}, \sigma),
i=1,\dots,N.\]</span> In statistics notation such as used in <em>Bayesian Data
Analysis</em>, the normal distribution is written in terms of mean and
variance, or squared standard deviation: <span class="math inline">\(y \sim \mbox{normal}(\mu,\sigma^2)\)</span>. But in this book we will follow Stan
notation and write <span class="math inline">\(y \sim \mbox{normal}(\mu,\sigma)\)</span>.</p>
<p>To perform Bayesian inference we also need a prior distribution on all
the parameters, which in this case are <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and
<span class="math inline">\(\sigma\)</span>.</p>
<p>First, the parameters have some mathematical constraints. The
available ratings are 0 through 5, so each <span class="math inline">\(\theta\)</span> must fall in that
range as well, and <span class="math inline">\(\sigma\)</span> cannot be more than 2.5, and that is the
maximum standard deviation among all distributions defined in the
range <span class="math inline">\([0,5]\)</span> (with that maximum attained with the distribution with
all its mass at the extremes, with half the responses at 0 and half at
5).</p>
<p>Beyond this, we will assume that movies typically get ratings of
around three stars. We’ll give normal prior distributions with mean 3
and standard deviation 1 to each of the parameters <span class="math inline">\(\theta_1\)</span> and
<span class="math inline">\(\theta_2\)</span>, which represent the underlying or population-average
ratings of the two movies.</p>
<p>Here is the resulting Stan model:</p>
<pre><code>data {
  int N;
  vector[N] y;
  int&lt;lower=1, upper=2&gt; movie[N];
}
parameters {
  vector&lt;lower=0, upper=5&gt;[2] theta;
  real&lt;lower=0, upper=2.5&gt; sigma_y;
}
model {
  theta ~ normal(3, 1);
  y ~ normal(theta[movie], sigma_y);
  /* equivalently:
    for (j in 1:2){
      theta[j] ~ normal(3, 1);
    }
    for (n in 1:N){
      y[n] ~ normal(theta[movie[n]], sigma_y);
    }
  */
}</code></pre>
<p>Near the bottom of the above program, the lines blocked off with a
slash and star are comments, indicating how the vectorization in the
Stan language allows us to assign an entire set of probability
distributions at once.</p>
<p>We next construct data for our two movies. For movie 1, we suppose
the two ratings are 3 and 5, which give the posited average rating of
4. For movie 2, we suppose the ratings are 10 2’s, 20 3’s, 30 4’s,
and 40 5’s, which again average to 4. This set of hypothetical
ratings does <em>not</em> follow the assumed normal distribution, but that’s
fine; we can still fit the model.</p>
<p>Here is the result:</p>
<pre><code>Inference for Stan model: ratings_1.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd   2.5%    25%   50%   75%   98% n_eff Rhat
theta[1]   3.6    0.01 0.56   2.50   3.25   3.6   4.0   4.7  2787    1
theta[2]   4.0    0.00 0.11   3.78   3.92   4.0   4.1   4.2  3309    1
sigma_y    1.0    0.00 0.07   0.89   0.97   1.0   1.1   1.2  3042    1
lp__     -54.2    0.04 1.40 -57.94 -54.89 -53.9 -53.2 -52.7  1456    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:09:42 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The estimated population average popularity is 3.6 for the first movie
and 4.0 for the second. We hae more information on the second movie,
so we can more precisely estimate it as truly meriting 4 stars in the
public assessment: the 50% posterior interval for <span class="math inline">\(\theta_2\)</span> is <span class="math inline">\([3.9, 4.1]\)</span>. In contrast, we have a lot more uncertainty about <span class="math inline">\(\theta_1\)</span>,
which according to our model has a 50% probability of being in the
range <span class="math inline">\([3.2, 4.0]\)</span> but could be as low as 2.5 or as high as 4.7.</p>
<p>So, which movie to see? It depends on your goals. Movie 2 is a safer
bet, with a higher estimated quality. But Movie 1 has a small chance
of being outstanding, along with a moderate chance of being mediocre.
You can see Movie 1 if you want to roll the dice.</p>
</div>
<div id="estending-the-model-to-j-movies" class="section level3 unnumbered">
<h3>Estending the model to J movies</h3>
<p>It is easy to expand the Stan program to allow an arbitrary number of movies:</p>
<pre><code>data {
  int N;
  vector[N] y;
  int J;
  int&lt;lower=1, upper=J&gt; movie[N];
}
parameters {
  vector&lt;lower=0, upper=5&gt;[J] theta;
  real&lt;lower=0, upper=2.5&gt; sigma_y;
}
model {
  theta ~ normal(3, 1);
  y ~ normal(theta[movie], sigma_y);
}</code></pre>
<p>As usual, to really understand this model, it helps to be able to
simulate fake data. Let’s set <span class="math inline">\(J=40\)</span> movies and let the number of
times each movie is rated be a random number, uniformly distributed
between 0 and 100. We start by drawing parameters <span class="math inline">\(\theta_j\)</span> from a
normal distribution with mean 3 and standard deviation 0.5 (so that
the movies’ true popularities are mostly between 2 and 4 on that 0–5
scale), and then sample continuous individual ratings <span class="math inline">\(z_i\)</span> for each
movie <span class="math inline">\(j[i]\)</span> from a normal distribution with mean <span class="math inline">\(\theta_{j[i]}\)</span> and
standard deviation 2: <span class="math display">\[ \theta_j \sim \mbox{normal}(3.0, 0.5), \mbox{
for } j=1,\dots,J \\ z_i \sim \mbox{normal}(\theta_{j[i]}, 2.0),
\mbox{ for } i=1,\dots,n.  \]</span> To keep things simple, we will ignore
the fact that the ratings are constrained to be between 0 and 5;
instead, we will simulate continuous ratings on an unbounded scale.
It is not difficult in Stan to model ordered discrete responses; see
Chapter **. Here, though, we will ignore that feature of the data.</p>
<p>We simulate the fake data in R and then fit the new model in Stan.</p>
<p>Here is the result:</p>
<pre><code>Inference for Stan model: ratings_2.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd     2.5%     25%     50%     75%     98%
theta[1]      3.3    0.00 0.23     2.80     3.1     3.3     3.4     3.7
theta[2]      3.2    0.00 0.23     2.75     3.0     3.2     3.4     3.7
theta[3]      3.6    0.01 0.43     2.79     3.3     3.6     3.9     4.5
theta[4]      2.3    0.01 0.43     1.45     2.0     2.3     2.6     3.2
theta[5]      2.5    0.00 0.21     2.06     2.4     2.5     2.6     2.9
theta[6]      3.0    0.00 0.22     2.60     2.9     3.0     3.2     3.5
theta[7]      3.0    0.01 0.77     1.44     2.5     3.0     3.5     4.5
theta[8]      2.4    0.00 0.22     1.94     2.2     2.4     2.5     2.8
theta[9]      2.1    0.00 0.21     1.68     1.9     2.1     2.2     2.5
theta[10]     3.4    0.00 0.25     2.91     3.2     3.4     3.6     3.9
theta[11]     3.9    0.00 0.27     3.31     3.7     3.9     4.0     4.4
theta[12]     2.7    0.00 0.30     2.10     2.5     2.7     2.9     3.3
theta[13]     2.7    0.00 0.24     2.20     2.5     2.7     2.8     3.1
theta[14]     2.4    0.00 0.24     1.92     2.2     2.4     2.5     2.9
theta[15]     2.9    0.01 0.37     2.21     2.7     2.9     3.2     3.7
theta[16]     2.4    0.00 0.21     1.93     2.2     2.4     2.5     2.8
theta[17]     2.9    0.01 0.44     1.98     2.5     2.8     3.1     3.7
theta[18]     3.3    0.00 0.25     2.81     3.1     3.3     3.5     3.8
theta[19]     2.5    0.00 0.22     2.04     2.3     2.5     2.6     2.9
theta[20]     2.3    0.00 0.34     1.62     2.1     2.3     2.5     3.0
theta[21]     3.6    0.00 0.23     3.13     3.4     3.6     3.7     4.0
theta[22]     2.3    0.00 0.33     1.66     2.1     2.3     2.5     3.0
theta[23]     2.6    0.01 0.36     1.89     2.4     2.6     2.9     3.3
theta[24]     3.3    0.00 0.20     2.90     3.2     3.3     3.4     3.7
theta[25]     3.0    0.00 0.24     2.52     2.9     3.0     3.2     3.5
theta[26]     3.4    0.00 0.21     2.99     3.3     3.4     3.5     3.8
theta[27]     2.8    0.01 0.44     1.92     2.5     2.8     3.1     3.6
theta[28]     3.0    0.00 0.20     2.59     2.9     3.0     3.1     3.4
theta[29]     2.2    0.00 0.23     1.71     2.0     2.2     2.3     2.6
theta[30]     2.8    0.01 0.49     1.85     2.5     2.8     3.1     3.8
theta[31]     2.5    0.00 0.20     2.07     2.3     2.5     2.6     2.8
theta[32]     3.4    0.00 0.19     2.96     3.2     3.4     3.5     3.7
theta[33]     2.2    0.00 0.28     1.69     2.0     2.2     2.4     2.8
theta[34]     3.6    0.00 0.32     2.93     3.4     3.6     3.8     4.2
theta[35]     3.5    0.01 0.42     2.66     3.2     3.5     3.8     4.3
theta[36]     3.4    0.01 0.40     2.56     3.1     3.4     3.6     4.1
theta[37]     3.2    0.00 0.30     2.60     3.0     3.2     3.4     3.8
theta[38]     2.7    0.01 0.88     0.96     2.1     2.8     3.4     4.4
theta[39]     2.9    0.00 0.32     2.21     2.6     2.9     3.1     3.5
theta[40]     3.0    0.00 0.24     2.59     2.9     3.0     3.2     3.5
sigma_y       2.0    0.00 0.03     1.95     2.0     2.0     2.0     2.1
lp__      -2544.5    0.12 4.87 -2554.77 -2547.6 -2544.2 -2541.1 -2535.9
          n_eff Rhat
theta[1]   5086    1
theta[2]   5115    1
theta[3]   5290    1
theta[4]   5307    1
theta[5]   6113    1
theta[6]   5823    1
theta[7]   5449    1
theta[8]   5332    1
theta[9]   4728    1
theta[10]  5411    1
theta[11]  4770    1
theta[12]  5733    1
theta[13]  5218    1
theta[14]  6083    1
theta[15]  5329    1
theta[16]  5109    1
theta[17]  5117    1
theta[18]  5730    1
theta[19]  5811    1
theta[20]  6658    1
theta[21]  5521    1
theta[22]  5267    1
theta[23]  4690    1
theta[24]  5042    1
theta[25]  5328    1
theta[26]  6411    1
theta[27]  5580    1
theta[28]  4640    1
theta[29]  4833    1
theta[30]  4542    1
theta[31]  4821    1
theta[32]  5379    1
theta[33]  5271    1
theta[34]  5392    1
theta[35]  5237    1
theta[36]  5252    1
theta[37]  5148    1
theta[38]  5164    1
theta[39]  5053    1
theta[40]  4689    1
sigma_y    5860    1
lp__       1657    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:10:21 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>We can check the fit by plotting the posterior inferences against the true parameter values:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Roughly half the 50% intervals and 95% of the 95% intervals contain
the true parameter value, which is about what we would expect to see,
given that we have simulated data from the model we are fitting.</p>
<p>The intervals in the above graph vary in width. The more data we have
for any given movie, the more precise is our estimate of its
underlying popularity. Here is a graph showing the width of the 50%
interval as a function of sample size:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-19-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The only reason the points do not completely fall along a smooth curve
here is that the intervals are computed using simulation. Here we
have 4000 simulation draws. If we were to run Stan longer and obtain
more iterations, then the resulting intervals would be more stable,
and the above graph would look smoother. For all practical purposes,
though, we have enough simulations and enough iterations. For
example, if <span class="math inline">\(\theta_1\)</span> has a posterior 50% interval of <span class="math inline">\([3.1, 3.5]\)</span>,
there is no real reason to get a huge number of simulations and to
find out that the precise interval is <span class="math inline">\([3.13, 3.48]\)</span>, as this makes no
real difference in our understanding of <span class="math inline">\(\theta_3\)</span>, nor should it
seriously affect any decision we might want to make using these data.</p>
</div>
<div id="item-response-model-with-parameters-for-raters-and-for-movies" class="section level3 unnumbered">
<h3>Item-response model with parameters for raters and for movies</h3>
<p>There are many ways to extend the above model. To start with, we can
recognize that different people rate different movies, and each rater
uses his or own scale. Suppose, for example, that some people tend to
give high ratings and others tend to give low ratings, and the sorts
of people who give high ratings are more likely to watch romantic
comedies, while the tougher judges more frequently watch crime movies.
Then a simple comparison of average ratings will be unfair to the
crime movies, as this does not take into account systematic
differences between raters.</p>
<p>We can model rater effects using what is called an <em>item-response
model</em>, the simplest form of which looks like this, for a numerical
rating <span class="math inline">\(y_i\)</span> of movie <span class="math inline">\(j[i]\)</span> by rater <span class="math inline">\(k[i]\)</span>,</p>
<p><span class="math display">\[
y_i \sim \mbox{normal}(a_{j[i]} - b_{k[i]}, \sigma_y),
\]</span>
Here, <span class="math inline">\(a_j\)</span> is a parameter that could be said to represent movie
“quality,” corresponding to the average rating that movie <span class="math inline">\(j\)</span> would
receive, if it were reviewed by average raters. The parameter <span class="math inline">\(b_k\)</span>
represents the “difficulty” of rater <span class="math inline">\(k\)</span>: higher values of <span class="math inline">\(b_k\)</span>
correspond to raters who give tougher judgments of equivalent movies.</p>
<p>The previous model is equivalent to this new model with all the
<span class="math inline">\(b_k\)</span>’s fixed at zero. When fitting the new model, we constrain the
<span class="math inline">\(b_k\)</span>’s to come from a distribution whose average is zero. Some such
constraint is necessary because otherwise the model would not be
<em>identified</em>: for example, you could add 100 to each of the <span class="math inline">\(a_j\)</span>’s
and <span class="math inline">\(-100\)</span> to each of the <span class="math inline">\(b_k\)</span>’s and not change any of the predictors
at all.</p>
<p>Our full model looks like this:</p>
<p><span class="math display">\[
y_i \sim \mbox{normal}(a_{j[i]} - b_{k[i]}, \sigma_y),
  \mbox{ for } i=1,\dots, N
\\
a_j \sim \mbox{normal}(\mu_a, \sigma_a), \mbox{ for } j=1,\dots, J
\\
b_k \sim \mbox{normal}(0, \sigma_b), \mbox{ for } k=1,\dots, K.
\]</span></p>
<p>We also need prior distributions for the as-yet-unmodeled parameters
<span class="math inline">\(\mu_a, \sigma_a, \sigma_b, \sigma_y\)</span>.</p>
<p>But before setting that up, we re-express the model in a way that will
be generally useful:</p>
<p><span class="math display">\[
y_i
  \sim \mbox{normal}(\mu + \sigma_a*\alpha_{j[i]} - \sigma_b*\beta_{k[i]},
                     \sigma_y),
  \mbox{ for } i=1,\dots, N
\\
\alpha_j \sim \mbox{normal}(0, 1),
  \mbox{ for } j=1,\dots, J
\\
\beta_k \sim \mbox{normal}(0, 1),
  \mbox{ for } k=1,\dots, K.
\]</span>
This new version, sometimes called the <em>non-centered
parameterization</em>, is convenient because it separates the scaled and
unscaled parameters; also it can have certain computational
advantages, as discussed here: [point to Mike Betancourt’s case
study?]. The new models are equivalent, with the movle quality
parameters being expressed as, <span class="math inline">\(a_j = \mu + \sigma_{\alpha}*\alpha_j\)</span>.</p>
<p>And now we can add prior distributions. We will start with uniform
priors (subject to the constraint that <span class="math inline">\(\sigma_a\)</span>, <span class="math inline">\(\sigma_b\)</span>, and
<span class="math inline">\(\sigma_y\)</span> must all be positive), adding in prior information later if
the data are weak enough that this seems necessary.</p>
<p>Here is the Stan program:</p>
<pre><code>data {
  int N;
  vector[N] y;
  int J;
  int K;
  int&lt;lower=1, upper=J&gt; movie[N];
  int&lt;lower=1, upper=K&gt; rater[N];
}
parameters {
  vector[J] alpha;
  vector[K] beta;
  real mu;
  real&lt;lower=0&gt; sigma_a;
  real&lt;lower=0&gt; sigma_b;
  real&lt;lower=0&gt; sigma_y;
}
transformed parameters {
  vector[J] a;
  a = mu + sigma_a * alpha;
}
model {
  y ~ normal(mu + sigma_a * alpha[movie] + sigma_b * beta[rater], sigma_y);
  alpha ~ normal(0, 1);
  beta ~ normal(0, 1);
}</code></pre>
<p>As usual, we’ll check it by simulating fake data, then fitting the
model in Stan and checking that the parameters are recovered.</p>
<p>We’ll start by simulating data from <span class="math inline">\(J=40\)</span> movkes and <span class="math inline">\(K=100\)</span> raters,
with each person rating each movie, and with parameters <span class="math inline">\(\mu=3\)</span> (thus,
an average rating of 3 for all movies and all raters), <span class="math inline">\(\sigma_a=0.5\)</span>,
<span class="math inline">\(\sigma_b=0.5\)</span> (thus, the same amount of variation in raters’
difficulties than in the quality of movies) and <span class="math inline">\(\sigma_y=2\)</span>, as
before.</p>
<p>Here is part of the Stan fit. We just display the hyperparameters, to
save space omitting the parameter vectors <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> and the
transformed parameter vector <span class="math inline">\(a\)</span>.</p>
<pre><code>Inference for Stan model: ratings_3.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

        mean se_mean   sd 2.5%  25%  50%  75%  98% n_eff Rhat
mu      2.89       0 0.10 2.69 2.82 2.88 2.95 3.08   868    1
sigma_a 0.52       0 0.07 0.40 0.47 0.52 0.57 0.68  1180    1
sigma_b 0.36       0 0.05 0.27 0.32 0.36 0.39 0.45  1996    1
sigma_y 2.02       0 0.02 1.97 2.01 2.02 2.04 2.07  7515    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:12:04 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Stan did fine recovering the variance parameters. The mean level
<span class="math inline">\(\mu\)</span> is more difficult to nail down, but the true value of 3.0 is
within the range of posterior uncertainty.</p>
<p>Let’s check the coverage for the <span class="math inline">\(\alpha\)</span>’s and <span class="math inline">\(\beta\)</span>’s:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-23-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now let’s put our model to more of a challenge by giving it unbalanced
data. Let’s divide the movies into two groups: romantic comedies
(movies <span class="math inline">\(j=1,\dots,20\)</span>) and crime stories (movies <span class="math inline">\(j=21,\dots,40\)</span>),
and set things up so that the more difficult reviewers (those with
positive values of <span class="math inline">\(\alpha_k\)</span>) are more likely to review crime
stories.</p>
<p>We’ll set up the simulation as follows. Each of the <span class="math inline">\(K\)</span> people might
rate each of the <span class="math inline">\(J\)</span> movies. If <span class="math inline">\(\beta_k&gt;0\)</span>, then person <span class="math inline">\(k\)</span> will have
a 30% chance of rating each romantic comedy and a 60% chance of rating
each crime movie. If <span class="math inline">\(\beta_k&lt;0\)</span>, then the probabilities are
reversed, and person <span class="math inline">\(k\)</span> has a 30% chance of rating each romantic
comedy and a 60% chance of rating each crime movie. So, in the data,
we’ll expect to see tougher reviews on the crime stories.</p>
<p>We simulate this model, using the same ratings as before but just
selecting a subset according to the above-defined probabilities.</p>
<p>We then fit the model in Stan.</p>
<pre><code>Inference for Stan model: ratings_3.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

        mean se_mean   sd 2.5%  25%  50%  75%  98% n_eff Rhat
mu      2.89       0 0.10 2.70 2.82 2.89 2.96 3.08   815    1
sigma_a 0.52       0 0.07 0.40 0.47 0.52 0.57 0.68  1246    1
sigma_b 0.36       0 0.05 0.27 0.33 0.36 0.39 0.46  1937    1
sigma_y 2.02       0 0.02 1.98 2.00 2.02 2.04 2.07  6937    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:57:15 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>And display as before. In the left graph we use open circles for the
romantic comedies (those with <span class="math inline">\(j=1,\dots,10\)</span>) and solid circles for
the crime movies (<span class="math inline">\(j=11,\dots,20\)</span>). In the right graph we use open
circles for the nice reviewers (those with <span class="math inline">\(\beta_k&lt;0\)</span>) and solid
circles for the difficult reviewers (<span class="math inline">\(\beta_k&lt;0\)</span>). Coverage still
seems fine, which it should be—again, we ran our simulations under
the model that we later fit—but it is still gratifying to see, as a
confirmation that we are not making any obvious mistakes.</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-26-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We next do some analysis to show how naive averaging of ratings will
give misleading estimates of movie quality, and the model-based
estimates correct for this bias. We first compute the average
observed rating, <span class="math inline">\(\bar{y}_j\)</span> for each movie <span class="math inline">\(j\)</span>.</p>
<p>Recall that in our model this maps to the transformed parameter
<span class="math inline">\(a_j=\mu + \sigma_a*\alpha_j\)</span>, the expected average rating that we
would see if everyone in the population rated every movie. We compute
the posterior median of <span class="math inline">\(a_j\)</span> and also recall its true value from the
process used to simulate the data.</p>
<p>We then plot true movie quality (the parameter , first versus raw
average rating, then versus the model-based estimate:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-29-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The raw averages for the romantic comedies are mostly too high: The
open circles on the first plot are mostly to the right of the diagonal
line, implying that those averages <span class="math inline">\(\bar{y}_j\)</span> are higher than the
true values of the movie quality parameters <span class="math inline">\(a_j\)</span>. Meanwhile the raw
averages for the crime movies are mostly too low, with the solid dots
on the first plot mostly to the left of the diagonal line. This all
makes sense, as we have constructed our simulation so that the
romantic comedies are more likely to be rated by nicer reviewers, and
the crime movies are more likely to rated by tougher reviewers.</p>
<p>The model adjusts for these biases, though, and so the model-based
estimates, shown in the second plot above, do not have these
systematic problems.</p>
</div>
<div id="other-potential-extentions-to-the-model" class="section level3 unnumbered">
<h3>Other potential extentions to the model</h3>
<p>Here are some of the many ways in which the model could be generalized
in order to make it more realistic:</p>
<ul>
<li><p>More popular movies should get more ratings. So we might want to
extend the model to allow the probability of a person rating a movie
to depend on the movie’s popularity, which here is coded by the
parameter <span class="math inline">\(\alpha_j\)</span>.</p></li>
<li><p>Different people have different preferences for different genres.
The model could capture this by allowing each person to have a
vector of difficulty parameters, one for each genre.</p></li>
<li><p>Movie ratings are discrete; we could replace the normal distribution
for <span class="math inline">\(y_i\)</span> by an ordered logistic model which would give
probabilities of each of the discrete responses from 0 through 5.</p></li>
</ul>
</div>
</div>
<div id="world-cup" class="section level2">
<h2><span class="header-section-number">4.4</span> World Cup</h2>
<p>We fit a model to estimate the abilities of the teams in the 2014
soccer World Cup. We fit a simple linear item response model, using
the score differentials as data (ignoring the shoot-outs). We also
have a feeling that when the game is not close the extra goals don’t
provide as much information so we’ll fit the model on the square-root
scale.</p>
<p>The model is as follows: if game <span class="math inline">\(i\)</span> has teams <span class="math inline">\(j_1\)</span> and team <span class="math inline">\(j_2\)</span>
playing, and they score <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> goals, respectively, then the
data point for this game is <span class="math inline">\(y_i = \mbox{sign}(z_1-z_2)*\sqrt{|z_1-z_2|}\)</span>, and the data model is: <span class="math inline">\(y_i \sim \mbox{normal}(a_{j_1[i]}-a_{j_2[i]}, \sigma_y)\)</span>, where <span class="math inline">\(a_{j_1}\)</span>
and <span class="math inline">\(a_{j_2}\)</span> are the ability parameters (to use psychometrics jargon)
for the two teams and <span class="math inline">\(\sigma_y\)</span> is a scale parameter estimated from
the data. But then before fitting the model we was thinking of
occasional outliers such as that Germany-Brazil match so we decided
that a <span class="math inline">\(t\)</span> model could make more sense:
<span class="math display">\[
y_i \sim \mbox{t}(\nu, a_{j_1[i]}-a_{j_2[i]}, \sigma_y),
\]</span>
setting the degrees of freedom to <span class="math inline">\(\nu=7\)</span> which has been occasionally
recommended as a robust alternative to the normal.</p>
<p>It turned out, when the model was all fit and we started tinkering
with it, that neither the square root transformation nor the
long-tailed <span class="math inline">\(t\)</span> distribution were really necessary to model the soccer
scores; a simple normal model would have been fine. But we’ll work
with this particular model because that was how we first thought of
setting it up.</p>
<p>There weren’t so many World Cup games (only 64 games in total for 32
teams) so we augmented the dataset by partially pooling the ability
parameters toward an external data source, something called the Soccer
Power Index that was available on the internet a month before the
tournament. We took the rankings, with Brazil at the top (getting a
score of 32) and Australia at the bottom (with a score of 1), and then
for simplicity in interpretation of the parameters we rescaled these
to have mean 0 and standard deviation 1/2, to get “prior scores” that
ranged from <span class="math inline">\(-0.83\)</span> to + <span class="math inline">\(0.83\)</span>.</p>
<p>Our model for the team abilities was then simply, <span class="math inline">\(a_j \sim \mbox{normal}(\mu + b*\mbox{prior\_score}_j, \sigma_a)\)</span>, which we
write as <span class="math inline">\(a_j=\mu+b*\mbox{prior\_score}_j+\sigma_a*\alpha_j\)</span>, with
<span class="math inline">\(\alpha_j\sim\mbox{normal}(0,1) \mbox{ for } j=1,\dots,J=32\)</span>.
Actually, though, all we care about are the relative, not the
absolute, team abilities, so we can just set <span class="math inline">\(\mu=0\)</span>, so that the
model is, <span class="math display">\[a_j = b*\mbox{prior\_score}_j+\sigma_a*\alpha_j, \mbox{
with } \alpha_j \sim \mbox{normal}(0, 1), \mbox{ for }
j=1,\dots,J=32.\]</span> At this point we should probably add weakly
informative priors for <span class="math inline">\(b\)</span>, <span class="math inline">\(\sigma_a\)</span>, and <span class="math inline">\(\sigma_y\)</span>, but we didn’t
bother. We can always go back and add these to stabilize the
inferences, but 32 teams should be enough to estimate these parameters
so we don’t think it will be necessary in this case.</p>
<p>We now set up the model in Stan:</p>
<pre><code>/* This program has a mistake in it, as will be explained later */

data {
  int N_teams;
  int N_games;
  vector[N_teams] prior_score;
  int team_1[N_games];
  int team_2[N_games];
  vector[N_games] score_1;
  vector[N_games] score_2;
  real df;
}
transformed data {
  vector[N_games] dif;
  vector[N_games] sqrt_dif;
  dif = score_1 - score_2;
  for (i in 1:N_games){
    sqrt_dif[i] = (step(dif[i]) - 0.5)*sqrt(fabs(dif[i]));
  }
}
parameters {
  vector[N_teams] alpha;
  real b;
  real&lt;lower=0&gt; sigma_a;
  real&lt;lower=0&gt; sigma_y;
}
transformed parameters {
  vector[N_teams] a;
  a = b*prior_score + sigma_a*alpha;
}
model {
  alpha ~ normal(0, 1);
  sqrt_dif ~ student_t(df, a[team_1] - a[team_2], sigma_y);
}</code></pre>
<p>The stuff in the transformed data block is to transform the raw data
into signed square root differentials. (The function “fabs” is short
for “floating point absolute value.”) It turns out this code has a
mistake in it, which we will get to in a bit.</p>
<p>The simulations converge, and the estimates seem reasonable.</p>
<pre><code>Inference for Stan model: worldcup_first_try.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd   2.5%   25%   50%   75%   98% n_eff Rhat
a[1]     0.29    0.00 0.14  -0.01  0.20  0.30  0.39  0.54  3564    1
a[2]     0.37    0.00 0.12   0.13  0.29  0.37  0.44  0.62  4815    1
a[3]     0.48    0.00 0.16   0.21  0.36  0.47  0.59  0.82  1391    1
a[4]     0.17    0.00 0.18  -0.23  0.06  0.20  0.30  0.47  2089    1
a[5]     0.28    0.00 0.14  -0.02  0.19  0.28  0.36  0.57  5984    1
a[6]     0.30    0.00 0.13   0.04  0.22  0.29  0.38  0.58  4036    1
a[7]     0.33    0.00 0.15   0.07  0.23  0.31  0.43  0.66  3158    1
a[8]     0.14    0.00 0.15  -0.20  0.06  0.16  0.24  0.42  4724    1
a[9]     0.04    0.00 0.17  -0.35 -0.06  0.07  0.16  0.31  1942    1
a[10]    0.20    0.00 0.13  -0.05  0.12  0.19  0.28  0.48  4746    1
a[11]    0.33    0.00 0.17   0.07  0.20  0.32  0.44  0.69  1231    1
a[12]    0.05    0.00 0.15  -0.27 -0.04  0.07  0.14  0.32  4750    1
a[13]    0.05    0.00 0.15  -0.27 -0.03  0.06  0.14  0.34  5222    1
a[14]    0.03    0.00 0.14  -0.28 -0.05  0.04  0.11  0.32  5997    1
a[15]   -0.03    0.00 0.15  -0.36 -0.11 -0.01  0.06  0.25  5085    1
a[16]   -0.07    0.00 0.15  -0.41 -0.16 -0.06  0.02  0.19  3392    1
a[17]   -0.05    0.00 0.15  -0.38 -0.14 -0.04  0.04  0.24  5297    1
a[18]    0.00    0.00 0.14  -0.28 -0.08 -0.01  0.08  0.30  4603    1
a[19]   -0.03    0.00 0.14  -0.29 -0.11 -0.04  0.05  0.27  5135    1
a[20]    0.01    0.00 0.14  -0.24 -0.08 -0.01  0.10  0.32  3494    1
a[21]   -0.14    0.00 0.15  -0.45 -0.22 -0.13 -0.05  0.16  6616    1
a[22]   -0.12    0.00 0.14  -0.40 -0.20 -0.12 -0.04  0.17  5756    1
a[23]   -0.18    0.00 0.16  -0.52 -0.26 -0.17 -0.09  0.14  5436    1
a[24]   -0.16    0.00 0.14  -0.42 -0.24 -0.16 -0.08  0.14  5014    1
a[25]   -0.26    0.00 0.16  -0.60 -0.36 -0.25 -0.17  0.03  3858    1
a[26]   -0.03    0.00 0.18  -0.31 -0.17 -0.04  0.09  0.35  1280    1
a[27]   -0.29    0.00 0.15  -0.62 -0.38 -0.29 -0.20  0.00  5418    1
a[28]   -0.41    0.00 0.17  -0.79 -0.51 -0.38 -0.29 -0.13  2330    1
a[29]   -0.30    0.00 0.15  -0.60 -0.39 -0.30 -0.21  0.01  5314    1
a[30]   -0.42    0.00 0.16  -0.76 -0.51 -0.41 -0.31 -0.13  3689    1
a[31]   -0.23    0.00 0.16  -0.50 -0.34 -0.24 -0.13  0.11  2481    1
a[32]   -0.39    0.00 0.15  -0.69 -0.48 -0.39 -0.29 -0.09  6363    1
b        0.45    0.00 0.10   0.25  0.39  0.45  0.52  0.65  3344    1
sigma_a  0.16    0.00 0.08   0.01  0.11  0.17  0.22  0.31   591    1
sigma_y  0.42    0.00 0.05   0.33  0.39  0.42  0.46  0.54  1933    1
lp__     0.02    0.26 6.57 -13.37 -4.50  0.46  4.71 11.56   617    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 22:57:59 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The simulations converge, and we can look at the parameter estimates:</p>
<ul>
<li><p>Recall that the 32 teams are listed in order of their prior ranking,
with Brazil and Argentina at the top and Australia at the bottom,
so the posterior mean estimates for the team abilities <span class="math inline">\(a\)</span> seem
reasonable. The posterior intervals for the different teams
overlap a lot, which makes sense given that most of teams only play
3 or 4 games in the tournament.</p></li>
<li><p>The estimated coefficient <span class="math inline">\(b\)</span> is positive, indicating that teams
with higher prior rankings did better in the tournament, which makes
sense; the estimate of 0.46 implies that a good team is about half a
goal (on the square-root scale) better than a poor team. We can
give this latter interpretation because we have already put the
prior score predictor on a standardized scale.</p></li>
<li><p>The group-level error standard deviation <span class="math inline">\(\sigma_a\)</span> is estimated at
0.13 which is a small value, which indicates that, unsurprisingly,
our final estimates of team abilities are not far from the initial
ranking. (If <span class="math inline">\(\sigma_a\)</span> were exactly zero, then the team abilities
would be a perfect linear function of those prior rankings. We can
attribute this good fit to a combination of two factors: first, the
initial ranking is pretty accurate; second, there aren’t a lot of
data points here so not much information that would pull the teams
away from this assumed linear model.</p></li>
<li><p>The data-level error <span class="math inline">\(\sigma_y\)</span> is estimated at 0.42, implying that
the uncertainty in any game is about half a goal on the square-root
scale, about as much as the variation between good and bad teams.
On any given day, any team could beat any other team.</p></li>
</ul>
<p>Now it’s time to make some graphs. First a simple list of estimates
and standard errors of team abilities. We’ll order the teams based on
prior ranking, which makes sense for two reasons. First, this ordering
is informative: there’s a general trend from good to bad so it should
be easy to understand the results. Second, the prior ranking is what
we were using to pull toward in the multilevel model, so this graph is
equivalent to a plot of estimate vs. group-level predictor, which is
the sort of graph we like to make to understand what a multilevel
model is doing.</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-33-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>At this point we could compute lots of fun things such as the
probability that Argentina would beat Germany if the final were played
again, but it’s clear enough from this picture that the estimate will
be close to 50% so really the model isn’t giving us much for that one
game.</p>
<p>One thing we should try to understand, though, is how much of these
estimates are coming from the prior ranking? So we very slightly alter
the model, changing two lines by moving <span class="math inline">\(b\)</span> from the parameters to the
data block in the Stan program. Then we call the model with <span class="math inline">\(b\)</span> set
to 0.</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-35-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This is roughly similar to before but a lot noisier.</p>
<p>Now let’s check model fit. For this we’ll go back to the model that
includes the prior ranking as a linear predictor, predicting
replications of the games using a generated quantities block:</p>
<pre><code>generated quantities {
  vector[N_games] y_rep
    = student_t_rng(df, a[team_1] - a[team_2], sigma_y);
  vector[N_games] y_rep_original_scale
    = y_rep * abs(y_rep);
}</code></pre>
<p>We re-fit the model and produce the replications. The result is a
matrix of simulations, y_rep_original_scale, representing the
posterior distribution of the outcomes of the 64 games, if the
tournament were to be repeated. For each game we can then collect a
95% predictive interval, and we plot these along with the actual game
outcomes:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-37-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Something went wrong. Far more than 5% of the data points are outside
the 95% intervals.</p>
<p>The next step is to figure out what happened. Our first thought was
that there was some problem with the <span class="math inline">\(t\)</span> distribution—but replacing
it by a normal, or keeping the <span class="math inline">\(t\)</span> but estimating the dsgrees of
freedom parameter, did not change anything noticeably. Our next idea
was that the discretness of the data could be causing the problem.
But, no, that wasn’t it either: the poor coverage of these intervals
goes well beyond rounding error.</p>
<p>What about the square-root transformation? Could that be the problem?
Let’s re-fit the model on the original scale:</p>
<pre><code>/* This program has a mistake in it, as will be explained later */

data {
  int N_teams;
  int N_games;
  vector[N_teams] prior_score;
  int team_1[N_games];
  int team_2[N_games];
  vector[N_games] score_1;
  vector[N_games] score_2;
  real df;
}
transformed data {
  vector[N_games] dif;
  dif = score_1 - score_2;
}
parameters {
  vector[N_teams] alpha;
  real b;
  real&lt;lower=0&gt; sigma_a;
  real&lt;lower=0&gt; sigma_y;
}
transformed parameters {
  vector[N_teams] a;
  a = b*prior_score + sigma_a*alpha;
}
model {
  alpha ~ normal(0, 1);
  dif ~ student_t(df, a[team_1] - a[team_2], sigma_y);
}
generated quantities {
  vector[N_games] y_rep;
  for (n in 1:N_games) {
    y_rep[n] = student_t_rng(df, a[team_1[n]] - a[team_2[n]], sigma_y);
  }
}</code></pre>
<p>And then again we fit the model and produce the graph of inferences:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-40-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The parameter estimates are similar to before, but on a different
scale, which makes sense given that we’re no longer working on the
square root scale.</p>
<p>Next we make the graph comparing game outcomes to 95% posterior
predictive intervals:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-41-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This looks fine: approximately 95% of the game outcomes fall within
the 95% predictive intervals.</p>
<p>At this point we could declare stop and declare victory, but first we
would like to figure out what went wrong with that square root model.
We look again at the code and find the error, which is inside the
transformed data block of our original Stan program:</p>
<pre><code>sqrt_dif[i] = (step(dif[i]) - 0.5)*sqrt(fabs(dif[i]));</code></pre>
<p>That last line is wrong—it’s missing a factor of 2. Stan doesn’t
have a sign() function so I hacked something together using
“step(dif[i]) - 0.5”. But this difference takes on the value <span class="math inline">\(+0.5\)</span> if
dif is positive or <span class="math inline">\(-0.5\)</span> if dif is negative. Here is the correct
code:</p>
<pre><code>sqrt_dif[i] = 2*(step(dif[i]) - 0.5)*sqrt(fabs(dif[i]));</code></pre>
<p>We now put fix the Stan program, re-fit the model, and display the
parameter estimates and the two graphs as before:</p>
<pre><code>Inference for Stan model: worldcup_no_sqrt.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd   2.5%   25%    50%    75%    98% n_eff Rhat
b         1.21    0.01 0.29   0.65   1.0   1.21   1.40   1.79  3075    1
sigma_a   0.37    0.01 0.22   0.02   0.2   0.36   0.52   0.83   828    1
sigma_y   1.32    0.00 0.16   1.03   1.2   1.31   1.42   1.68  2740    1
lp__    -73.29    0.23 5.83 -84.89 -77.2 -73.30 -69.30 -62.41   660    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:00:06 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-43-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-44-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>All is fine now. In retrospect we never needed that square root in
the first place, but it’s good to have figured out our error, in case
we need to fit such a model in the future. It was also instructive
how we found that mistake through a routine plot comparing data to the
posterior predictive distribution.</p>
<p>The final 95% predictive intervals are very wide, indicating that with
the information used in this model, we can’t say much about any
individual game. That’s fine; it is what it is.</p>
</div>
<div id="sex-ratio" class="section level2">
<h2><span class="header-section-number">4.5</span> Sex ratio</h2>
<p>We can use prior information to refine estimates from noisy studies.
For example, several years ago a researcher analyzed data from a
survey of 3000 Americans and observed a correlation between
attractiveness of parents and the sex of their children. In
particular, the survey coded adults into five attractiveness
categories, and it turned out that 56% of the children of parents in
the highest attractiveness category were girls, compared to 48% of the
children of parents in the other categories. The difference of 8% had
a standard error (based on the usual formula for the difference in
proportions) of 3%.</p>
<p>The observed difference is more than 2 standard errors from zero,
meeting the usual standard of statistical significance, and indeed the
claim that beautiful parents have more daughters was published in a
scientific journal and received wide publicity.</p>
<p>This is, however, not the end of the story. It is well known that the
variation in the human sex ratio occurs in a very narrow range. For
example a recent count in the United States reported 48.7% girls among
whites and 49.2% among blacks. Similar differences of half of a
percentage point or less have been found when comparing based on
factors such as birth order, maternal age, or season of birth. Thus
we would find it hard to believe that any difference between more and
less attractive parents could be as large as 0.5%.</p>
<p>We now perform Bayesian inference using the template above. The
parameter of interest here, <span class="math inline">\(\theta\)</span>, is the probability of girl
births among beautiful parents, minus the probability among other
parents, all in the general population of Americans. As is often the
case, we are interested in the comparison of <span class="math inline">\(\theta\)</span> to zero: Is
there strong evidence that <span class="math inline">\(\theta&gt;0\)</span>, which would imply that more
attractive parents are more likely to have girls?</p>
<p>We can express our scientific knowledge as a prior distribution on
<span class="math inline">\(\theta\)</span> with mean 0% and standard deviation 0.25%. The prior mean of
zero says that, in advance of seeing the data we would have no reason
to expect beautiful parents to have an elevated or depressed rate of
girl births. The prior standard deviation of 0.25% says that we find
it highly implausible that the true value of <span class="math inline">\(\theta\)</span> is higher than
0.5% or lower than -0.5%.</p>
<p>For convenience we are expressing our estimates and uncertainties on a
percentage scale, to avoid the awkwardness of working with expressions
such as 0.0025 and possibly dropping a zero somewhere.</p>
<p>In this case, we could perform Bayesian inference analytically: with
normally distributed data <span class="math inline">\(y\)</span> with standard error <span class="math inline">\(\sigma_y\)</span> and a
normal<span class="math inline">\((\mu_0,\sigma_0)\)</span> prior distribution, <span class="math inline">\(\theta\)</span> is normally
distributed in its posterior distribution, with
<span class="math display">\[\mbox{posterior mean: } \
\frac{\frac{1}{\sigma_0^2} \, \mu_0 + \frac{1}{\sigma_y^2}y}
     {\frac{1}{\sigma_0^2} + \frac{1}{\sigma_y^2}}
\]</span>
and
<span class="math display">\[\mbox{posterior sd: } \
\sqrt{\frac{1}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma_y^2}}}
\]</span></p>
<p>But we are teaching Stan here, so we shall demonstrate the fit in
Stan. Here is the Stan code:</p>
<pre><code>data {
  real y;
  real&lt;lower=0&gt; sigma_y;
  real mu_0;
  real&lt;lower=0&gt; sigma_0;
}
parameters {
  real theta;
}
model {
  y ~ normal(theta, sigma_y);
  theta ~ normal(mu_0, sigma_0);
}</code></pre>
<p>One advantage of using Stan here, rather than the formula, is that
with Stan it is easy to alter the model, for example changing the
prior distribution from normal to <span class="math inline">\(t\)</span>, or adding additional data in
some way or another.</p>
<p>But for now we shall stick with the above normal model, feeding in the
data <span class="math inline">\(y = 8, \sigma_y = 3, \mu_0 = 0, \sigma_0 = 0.25\)</span>, to obtain the following
result:</p>
<pre><code>Inference for Stan model: normal_normal.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
theta  0.05    0.01 0.24 -0.43 -0.12  0.04  0.21  0.52  1479    1
lp__  -4.01    0.02 0.68 -5.86 -4.19 -3.75 -3.58 -3.53  1997    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:00:43 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<div id="time-series-competition" class="section level2">
<h2><span class="header-section-number">4.6</span> Time series competition</h2>
<p>A few years ago someone sent me an email about a “Global Climate
Challenge” that he had seen online, and which was introduced as
follows:</p>
<blockquote>
<p>It has often been claimed that alarm about global warming is
supported by observational evidence. I have argued that there is no
observational evidence for global-warming alarm: rather, all claims
of such evidence rely on invalid statistical analyses.</p>
</blockquote>
<blockquote>
<p>Some people, though, have asserted that the statistical analyses are
valid. Those people assert, in particular, that they can determine,
via statistical analysis, whether global temperatures have been
increasing more than would be reasonably expected by random natural
variation. Those people do not present any counter to my argument,
but they make their assertions anyway.</p>
</blockquote>
<blockquote>
<p>In response to that, I am sponsoring a contest: the prize is
$100,000. In essence, the prize will be awarded to anyone who can
demonstrate, via statistical analysis, that the increase in global
temperatures is probably not due to random natural variation.</p>
</blockquote>
<p>How to win the money?</p>
<blockquote>
<p>The file <code>data/Series1000.txt</code> contains 1000 time series. Each
series has length 135: the same as that of the most commonly studied
series of global temperatures (which span 1880–2014). The 1000
series were generated as follows. First, 1000 random series were
obtained (via a trendless statistical model fit for global
temperatures). Then, some randomly-selected series had a trend added
to them. Some trends were positive; the others were negative. Each
individual trend was 1°C/century (in magnitude)—which is greater
than the trend claimed for global temperatures.</p>
</blockquote>
<blockquote>
<p>A prize of $100,000 (one hundred thousand U.S. dollars) will be
awarded to the first person who submits an entry that correctly
identifies at least 900 series: which series were generated by a
trendless process and which were generated by a trending process.</p>
</blockquote>
<p>But also this:</p>
<blockquote>
<p>Each entry must be accompanied by a payment of $10.</p>
</blockquote>
<p>OK, now it’s time to get to work. We start by downloading and
graphing the data.
<img src="simple-examples_files/figure-html/unnamed-chunk-48-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Aha! The lines are fanning out from a common starting point. We’ll fit
a regression to each line and then summarize each line by its average
slope.</p>
<p>We multiplied the slopes (and standard errors) by 100 to put them on a
per-century scale to match the above instructions.</p>
<p>Next we plot the estimated slopes and their standard errors:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-50-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>OK, not much information in the se’s. How about a histogram of the
estimated slopes?</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-51-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Based on the problem description, I’d expect to see distributions
centered at 0, -1, and 1. It looks like this might be the case.</p>
<p>So let’s fit a mixture model. That’s easy. Here’s the Stan program:</p>
<pre><code>data {
  int K;
  int N;
  real y[N];
  real mu[K];
}
parameters {
  simplex[K] theta;
  real sigma;
}
model {
  real ps[K];
  sigma ~ cauchy(0,2.5);
  mu ~ normal(0,10);
  for (n in 1:N) {
    for (k in 1:K) {
      ps[k] = log(theta[k]) + normal_lpdf(y[n] | mu[k], sigma);
    }
    target += log_sum_exp(ps);
  }
}</code></pre>
<p>We now run the program and display the results:</p>
<pre><code>
SAMPLING FOR MODEL &#39;mixture&#39; NOW (CHAIN 1).
Chain 1: Gradient evaluation took 0.000276 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.76 seconds.
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1:  Elapsed Time: 1.71105 seconds (Warm-up)
Chain 1:                1.529 seconds (Sampling)
Chain 1:                3.24005 seconds (Total)

SAMPLING FOR MODEL &#39;mixture&#39; NOW (CHAIN 2).
Chain 2: Gradient evaluation took 0.00024 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.4 seconds.
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2:  Elapsed Time: 1.76644 seconds (Warm-up)
Chain 2:                1.61146 seconds (Sampling)
Chain 2:                3.3779 seconds (Total)

SAMPLING FOR MODEL &#39;mixture&#39; NOW (CHAIN 3).
Chain 3: Gradient evaluation took 0.000278 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.78 seconds.
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3:  Elapsed Time: 3.05249 seconds (Warm-up)
Chain 3:                1.4907 seconds (Sampling)
Chain 3:                4.54318 seconds (Total)

SAMPLING FOR MODEL &#39;mixture&#39; NOW (CHAIN 4).
Chain 4: Gradient evaluation took 0.000252 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.52 seconds.
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4:  Elapsed Time: 1.6733 seconds (Warm-up)
Chain 4:                1.5434 seconds (Sampling)
Chain 4:                3.21671 seconds (Total)
Inference for Stan model: mixture.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd     2.5%      25%      50%      75%
theta[1]     0.54    0.00 0.02     0.50     0.52     0.54     0.55
theta[2]     0.24    0.00 0.02     0.21     0.23     0.24     0.25
theta[3]     0.22    0.00 0.02     0.19     0.21     0.22     0.23
sigma        0.40    0.00 0.02     0.37     0.39     0.40     0.42
lp__     -1174.72    0.03 1.19 -1177.79 -1175.27 -1174.42 -1173.85
              98% n_eff Rhat
theta[1]     0.58  3422    1
theta[2]     0.27  3612    1
theta[3]     0.26  3414    1
sigma        0.44  3016    1
lp__     -1173.38  1974    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:01:38 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Convergence is fine: <span class="math inline">\(\widehat{R}\)</span> is close to 1 for everything. The
estimated weights of the three mixture components are approximately
0.5, 0.25, 0.25. Given that the problem was made up, I’m guessing the
weights of the underlying data-generation process are exactly 1/2,
1/4, and 1/4. The standard deviation of the slopes within each
component is 0.4, or close to it. We could also try fitting a model
where the standard deviations of the three components differ, but we
won’t, partly because the description given with the simulated data
described the change as adding a trend, and partly because the above
histogram doesn’t seem to show any varying of the widths of the
mixture components.</p>
<p>OK, now we’re getting somewhere. To make predictions, we need to know,
for each series, the probability of it being in each of the three
components. We’ll compute these probabilities by adding a generated
quantities block to the Stan program:</p>
<pre><code>generated quantities {
  matrix[N,K] p;
  for (n in 1:N){
    vector[K] p_raw;
    for (k in 1:K){
      p_raw[k] &lt;- theta[k]*exp(normal_log(y[n], mu[k], sigma));
    }
    for (k in 1:K){
      p[n,k] &lt;- p_raw[k]/sum(p_raw);
    }
  }
}</code></pre>
<p>We then re-fit the model, extract the <span class="math inline">\(p\)</span>’s, and average them over the
posterior simulations.</p>
<p>We now have a <span class="math inline">\(1000\times 3\)</span> matrix of probabilities. Let’s take a
look at the first ten rows</p>
<pre><code>      [,1] [,2] [,3]
 [1,] 0.08 0.00 0.92
 [2,] 0.40 0.60 0.00
 [3,] 0.93 0.01 0.06
 [4,] 0.83 0.17 0.00
 [5,] 0.82 0.18 0.00
 [6,] 0.95 0.01 0.05
 [7,] 0.74 0.00 0.26
 [8,] 0.86 0.14 0.00
 [9,] 0.11 0.00 0.89
[10,] 0.87 0.00 0.13</code></pre>
<p>So, the first series is probably drawn from the sloping-upward model;
the second might be from the null model or it might be from the
sloping-downward model; the third, fourth, fifth, sixth, seventh, and
eighth are probably from the null model; the ninth is probably from
the sloping-upward model; and so forth.</p>
<p>We’ll now program this: for each of the 1000 series in the dataset,
we’ll pick which of the three mixture components has the highest
probability. We’ll save the probability and also which component is
being picked.</p>
<p>And now we can sum this over the 1000 series. We’ll compute the number
of series assigned to each of the three choices:</p>
<pre><code>choice
  1   2   3 
559 232 209 </code></pre>
<p>The guesses are not quite in proportion 500, 250, 250. There seem to
be too many guesses of zero slope and not enough of positive and
negative slopes. But that makes sense given the decision problem: we
want to maximize the number of correct guesses so we end up
disproportionately guessing the most common category. That’s fine;
it’s how it will be.</p>
<p>And we can compute the expected number of correct guesses (based on
the posterior distribution we have here), and the standard deviation
of the number of correct guesses (based on the reasonable
approximation of independence of the 1000 series conditional on the
model). And then we’ll print all these numbers:</p>
<pre><code>[1] 854.5  10.3</code></pre>
<p>Interesting. The expected number of correct guesses is 854.1. Not
quite the 900 that’s needed to win! The standard error of the number
of correct guesses is 10.3, so 900 is over 5 standard errors away from
our expected number correct. That’s bad news!</p>
<p>How bad is it? We can compute the normal cumulative density fucntion
to get the probability of at least 900 successes; that’s
<span class="math inline">\(\mbox{pnorm}(854.1, 900, 10.3)\)</span>:</p>
<pre><code>[1] 4.9e-06</code></pre>
<p>That’s a small number; here’s its reciprocal:</p>
<pre><code>[1] 2e+05</code></pre>
<p>That’s a 1-in-230,000 chance of winning the big prize!</p>
<p>But we only have to get <em>at least</em> 900. So we can do the continuity
correction and evaluate the probability of at least 899.5 successes,
which, when inverted, yields:</p>
<pre><code>[1] 161775</code></pre>
<p>Nope, still no good. For the bet to be worth it, even in the crudest
sense of expected monetary value, the probability of winning would
have to be at least 1 in 10,000. (Recall that the prize is $100,000
but the cost of entry is $10.) And that’s all conditional on the
designer of the study doing everything exactly as he said, and not
playing with multiple seeds for the random number generator,
etc. After all, he could well have first chosen a seed and generated
the series, then performed something like the above analysis and
checked that the most natural estimate gave only 850 correct or so,
and in the very unlikely event that the natural estimate gave 900 or
close to it, just re-running with a new seed. I have no reason to
think that the creator of this challenge did anything like that; my
point here is only that, even if he did his simulation in a completely
clean way, our odds of winning are about 1 in 200,000—about 1/20th
what we’d need for this to be a fair game.</p>
<p>There is one more thing, though: the data are highly autocorrelated,
so least-squares regression may not be the most efficient way to
estimate these slopes. If we can estimate the slopes more precisely,
we can get more discrimination in our predictions. Maybe there is a
way to win the game by extracting more information from each series,
but it won’t be easy.</p>
<p>You could say that the above all demonstrates the designer’s point,
that you can’t identify a trend in a time series of this length. But I
don’t think it would make sense to draw that conclusion from this
exercise. After all, you can just tweak the parameters in the problem
a bit—or simply set the goal to 800 correct instead of 900—and the
game becomes easy to win. Or, had the game been winnable as initially
set up, you could just up the threshold to 950, and again it would
become essentially impossible to win. Conversely, if the designer of
the challenge had messed up his calculations and set the threshold to
800, and someone had sent in a winning entry, it wouldn’t disprove his
claims about climate science, it would just mean he hadn’t been
careful enough in setting up his bet.</p>
</div>
<div id="declining-exponential" class="section level2">
<h2><span class="header-section-number">4.7</span> Declining exponential</h2>
<p>Let’s fit the following simple model: <span class="math inline">\(y = ae^{-bx} + \mbox{error}\)</span>,
given data <span class="math inline">\((x,y)_i\)</span>:
<span class="math display">\[
y_i = ae^{-bx_i} + \epsilon_i, \mbox{ for } i=1,\dots,N,
\]</span>
We shall assume the errors are independent and normally
distributed: <span class="math inline">\(\epsilon_i \sim \mbox{normal}(0,\sigma)\)</span>.</p>
<p>Here is the model in Stan:</p>
<pre><code>data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real a;
  real b;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(a*exp(-b*x), sigma);
  a ~ normal(0, 10);
  b ~ normal(0, 10);
  sigma ~ normal(0, 10);
}</code></pre>
<p>We have given the parameters <span class="math inline">\(a\)</span>, and <span class="math inline">\(b\)</span>, and <span class="math inline">\(\sigma\)</span> normal prior
distibutions centered at 0 with standard deviation 10. In addition,
the parameter <span class="math inline">\(\sigma\)</span> is constrained to be positive. The purpose of
the prior distributions is to keep the computations at a reasonable
value. If we were working on a problem in which we thought that <span class="math inline">\(a\)</span>,
<span class="math inline">\(b\)</span>, or <span class="math inline">\(\sigma\)</span> could be much greater than 10, we would want to use a
weaker prior distribution.</p>
<p>Another point about the above Stan program: the model for <span class="math inline">\(y\)</span> is
vectorized and could instead have been written more explicitly as a
loop:</p>
<pre><code>  for (i in 1:N){
    y[i] ~ normal(a*exp(-b*x[i]), sigma);
  }</code></pre>
<p>We prefer the vectorized version as it is more compact and it also
runs faster in Stan, for reasons discussed elsewhere in this book.</p>
<p>To demonstrate our exponential model, we fit it to fake data. We’ll
simulate <span class="math inline">\(N=100\)</span> data points with predictors <span class="math inline">\(x\)</span> uniformly distributed
between 0 and 10, from the above model with <span class="math inline">\(a=0.2, b=0.3, \sigma=0.5\)</span>.</p>
<p>Here is a graph of the true curve and the simulated data:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-64-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And then we fit the model:</p>
<pre><code>Inference for Stan model: exponential.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
a      4.95    0.00 0.21  4.52  4.81  4.96  5.10  5.36  1814    1
b      0.30    0.00 0.02  0.27  0.29  0.30  0.31  0.33  2136    1
sigma  0.49    0.00 0.04  0.43  0.47  0.49  0.51  0.57  2306    1
lp__  21.05    0.03 1.19 18.00 20.50 21.35 21.94 22.44  1908    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:03:38 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Recall that the true parameter values were <span class="math inline">\(a=5.0, b=0.3, \sigma=0.5\)</span>.
Here the model is simple enough and the data are clean enough that we
can estimate all three of these parameters with reasonable precision
from the data, as can be seen from the 95% intervals above.</p>
<p>Alternatively, we might want to say ahead of time that we are fitting
a declining exponential curve that starts positive and descends to
zero. We would thus want to constrain the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to be
positive, which we can do in the parameters block:</p>
<pre><code>  real&lt;lower=0&gt; a;
  real&lt;lower=0&gt; b;</code></pre>
<p>Otherwise we leave the model unchanged. In this case the results turn
out to be very similar:</p>
<pre><code>Inference for Stan model: exponential_positive.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
a      4.95    0.01 0.21  4.54  4.81  4.95  5.08  5.37  1627    1
b      0.30    0.00 0.02  0.27  0.29  0.30  0.31  0.33  1962    1
sigma  0.49    0.00 0.04  0.42  0.47  0.49  0.51  0.57  2350    1
lp__  21.41    0.03 1.25 18.06 20.84 21.72 22.32 22.84  1507    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:04:19 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>With weaker data, though, the constraints could make a difference. We
could experiment on this by doing the same simulation but with just
<span class="math inline">\(N=10\)</span> data points:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-69-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>First we fit the unconstrained model:</p>
<pre><code>Inference for Stan model: exponential.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd   2.5%  25%   50%   75%  98% n_eff Rhat
a      5.27    0.05 0.97   3.70  4.8  5.18  5.61  7.8   317  1.0
b      0.92    0.28 1.79   0.22  0.3  0.36  0.48  7.7    41  1.1
sigma  0.98    0.06 0.46   0.51  0.7  0.85  1.11  2.2    59  1.1
lp__  -3.63    0.42 2.85 -11.13 -4.5 -2.56 -1.63 -0.9    47  1.1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:04:21 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Then we fit the constrained model:</p>
<pre><code>Inference for Stan model: exponential_positive.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
a      5.14    0.02 0.75  3.83  4.71  5.10  5.51  6.69  1312    1
b      0.58    0.05 1.18  0.21  0.30  0.35  0.43  3.52   620    1
sigma  0.89    0.01 0.35  0.51  0.66  0.81  1.01  1.78   668    1
lp__  -2.32    0.07 1.72 -6.56 -3.11 -1.83 -1.04 -0.38   563    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:04:22 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Different things can happen with different sets of simulated data, but
the inference with the positivity constraints will typically be much
more stable. Of course, we would only want to constrain the model in
this way if we knew that the positivity restriction is appropriate.</p>
<p>Now suppose that the data are also restricted to be positive. Then we
need a different error distribution, as the above model with additive
normal errors can yield negative data.</p>
<p>Let’s try a multiplicative error, with a lognormal distribution:</p>
<p><span class="math display">\[
y_i = ae^{-bx_i} * \epsilon_i, \mbox{ for } i=1,\dots,N\\
\log\epsilon_i \sim \mbox{normal}(0,\log\sigma), \mbox{ for } i=1,\dots,N
\]</span></p>
<p>Here is the model in Stan:</p>
<pre><code>data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real&lt;lower=0&gt; a;
  real&lt;lower=0&gt; b;
  real&lt;lower=0&gt; sigma;
}
model {
  vector[N] y_pred;
  y_pred = a*exp(-b*x);
  y ~ lognormal(log(y_pred), sigma);
  a ~ normal(0, 10);
  b ~ normal(0, 10);
  sigma ~ normal(0, 10);
}</code></pre>
<p>As before, we can simulate fake data from this model:
<img src="simple-examples_files/figure-html/unnamed-chunk-75-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can then fit the model to the simulated data and check that the
parameters are approximately recovered:</p>
<pre><code>Inference for Stan model: exponential_positive_lognormal.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
a      5.36    0.01 0.46  4.51  5.03  5.34  5.68  6.28  1725    1
b      0.30    0.00 0.01  0.27  0.29  0.30  0.31  0.32  1809    1
sigma  0.46    0.00 0.03  0.40  0.43  0.45  0.48  0.53  2272    1
lp__  28.66    0.03 1.24 25.43 28.11 28.97 29.57 30.09  1384    1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:05:01 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<div id="sum-of-declining-exponentials" class="section level2">
<h2><span class="header-section-number">4.8</span> Sum of declining exponentials</h2>
<p>From the numerical analysis literature, here is an example of an
inference problem that appears simple but can be suprisingly
difficult. The challenge is to esitmate the parameters of a sum of
declining exponentials: <span class="math inline">\(y = a_1e^{-b_1x} + a_2e^{-b_2x}\)</span>. This is
also called an inverse problem, and it can be challenging to decompose
these two declining functions.</p>
<p>This expression, and others like it, arise in many examples, including
in pharmacology, where <span class="math inline">\(x\)</span> represents time and <span class="math inline">\(y\)</span> could be the
concentration of a drug in the blood of someoone who was given a
specfied dose at time 0. In a simple <em>two-compartment model</em>, the
total concentration will look like a sum of declining exponentials.</p>
<p>To set this up as a statistics problem, we add some noise to the
system. We want the data to always be positive so our noise will be
multiplicative:
<span class="math display">\[
y_i = (a_1e^{-b_1x_i} + a_2e^{-b_2x_i}) * \epsilon_i, \mbox{ for } i=1,\dots,N,
\]</span>
with lognormally-distributed errors <span class="math inline">\(\epsilon\)</span>.</p>
<p>Here is the model in Stan:</p>
<pre><code>data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  vector&lt;lower=0&gt;[2] a;
  positive_ordered[2] b;
  real&lt;lower=0&gt; sigma;
}
model {
  vector[N] y_pred;
  y_pred = a[1]*exp(-b[1]*x) + a[2]*exp(-b[2]*x);
  y ~ lognormal(log(y_pred), sigma);
}</code></pre>
<p>The coefficients <span class="math inline">\(a\)</span> and the residual standard deviation <span class="math inline">\(\sigma\)</span> are
constrained to be positive. The parameters <span class="math inline">\(b\)</span> are also
positive—these are supposed to be declining, not increasing,
exponentials—and are also constrained to be ordered, so that
<span class="math inline">\(b_1&lt;b_2\)</span>. We need this to keep the model <em>identified</em>: Without some
sort of restriction, there would be no way from the data to tell which
component is labeled 1 and which is 2. So we arbitrarily label the
component with lower value of <span class="math inline">\(b\)</span>—that is, the one that declines
more slowly—as the first one, and the component with higher value of
<span class="math inline">\(b\)</span> to be the second. We programmed the positive_ordered type into
Stan because this sort of identification problem comes up fairly often
in applications.</p>
<p>We’ll try out our Stan model by simulating fake data from a model
where the two curves should be cleanly distinguished, setting
<span class="math inline">\(b_1=0.1\)</span> and <span class="math inline">\(b_2=2.0\)</span>, a factor of 20 apart in scale. We’ll
simulate 1000 data points where the predictors <span class="math inline">\(x\)</span> are uniformlly
spaced from 0 to 10, and, somewhat arbitrarily, set <span class="math inline">\(a_1=1.0\)</span>,
<span class="math inline">\(a_2=0.8\)</span>, and <span class="math inline">\(\sigma=0.2\)</span>. We can then simulate from the lognormal
distribution to generate the data <span class="math inline">\(y\)</span>.</p>
<p>Here is a graph of the true curve and the simulated data:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-80-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And then we fit the model:</p>
<pre><code>Inference for Stan model: sum_of_exponentials.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd    2.5%     25%     50%     75%     98% n_eff
a[1]     0.99    0.00 0.02    0.94    0.97    0.99    1.00    1.03  1522
a[2]     0.92    0.00 0.10    0.74    0.85    0.92    0.98    1.13  2821
b[1]     0.10    0.00 0.00    0.09    0.10    0.10    0.10    0.11  1629
b[2]     2.28    0.01 0.34    1.68    2.04    2.25    2.48    3.03  1986
sigma    0.20    0.00 0.00    0.19    0.20    0.20    0.20    0.21  2924
lp__  1113.05    0.04 1.64 1109.16 1112.23 1113.40 1114.22 1115.23  1324
      Rhat
a[1]     1
a[2]     1
b[1]     1
b[2]     1
sigma    1
lp__     1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:05:57 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The parameters are recovered well, with the only difficulty being
<span class="math inline">\(b_2\)</span>, where the estimate is 1.89 but the true value is 2.0—but that
is well within the posterior uncertainty. Stan worked just fine on
this nonlinear model.</p>
<p>But now let’s make the problem just slightly more difficult. Instead
of setting the two parameters <span class="math inline">\(b\)</span> to 0.1 and 2.0, we’ll make them 0.1
and 0.2, so now only a factor of 2 separates the scales of the two
declining exponentials.</p>
<p>This should still be easy to fit in Stan, right? Wrong:</p>
<pre><code>Inference for Stan model: sum_of_exponentials.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd     2.5%      25%      50%      75%      98%
a[1]   1.8e+00    0.00 0.02  1.7e+00  1.7e+00  1.7e+00  1.8e+00  1.8e+00
a[2]   1.3e+00    0.20 0.64  1.5e-01  8.8e-01  1.3e+00  1.7e+00  2.7e+00
b[1]   1.3e-01    0.00 0.00  1.3e-01  1.3e-01  1.3e-01  1.3e-01  1.4e-01
b[2]  8.9e+307     NaN  Inf 5.1e+306 4.5e+307 9.0e+307 1.3e+308 1.7e+308
sigma  2.0e-01    0.00 0.00  1.9e-01  2.0e-01  2.0e-01  2.0e-01  2.1e-01
lp__   1.8e+03    0.19 1.80  1.8e+03  1.8e+03  1.8e+03  1.8e+03  1.8e+03
      n_eff Rhat
a[1]    261  1.0
a[2]     10  1.3
b[1]    287  1.0
b[2]    NaN  NaN
sigma   452  1.0
lp__     86  1.1

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:06:21 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>What happened?? It turns out that these two declining exponentials
are <em>very</em> hard to detect. Look: here’s a graph of the two-component
model for the expected data, <span class="math inline">\(y=1.0e^{-0.1x}+0.8e^{-0.2x}\)</span>:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-86-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And now we’ll overlay a graph of a particular <em>one-component</em> model,
<span class="math inline">\(y=1.8e^{-0.135x}\)</span>:</p>
<p><img src="simple-examples_files/figure-html/unnamed-chunk-87-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The two lines are strikingly close, and it would be essentially impossible to tell them apart based on noisy data, even 1000 measurements. So Stan had trouble recovering the true parameters from the data.</p>
<p>Still, if the parameters are difficult to fit, this should just result
in a high posterior uncertainty. Why did the Stan fit explode? The
problme in this case is that, since only one term in the model was
required to fit these data, the second term was completely free—and
the parameter <span class="math inline">\(\beta_2\)</span> was unbounded: there was nothing stopping it
from being estimated as arbitrarily large. This sort of unbounded
posterior distribution is called <em>improper</em> (see Bayesian Data
Analysis for a more formal definition), and there is no way of drawing
simulations from such a distribution, hence Stan does not
converge. The simulations drift off to infinity, as there is nothing
in the prior or likeliood that is keeping them from doing so.</p>
<p>To fix the problem, we can add some prior information. Here we shall
use our default, which is independent <span class="math inline">\(\mbox{norma}(0,1)\)</span> prior
densities on all the parameters; thus, we add these lines to the model
block in the Stan program:</p>
<pre><code>  a ~ normal(0, 1);
  b ~ normal(0, 1);
  sigma ~ normal(0, 1);</code></pre>
<p>For this particular example, all we really need is a prior on <span class="math inline">\(b\)</span>
(really, just <span class="math inline">\(b_2\)</span> because of the ordering), but to demonstrate the
point we shall assign default priors to everyhing. The priors are in
addition to the rest of the model; that is, they go on top of the
positivity and ordering constraints. So, for example, the prior for
<span class="math inline">\(\sigma\)</span> is the positive half of a normal, which is sometimes written
as <span class="math inline">\(\mbox{normal}^+(0,1)\)</span>.</p>
<p>We now can fit this new model to our data, and the results are much
more stable:</p>
<pre><code>Inference for Stan model: sum_of_exponentials_with_priors.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd    2.5%     25%     50%     75%     98% n_eff
a[1]     1.47    0.02 0.29    0.66    1.33    1.59    1.68    1.74   136
a[2]     0.36    0.02 0.27    0.05    0.17    0.26    0.49    1.13   138
b[1]     0.12    0.00 0.01    0.08    0.11    0.12    0.13    0.13   157
b[2]     0.66    0.02 0.53    0.16    0.27    0.48    0.89    2.04   651
sigma    0.20    0.00 0.00    0.19    0.20    0.20    0.20    0.21  1217
lp__  1109.32    0.06 1.73 1104.97 1108.45 1109.72 1110.57 1111.47   719
      Rhat
a[1]   1.1
a[2]   1.1
b[1]   1.0
b[2]   1.0
sigma  1.0
lp__   1.0

Samples were drawn using NUTS(diag_e) at Sun Dec 16 23:08:33 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The fit is far from perfect—compare to the true parameter values,
$a_1=1.0, a_2=0.8, b_1=0.1, b_2=0.2—but we have to expect that. As
explained above, the data at hand do not identify the parameters, so
all we can hope for in a posterior distribution is some summary of
uncertainty.</p>
<p>The question then arises, what about those prior distributions? We
can think about them in a couple different ways.</p>
<p>From one direction, we can think of scaling. We are using priors
centered at 0 with a scale of 1; this can be reasonable if the
parameters are on “unit scale,” meaning that we expect them to be of
order of magnitude around 1. Not all statistical models are on unit
scale. For example, in the above model, if the data <span class="math inline">\(y_i\)</span> are on unit
scale, but the data values <span class="math inline">\(x_i\)</span> take on values in the millions, then
we’d probably expect the parameters <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> to be roungly on
the scale of <span class="math inline">\(10^{-6}\)</span>. In such a case, we’d want to rescale <span class="math inline">\(x\)</span> so
that the coefficients <span class="math inline">\(b\)</span> are more interpretable. Similarly, if the
values of <span class="math inline">\(y\)</span> ranged in the millions, then the coefficents <span class="math inline">\(a\)</span> would
have to be of order <span class="math inline">\(10^6\)</span>, and, again, we would want ot reascale the
data or the model so that <span class="math inline">\(a\)</span> would be on unit scale. By using unit
scale priors, we are implicitly assuming the model has been scaled.</p>
<p>From the other direction, instead of adapting the model to the prior
distribution, we could adapt the prior to the model. That would imply
an understanding of a reasonable range of values for the parameters,
based on the context of the problem. In any particular example this
could be done by simulating parameter vectors from the prior
distribution and graphing the corresponding curves of expected data,
and seeing if these could plausibly cover the possible cases that
might arise in the particular problem being studied.</p>
<p>No matter how it’s done, infernece has to come from somewhere, and if
the data are weak, you need to put in prior information if your goal
is to make some statement about possible parameter values, and from
there to make probabilistic predictions and decisions.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prior-distributions-and-models-for-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="workflow-in-action.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
