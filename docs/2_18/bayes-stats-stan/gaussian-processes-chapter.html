<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="clustering-chapter.html">
<link rel="next" href="directions-rotations-and-hyperspheres.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="7" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression Models</a></li>
<li class="chapter" data-level="8" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>8</b> Time-Series Models</a></li>
<li class="chapter" data-level="9" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>9</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="10" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>10</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="11" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>11</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="12" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>12</b> Finite Mixtures</a></li>
<li class="chapter" data-level="13" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>13</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="14" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>14</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="15" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>15</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="16" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>16</b> Clustering Models</a></li>
<li class="chapter" data-level="17" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>17</b> Gaussian Processes</a></li>
<li class="chapter" data-level="18" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>18</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="19" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>19</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="20" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>20</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="21" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>21</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="22" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>22</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="23" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>23</b> User-Defined Functions</a></li>
<li class="chapter" data-level="24" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>24</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="25" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>25</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="26" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>26</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="27" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>27</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="28" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>28</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="29" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>29</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="30" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>30</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>31</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="32" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>32</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gaussian-processes.chapter" class="section level1">
<h1><span class="header-section-number">17</span> Gaussian Processes</h1>
<p>Gaussian processes are continuous stochastic processes and thus may be
interpreted as providing a probability distribution over functions. A
probability distribution over continuous functions may be viewed,
roughly, as an uncountably infinite collection of random variables,
one for each valid input. The generality of the supported functions
makes Gaussian priors popular choices for priors in general
multivariate (non-linear) regression problems.</p>
<p>The defining feature of a Gaussian process is that the joint distribution of
the function’s value at a finite number of input points is a multivariate
normal distribution. This makes it tractable to both fit models from finite
amounts of observed data and make predictions for finitely many new data
points.</p>
<p>Unlike a simple multivariate normal distribution, which is
parameterized by a mean vector and covariance matrix, a Gaussian
process is parameterized by a mean function and covariance function.
The mean and covariance functions apply to vectors of inputs and
return a mean vector and covariance matrix which provide the mean and
covariance of the outputs corresponding to those input points in the
functions drawn from the process.</p>
<p>Gaussian processes can be encoded in Stan by implementing their mean and
covariance functions and plugging the result into the Gaussian form of their
sampling distribution, or by using the specialized covariance functions
outlined below. This form of model is straightforward and may be used for
simulation, model fitting, or posterior predictive inference. A more efficient
Stan implementation for the GP with a normally distributed outcome marginalizes
over the latent Gaussian process, and applies a Cholesky-factor
reparameterization of the Gaussian to compute the likelihood and the posterior
predictive distribution analytically.</p>
<p>After defining Gaussian processes, this chapter covers the basic
implementations for simulation, hyperparameter estimation, and
posterior predictive inference for univariate regressions,
multivariate regressions, and multivariate logistic regressions.
Gaussian processes are general, and by necessity this chapter
only touches on some basic models. For more information, see
<span class="citation">Rasmussen and Williams (<a href="#ref-RasmussenWilliams:2006">2006</a>)</span>.</p>
<div id="gaussian-process-regression" class="section level2">
<h2><span class="header-section-number">17.1</span> Gaussian Process Regression</h2>
<p>The data for a multivariate Gaussian process regression consists of a
series of <span class="math inline">\(N\)</span> inputs <span class="math inline">\(x_1,\ldots,x_N \in \mathbb{R}^D\)</span> paired with outputs
<span class="math inline">\(y_1,\ldots,y_N \in \mathbb{R}\)</span>. The defining feature of Gaussian
processes is that the probability of a finite number of outputs <span class="math inline">\(y\)</span>
conditioned on their inputs <span class="math inline">\(x\)</span> is Gaussian:
<span class="math display">\[
y \sim \mathsf{multivariate\ normal}(m(x), K(x | \theta)),
\]</span>
where <span class="math inline">\(m(x)\)</span> is an <span class="math inline">\(N\)</span>-vector and <span class="math inline">\(K(x | \theta)\)</span> is an <span class="math inline">\(N \times N\)</span>
covariance matrix. The mean function <span class="math inline">\(m : \mathbb{R}^{N \times D} \rightarrow \mathbb{R}^{N}\)</span> can be anything, but the covariance function
<span class="math inline">\(K : \mathbb{R}^{N \times D} \rightarrow \mathbb{R}^{N \times N}\)</span> must produce
a positive-definite matrix for any input <span class="math inline">\(x\)</span>.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a></p>
<p>A popular covariance function, which will be used in the implementations later
in this chapter, is an exponentiated quadratic function,
<span class="math display">\[
  K(x | \alpha, \rho, \sigma)_{i, j}
= \alpha^2
\exp \left(
- \dfrac{1}{2 \rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2
\right)
+ \delta_{i, j} \sigma^2,
\]</span>
where <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\sigma\)</span> are hyperparameters defining the
covariance function and where <span class="math inline">\(\delta_{i, j}\)</span> is the Kronecker delta
function with value 1 if <span class="math inline">\(i = j\)</span> and value 0 otherwise; this
test is between the indexes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, not between values <span class="math inline">\(x_i\)</span> and
<span class="math inline">\(x_j\)</span>. This kernel is obtained through a convolution of two
independent Gaussian processes, <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>, with kernels
<span class="math display">\[
  K_1(x | \alpha, \rho)_{i, j}
= \alpha^2
\exp \left(
- \dfrac{1}{2 \rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2
\right)
\]</span>
and
<span class="math display">\[
  K_2(x | \sigma)_{i, j}
=
 \delta_{i, j} \sigma^2,
\]</span></p>
<p>The addition of <span class="math inline">\(\sigma^2\)</span> on the diagonal is important
to ensure the positive definiteness of the resulting matrix in the case of
two identical inputs <span class="math inline">\(x_i = x_j\)</span>. In statistical terms, <span class="math inline">\(\sigma\)</span> is
the scale of the noise term in the regression.</p>
<p>The hyperparameter <span class="math inline">\(\rho\)</span> is the <em>length-scale</em>, and corresponds to the
frequency of the functions represented by the Gaussian process prior with
respect to the domain. Values of <span class="math inline">\(\rho\)</span> closer to zero lead the GP to represent
high-frequency functions, whereas larger values of <span class="math inline">\(\rho\)</span> lead to low-frequency
functions. The hyperparameter <span class="math inline">\(\alpha\)</span> is the <em>marginal standard
deviation</em>. It controls the magnitude of the range of the function represented
by the GP. If you were to take the standard deviation of many draws from the GP
<span class="math inline">\(f_1\)</span> prior at a single input <span class="math inline">\(x\)</span> conditional on one value of <span class="math inline">\(\alpha\)</span> one
would recover <span class="math inline">\(\alpha\)</span>.</p>
<p>The only term in the squared exponential covariance function involving
the inputs <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is their vector difference, <span class="math inline">\(x_i - x_j\)</span>.
This produces a process with stationary covariance in the sense that
if an input vector <span class="math inline">\(x\)</span> is translated by a vector <span class="math inline">\(\epsilon\)</span> to <span class="math inline">\(x + \epsilon\)</span>, the covariance at any pair of outputs is unchanged, because
<span class="math inline">\(K(x | \theta) = K(x + \epsilon| \theta)\)</span>.</p>
<p>The summation involved is just the squared Euclidean distance between
<span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> (i.e., the <span class="math inline">\(L_2\)</span> norm of their difference, <span class="math inline">\(x_i - x_j\)</span>). This results in support for smooth functions in the process.
The amount of variation in the function is controlled by the free
hyperparameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\sigma\)</span>.</p>
<p>Changing the notion of distance from Euclidean to taxicab distance
(i.e., an <span class="math inline">\(L_1\)</span> norm) changes the support to functions which are
continuous but not smooth.</p>
</div>
<div id="simulating-from-a-gaussian-process" class="section level2">
<h2><span class="header-section-number">17.2</span> Simulating from a Gaussian Process</h2>
<p>It is simplest to start with a Stan model that does nothing more than
simulate draws of functions <span class="math inline">\(f\)</span> from a Gaussian process. In practical
terms, the model will draw values <span class="math inline">\(y_n = f(x_n)\)</span> for finitely many
input points <span class="math inline">\(x_n\)</span>.</p>
<p>The Stan model defines the mean and covariance functions in a
transformed data block and then samples outputs <span class="math inline">\(y\)</span> in the model using
a multivariate normal distribution. To make the model concrete, the
squared exponential covariance function described in the previous section
will be used with hyperparameters set to <span class="math inline">\(\alpha^2 = 1\)</span>, <span class="math inline">\(\rho^2 = 1\)</span>,
and <span class="math inline">\(\sigma^2 = 0.1\)</span>, and the mean function <span class="math inline">\(m\)</span> is defined to always
return the zero vector, <span class="math inline">\(m(x) = {\bf 0}\)</span>. Consider the following
implementation of a Gaussian process simulator.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
}
transformed data {
  matrix[N, N] K;
  vector[N] mu = rep_vector(0, N);
  for (i in 1:(N - 1)) {
    K[i, i] = 1 + 0.1;
    for (j in (i + 1):N) {
      K[i, j] = exp(-0.5 * square(x[i] - x[j]));
      K[j, i] = K[i, j];
    }
  }
  K[N, N] = 1 + 0.1;
}
parameters {
  vector[N] y;
}
model {
  y ~ multi_normal(mu, K);
}</code></pre>
<p>The above model can also be written more compactly using the specialized
covariance function that implements the exponentiated quadratic kernel.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
}
transformed data {
  matrix[N, N] K = cov_exp_quad(x, 1.0, 1.0);
  vector[N] mu = rep_vector(0, N);
  for (n in 1:N)
    K[n, n] = K[n, n] + 0.1;
}
parameters {
  vector[N] y;
}
model {
  y ~ multi_normal(mu, K);
}</code></pre>
<p>The input data are just the vector of inputs <code>x</code> and its size
<code>N</code>. Such a model can be used with values of <code>x</code> evenly
spaced over some interval in order to plot sample draws of functions
from a Gaussian process.</p>
<div id="multivariate-inputs" class="section level3 unnumbered">
<h3>Multivariate Inputs</h3>
<p>Only the input data needs to change in moving from a univariate model to a
multivariate model.</p>
<p>The only lines that change from the univariate model above are as follows.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; D;
  vector[D] x[N];
}
transformed data {
...
...</code></pre>
<p>The data are now declared as an array of vectors instead of an array of
scalars; the dimensionality <code>D</code> is also declared.</p>
<p>In the remainder of the chapter, univariate models will be used for simplicity,
but any of the models could be changed to multivariate in the same way as the
simple sampling model. The only extra computational overhead from a
multivariate model is in the distance calculation.</p>
</div>
<div id="cholesky-factored-and-transformed-implementation" class="section level3 unnumbered">
<h3>Cholesky Factored and Transformed Implementation</h3>
<p>A more efficient implementation of the simulation model can be
coded in Stan by relocating, rescaling and rotating an isotropic standard
normal variate. Suppose <span class="math inline">\(\eta\)</span> is an an isotropic standard normal variate
<span class="math display">\[
\eta \sim \mathsf{normal}({\bf 0}, {\bf 1}),
\]</span>
where <span class="math inline">\({\bf 0}\)</span> is an <span class="math inline">\(N\)</span>-vector of 0 values and <span class="math inline">\({\bf 1}\)</span> is the <span class="math inline">\(N \times N\)</span> identity matrix. Let <span class="math inline">\(L\)</span> be the Cholesky decomposition of
<span class="math inline">\(K(x | \theta)\)</span>, i.e., the lower-triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(LL^{\top} = K(x | \theta)\)</span>. Then the transformed variable <span class="math inline">\(\mu + L\eta\)</span> has the intended
target distribution,
<span class="math display">\[
  \mu + L\eta \sim \mathsf{multivariate\ normal}(\mu(x), K(x | \theta)).
\]</span></p>
<p>This transform can be applied directly to Gaussian process
simulation.</p>
<p>This model has the same data declarations for <code>N</code> and <code>x</code>,
and the same transformed data definitions of <code>mu</code> and
<code>K</code> as the previous model, with the addition of a transformed
data variable for the Cholesky decomposition. The parameters change
to the raw parameters sampled from an isotropic standard normal, and the
actual samples are defined as generated quantities.</p>
<pre><code>...
transformed data {
  matrix[N, N] L;
...
  L = cholesky_decompose(K);
}
parameters {
  vector[N] eta;
}
model {
  eta ~ std_normal();
}
generated quantities {
  vector[N] y;
  y = mu + L * eta;
}</code></pre>
<p>The Cholesky decomposition is only computed once, after the data are
loaded and the covariance matrix <code>K</code> computed. The isotropic
normal distribution for <code>eta</code> is specified as a vectorized
univariate distribution for efficiency; this specifies that each
<code>eta[n]</code> has an independent standard normal distribution. The sampled
vector <code>y</code> is then defined as a generated quantity using a direct
encoding of the transform described above.</p>
</div>
</div>
<div id="fit-gp.section" class="section level2">
<h2><span class="header-section-number">17.3</span> Fitting a Gaussian Process</h2>
<div id="gp-with-a-normal-outcome" class="section level3 unnumbered">
<h3>GP with a normal outcome</h3>
<p>The full generative model for a GP with a normal outcome,
<span class="math inline">\(y \in \mathbb{R}^N\)</span>, with inputs <span class="math inline">\(x \in \mathbb{R}^N\)</span>, for a finite <span class="math inline">\(N\)</span>:</p>
<p><span class="math display">\[ \begin{aligned}
  \rho &amp; \sim \mathsf{InvGamma}(5, 5) \\
  \alpha &amp; \sim \mathsf{normal}(0, 1) \\
  \sigma &amp; \sim \mathsf{normal}(0, 1) \\
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\
  y_i &amp; \sim \mathsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\}
\end{aligned} \]</span></p>
<p>With a normal outcome, it is possible to integrate out the Gaussian
process <span class="math inline">\(f\)</span>, yielding the more parsimonious model:</p>
<p><span class="math display">\[ \begin{aligned}
  \rho &amp; \sim \mathsf{InvGamma}(5, 5) \\
  \alpha &amp; \sim \mathsf{normal}(0, 1) \\
  \sigma &amp; \sim \mathsf{normal}(0, 1) \\
  y &amp; \sim \mathsf{multivariate\ normal}
  \left(0, K(x | \alpha, \rho) + \mathbf{I}_N \sigma^2\right) \\
\end{aligned} \]</span></p>
<p>It can be more computationally efficient when dealing with a normal
outcome to integrate out the Gaussian process, because this yields a
lower-dimensional parameter space over which to do inference. We’ll fit
both models in Stan. The former model will be referred to as the latent
variable GP, while the latter will be called the marginal likelihood
GP.</p>
<p>The hyperparameters controlling the covariance function of a Gaussian
process can be fit by assigning them priors, like we have in the
generative models above, and then computing the posterior distribution
of the hyperparameters given observed data. The priors on the
parameters should be defined based on prior knowledge of the scale of
the output values (<span class="math inline">\(\alpha\)</span>), the scale of the output noise
(<span class="math inline">\(\sigma\)</span>), and the scale at which distances are measured among inputs
(<span class="math inline">\(\rho\)</span>). See the <a href="gaussian-processes-chapter.html#priors-gp.section">Gaussian process priors
section</a> for more information about how to specify
appropriate priors for the hyperparameters.</p>
<p>The Stan program implementing the marginal likelihood GP is shown below. The
program is similar to the Stan programs that implement the simulation GPs
above, but because we are doing inference on the hyperparameters, we need to
calculate the covariance matrix <code>K</code> in the model block, rather than
the transformed data block.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
}
transformed data {
  vector[N] mu = rep_vector(0, N);
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
}
model {
  matrix[N, N] L_K;
  matrix[N, N] K = cov_exp_quad(x, alpha, rho);
  real sq_sigma = square(sigma);

  // diagonal elements
  for (n in 1:N)
    K[n, n] = K[n, n] + sq_sigma;

  L_K = cholesky_decompose(K);

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();

  y ~ multi_normal_cholesky(mu, L_K);
}</code></pre>
<p>The data block now declares a vector <code>y</code> of observed values <code>y[n]</code>
for inputs <code>x[n]</code>. The transformed data block now only defines the mean
vector to be zero. The three hyperparameters are defined as parameters
constrained to be non-negative. The computation of the covariance matrix
<code>K</code> is now in the model block because it involves unknown parameters and
thus can’t simply be precomputed as transformed data. The rest of the model
consists of the priors for the hyperparameters and the multivariate
Cholesky-parameterized normal likelihood, only now the value <code>y</code> is known
and the covariance matrix <code>K</code> is an unknown dependent on the
hyperparameters, allowing us to learn the hyperparameters.</p>
<p>We have used the Cholesky parameterized multivariate normal rather
than the standard parameterization because it allows us to the
<code>cholesky_decompose</code> function which has been optimized for both small
and large matrices. When working with small matrices the differences
in computational speed between the two approaches will not be
noticeable, but for larger matrices (<span class="math inline">\(N \gtrsim 100\)</span>) the Cholesky
decomposition version will be faster.</p>
<p>Hamiltonian Monte Carlo sampling is fast and effective for hyperparameter
inference in this model <span class="citation">Neal (<a href="#ref-Neal:1997">1997</a>)</span>. If the posterior is
well-concentrated for the hyperparameters the Stan implementation will fit
hyperparameters in models with a few hundred data points in seconds.</p>
<div id="latent-variable-gp" class="section level4 unnumbered">
<h4>Latent variable GP</h4>
<p>We can also explicitly code the latent variable formulation of a GP in Stan.
This will be useful for when the outcome is not normal. We’ll need to add a
small positive term, <span class="math inline">\(\delta\)</span> to the diagonal of the covariance matrix in order
to ensure that our covariance matrix remains positive definite.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}</code></pre>
<p>Two differences between the latent variable GP and the marginal likelihood GP
are worth noting. The first is that we have augmented our parameter block with
a new parameter vector of length <span class="math inline">\(N\)</span> called <span class="math inline">\(`eta`\)</span>. This is used in the model
block to generate a multivariate normal vector called <span class="math inline">\(f\)</span>, corresponding to the
latent GP. We put a <span class="math inline">\(\mathsf{normal}(0,1)\)</span> prior on <code>eta</code> like we did in the
Cholesky-parameterized GP in the simulation section. The second difference is
that our likelihood is now univariate, though we could code <span class="math inline">\(N\)</span> likelihood
terms as one <span class="math inline">\(N\)</span>-dimensional multivariate normal with an identity covariance
matrix multiplied by <span class="math inline">\(\sigma^2\)</span>. However, it is more efficient to use the
vectorized statement as shown above.</p>
</div>
</div>
<div id="discrete-outcomes-with-gaussian-processes" class="section level3 unnumbered">
<h3>Discrete outcomes with Gaussian Processes</h3>
<p>Gaussian processes can be generalized the same way as standard linear
models by introducing a link function. This allows them to be used as
discrete data models.</p>
<div id="poisson-gp" class="section level4 unnumbered">
<h4>Poisson GP</h4>
<p>If we want to model count data, we can remove the <span class="math inline">\(\sigma\)</span> parameter, and use
<code>poisson_log</code>, which implements a log link, for our likelihood rather
than <code>normal</code>. We can also add an overall mean parameter, <span class="math inline">\(a\)</span>, which
will account for the marginal expected value for <span class="math inline">\(y\)</span>. We do this because we
cannot center count data like we would for normally distributed data.</p>
<pre><code>data {
...
  int&lt;lower=0&gt; y[N];
...
}
...
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real a;
  vector[N] eta;
}
model {
...
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  y ~ poisson_log(a + f);
}</code></pre>
</div>
<div id="logistic-gaussian-process-regression" class="section level4 unnumbered">
<h4>Logistic Gaussian Process Regression</h4>
<p>For binary classification problems, the observed outputs <span class="math inline">\(z_n \in \{ 0,1 \}\)</span> are binary. These outputs are modeled using a Gaussian
process with (unobserved) outputs <span class="math inline">\(y_n\)</span> through the logistic link,
<span class="math display">\[
z_n \sim \mathsf{Bernoulli}(\mbox{logit}^{-1}(y_n)),
\]</span>
or in other words,
<span class="math display">\[
\mbox{Pr}[z_n = 1] = \mbox{logit}^{-1}(y_n).
\]</span></p>
<p>We can extend our latent variable GP Stan program to deal with classification
problems. Below <span class="math inline">\(a\)</span> is the bias term, which can help account for imbalanced
classes in the training data:</p>
<pre><code>data {
...
  int&lt;lower=0, upper=1&gt; z[N];
...
}
...
model {
...

  y ~ bernoulli_logit(a + f);
}</code></pre>
</div>
</div>
<div id="automatic-relevance-determination" class="section level3 unnumbered">
<h3>Automatic Relevance Determination</h3>
<p>If we have multivariate inputs <span class="math inline">\(x \in \mathbb{R}^D\)</span>, the squared exponential
covariance function can be further generalized by fitting a scale
parameter <span class="math inline">\(\rho_d\)</span> for each dimension <span class="math inline">\(d\)</span>,
<span class="math display">\[
  k(x | \alpha, \vec{\rho}, \sigma)_{i, j} = \alpha^2 \exp
\left(-\dfrac{1}{2}
\sum_{d=1}^D \dfrac{1}{\rho_d^2} (x_{i,d} - x_{j,d})^2
\right)
+ \delta_{i, j}\sigma^2.
\]</span>
The estimation of <span class="math inline">\(\rho\)</span> was termed “automatic relevance determination” in
<span class="citation">Neal (<a href="#ref-Neal:1996">1996</a><a href="#ref-Neal:1996">a</a>)</span>, but this is misleading, because the magnitude the scale of
the posterior for each <span class="math inline">\(\rho_d\)</span> is dependent on the scaling of the input data
along dimension <span class="math inline">\(d\)</span>. Moreover, the scale of the parameters <span class="math inline">\(\rho_d\)</span> measures
non-linearity along the <span class="math inline">\(d\)</span>-th dimension, rather than “relevance”
<span class="citation">Piironen and Vehtari (<a href="#ref-PiironenVehtari:2016">2016</a>)</span>.</p>
<p>A priori, the closer <span class="math inline">\(\rho_d\)</span> is to zero, the more nonlinear the
conditional mean in dimension <span class="math inline">\(d\)</span> is. A posteriori, the actual dependencies
between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> play a role. With one covariate <span class="math inline">\(x_1\)</span> having a
linear effect and another covariate <span class="math inline">\(x_2\)</span> having a nonlinear effect,
it is possible that <span class="math inline">\(\rho_1 &gt; \rho_2\)</span> even if the predictive relevance
of <span class="math inline">\(x_1\)</span> is higher <span class="citation">(Rasmussen and Williams <a href="#ref-RasmussenWilliams:2006">2006</a>, 80)</span>.
The collection of <span class="math inline">\(\rho_d\)</span> (or <span class="math inline">\(1/\rho_d\)</span>) parameters can also be
modeled hierarchically.</p>
<p>The implementation of automatic relevance determination in Stan is
straightforward, though it currently requires the user to directly code the
covariance matrix. We’ll write a function to generate the Cholesky of the
covariance matrix called <code>L_cov_exp_quad_ARD</code>.</p>
<pre><code>functions {
  matrix L_cov_exp_quad_ARD(vector[] x,
                            real alpha,
                            vector rho,
                            real delta) {
    int N = size(x);
    matrix[N, N] K;
    real sq_alpha = square(alpha);
    for (i in 1:(N-1)) {
      K[i, i] = sq_alpha + delta;
      for (j in (i + 1):N) {
        K[i, j] = sq_alpha
                      * exp(-0.5 * dot_self((x[i] - x[j]) ./ rho));
        K[j, i] = K[i, j];
      }
    }
    K[N, N] = sq_alpha + delta;
    return cholesky_decompose(K);
  }
}
data {
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; D;
  vector[D] x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  vector&lt;lower=0&gt;[D] rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K = L_cov_exp_quad_ARD(x, alpha, rho, delta);
    f = L_K * eta;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}</code></pre>
</div>
<div id="priors-gp.section" class="section level3">
<h3><span class="header-section-number">17.3.1</span> Priors for Gaussian Process Parameters {-}</h3>
<p>Formulating priors for GP hyperparameters requires the analyst to consider the
inherent statistical properties of a GP, the GP’s purpose in the model, and the
numerical issues that may arise in Stan when estimating a GP.</p>
<p>Perhaps most importantly, the parameters <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\alpha\)</span> are weakly
identified <span class="citation">Zhang (<a href="#ref-zhang-gp:2004">2004</a>)</span>. The ratio of the two
parameters is well-identified, but in practice we put independent priors on the
two hyperparameters because these two quantities are more interpretable than
their ratio.</p>
<div id="priors-for-length-scale" class="section level4 unnumbered">
<h4>Priors for length-scale</h4>
<p>GPs are a flexible class of priors and, as such, can represent a wide spectrum
of functions. For length scales below the minimum spacing of the covariates
the GP likelihood plateaus. Unless regularized by a prior, this flat
likelihood induces considerable posterior mass at small length scales where the
observation variance drops to zero and the functions supported by the GP being
to exactly interpolate between the input data. The resulting posterior not
only significantly overfits to the input data, it also becomes hard to
accurately sample using Euclidean HMC.</p>
<p>We may wish to put further soft constraints on the length-scale, but these are
dependent on how the GP is used in our statistical model.</p>
<p>If our model consists of only the GP, i.e.:</p>
<p><span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\
  y_i &amp; \sim \mathsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\} \\
  &amp; x \in \mathbb{R}^{N \times D}, \, f \in \mathbb{R}^N
\end{aligned} \]</span></p>
<p>we likely don’t need constraints beyond penalizing small
length-scales. We’d like to allow the GP prior to represent both
high-frequency and low-frequency functions, so our prior should put
non-negligible mass on both sets of functions. In this case, an
inverse gamma, <code>inv_gamma_lpdf</code> in Stan’s language, will work
well as it has a sharp left tail that puts negligible mass on
infinitesimal length-scales, but a generous right tail, allowing for
large length-scales. Inverse gamma priors will avoid infinitesimal length-scales
because the density is zero at zero, so the posterior for length-scale will be
pushed away from zero. An inverse gamma distribution is one of many
zero-avoiding or boundary-avoiding distributions. See
 for more on boundary-avoiding priors.</p>
<p>If we’re using the GP as a component in a larger model that includes an overall
mean and fixed effects for the same variables we’re using as the domain for the
GP, i.e.:</p>
<p><span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\ y_i &amp;
  \sim \mathsf{normal}(\beta_0 + x_i \beta_{[1:D]} + f_i, \sigma) \, \forall i
  \in \{1, \dots, N\} \\ &amp; x_i^T, \beta_{[1:D]} \in \mathbb{R}^D,\, x \in \mathbb{R}^{N
  \times D},\, f \in \mathbb{R}^N
\end{aligned} \]</span></p>
<p>we’ll likely want to constrain large length-scales as well. A length scale
that is larger than the scale of the data yields a GP posterior that is
practically linear (with respect to the particular covariate) and increasing
the length scale has little impact on the likelihood. This will introduce
nonidentifiability in our model, as both the fixed effects and the GP will
explain similar variation. In order to limit the amount of overlap between the
GP and the linear regression, we should use a prior with a sharper right tail
to limit the GP to higher-frequency functions. We can use a generalized inverse
Gaussian distribution:</p>
<p><span class="math display">\[ \begin{aligned}
  f(x | a, b, p) &amp; = \dfrac{(a/b)^{p/2}}{2K_p(\sqrt{ab})} x^{p - 1}\exp(-(ax + b
  / x)/2) \\
  &amp; x, a, b \in \mathbb{R}^{+}, \, p \in \mathbb{Z}
\end{aligned} \]</span></p>
<p>which has an inverse gamma left tail if <span class="math inline">\(p \leq 0\)</span> and an inverse Gaussian
right tail. This has not yet been implemented in Stan’s math library, but it
is possible to implement as a user defined function:</p>
<pre><code>functions {
  real generalized_inverse_gaussian_lpdf(real x, int p,
                                        real a, real b) {
    return p * 0.5 * log(a / b)
      - log(2 * modified_bessel_second_kind(p, sqrt(a * b)))
      + (p - 1) * log(x)
      - (a * x + b / x) * 0.5;
 }
}
data {
...</code></pre>
<p>If we have high-frequency covariates in our fixed effects, we may wish to
further regularize the GP away from high-frequency functions, which means we’ll
need to penalize smaller length-scales. Luckily, we have a useful way of
thinking about how length-scale affects the frequency of the functions
supported the GP. If we were to repeatedly draw from a zero-mean GP with a
length-scale of <span class="math inline">\(\rho\)</span> in a fixed-domain <span class="math inline">\([0,T]\)</span>, we would get a distribution
for the number of times each draw of the GP crossed the zero axis. The
expectation of this random variable, the number of zero crossings, is <span class="math inline">\(T / \pi \rho\)</span>. You can see that as <span class="math inline">\(\rho\)</span> decreases, the expectation of the number of
upcrossings increases as the GP is representing higher-frequency functions.
Thus, this is a good statistic to keep in mind when setting a lower-bound for
our prior on length-scale in the presence of high-frequency covariates.
However, this statistic is only valid for one-dimensional inputs.</p>
</div>
<div id="priors-for-marginal-standard-deviation" class="section level4 unnumbered">
<h4>Priors for marginal standard deviation</h4>
<p>The parameter <span class="math inline">\(\alpha\)</span> corresponds to how much of the variation is
explained by the regression function and has a similar role to the
prior variance for linear model weights. This means the prior can be
the same as used in linear models, such as a half-<span class="math inline">\(t\)</span> prior on <span class="math inline">\(\alpha\)</span>.</p>
<p>A half-<span class="math inline">\(t\)</span> or half-Gaussian prior on alpha also has the benefit of putting
nontrivial prior mass around zero. This allows the GP support the zero
functions and allows the possibility that the GP won’t contribute to the
conditional mean of the total output.</p>
</div>
</div>
<div id="predictive-inference-with-a-gaussian-process" class="section level3 unnumbered">
<h3>Predictive Inference with a Gaussian Process</h3>
<p>Suppose for a given sequence of inputs <span class="math inline">\(x\)</span> that the corresponding
outputs <span class="math inline">\(y\)</span> are observed. Given a new sequence of inputs <span class="math inline">\(\tilde{x}\)</span>,
the posterior predictive distribution of their labels is computed by
sampling outputs <span class="math inline">\(\tilde{y}\)</span> according to
<span class="math display">\[
p(\tilde{y}|\tilde{x},x,y)
\ = \
\frac{p(\tilde{y}, y|\tilde{x},x)}
     {p(y|x)}
\ \propto \
p(\tilde{y}, y|\tilde{x},x).
\]</span></p>
<p>A direct implementation in Stan defines a model in terms of the
joint distribution of the observed <span class="math inline">\(y\)</span> and unobserved <span class="math inline">\(\tilde{y}\)</span>.</p>
<pre><code>data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  vector[N1] y1;
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  real x[N];
  for (n1 in 1:N1) x[n1] = x1[n1];
  for (n2 in 1:N2) x[N1 + n2] = x2[n2];
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y1 ~ normal(f[1:N1], sigma);
}
generated quantities {
  vector[N2] y2;
  for (n2 in 1:N2)
    y2[n2] = normal_rng(f[N1 + n2], sigma);
}</code></pre>
<p>The input vectors <code>x1</code> and <code>x2</code> are declared as data, as is the
observed output vector <code>y1</code>. The unknown output vector <code>y2</code>, which
corresponds to input vector <code>x2</code>, is declared in the generated quantities
block and will be sampled when the model is executed.</p>
<p>A transformed data block is used to combine the input vectors
<code>x1</code> and <code>x2</code> into a single vector <code>x</code>.</p>
<p>The model block declares and defines a local variable for the combined output
vector <code>f</code>, which consists of the concatenation of the conditional mean
for known outputs <code>y1</code> and unknown outputs <code>y2</code>. Thus the
combined output vector <code>f</code> is aligned with the combined
input vector <code>x</code>. All that is left is to define the univariate
normal sampling statement for <code>y</code>.</p>
<p>The generated quantities block defines the quantity <code>y2</code>. We generate
<code>y2</code> by sampling <code>N2</code> univariate normals with each mean corresponding
to the appropriate element in <code>f</code>.</p>
<div id="predictive-inference-in-non-gaussian-gps" class="section level4 unnumbered">
<h4>Predictive Inference in non-Gaussian GPs</h4>
<p>We can do predictive inference in non-Gaussian GPs in much the
same way as we do with Gaussian GPs.</p>
<p>Consider the following full model for prediction using logistic Gaussian
process regression.</p>
<pre><code>data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  int&lt;lower=0, upper=1&gt; z1[N1];
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  real x[N];
  for (n1 in 1:N1) x[n1] = x1[n1];
  for (n2 in 1:N2) x[N1 + n2] = x2[n2];
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real a;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  z1 ~ bernoulli_logit(a + f[1:N1]);
}
generated quantities {
  int z2[N2];
  for (n2 in 1:N2)
    z2[n2] = bernoulli_logit_rng(a + f[N1 + n2]);
}</code></pre>
</div>
<div id="analytical-form-of-joint-predictive-inference" class="section level4 unnumbered">
<h4>Analytical Form of Joint Predictive Inference</h4>
<p>Bayesian predictive inference for Gaussian processes with Gaussian observations
can be sped up by deriving the posterior analytically, then directly sampling
from it.</p>
<p>Jumping straight to the result,
<span class="math display">\[
p(\tilde{y}|\tilde{x},y,x)
=
\mathsf{normal}(K^{\top}\Sigma^{-1}y,\
                \Omega - K^{\top}\Sigma^{-1}K),
\]</span>
where <span class="math inline">\(\Sigma = K(x | \alpha, \rho, \sigma)\)</span> is the result of applying the covariance
function to the inputs <span class="math inline">\(x\)</span> with observed outputs <span class="math inline">\(y\)</span>, <span class="math inline">\(\Omega = K(\tilde{x} | \alpha, \rho)\)</span> is the result of applying the covariance function to the
inputs <span class="math inline">\(\tilde{x}\)</span> for which predictions are to be inferred, and <span class="math inline">\(K\)</span>
is the matrix of covariances between inputs <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span>, which
in the case of the exponentiated quadratic covariance function
would be</p>
<p><span class="math display">\[
K(x | \alpha, \rho)_{i, j} = \eta^2 \exp(-\dfrac{1}{2 \rho^2}
\sum_{d=1}^D (x_{i,d} - \tilde{x}_{j,d})^2).
\]</span></p>
<p>There is no noise term including <span class="math inline">\(\sigma^2\)</span> because the indexes of
elements in <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span> are never the same.</p>
<p>This Stan code below uses the analytic form of the posterior and provides
sampling of the resulting multivariate normal through the Cholesky
decomposition. The data declaration is the same as for the latent variable
example, but we’ve defined a function called <code>gp_pred_rng</code> which will
generate a draw from the posterior predictive mean conditioned on observed data
<code>y1</code>. The code uses a Cholesky decomposition in triangular solves in order
to cut down on the the number of matrix-matrix multiplications when computing
the conditional mean and the conditional covariance of <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<pre><code>functions {
  vector gp_pred_rng(real[] x2,
                     vector y1,
                     real[] x1,
                     real alpha,
                     real rho,
                     real sigma,
                     real delta) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] L_K;
      vector[N1] K_div_y1;
      matrix[N1, N2] k_x1_x2;
      matrix[N1, N2] v_pred;
      vector[N2] f2_mu;
      matrix[N2, N2] cov_f2;
      matrix[N2, N2] diag_delta;
      matrix[N1, N1] K;
      K = cov_exp_quad(x1, alpha, rho);
      for (n in 1:N1)
        K[n, n] = K[n,n] + square(sigma);
      L_K = cholesky_decompose(K);
      K_div_y1 = mdivide_left_tri_low(L_K, y1);
      K_div_y1 = mdivide_right_tri_low(K_div_y1&#39;, L_K)&#39;;
      k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);
      f2_mu = (k_x1_x2&#39; * K_div_y1);
      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      cov_f2 = cov_exp_quad(x2, alpha, rho) - v_pred&#39; * v_pred;
      diag_delta = diag_matrix(rep_vector(delta, N2));

      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);
    }
    return f2;
  }
}
data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  vector[N1] y1;
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  vector[N1] mu = rep_vector(0, N1);
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
}
model {
  matrix[N1, N1] L_K;
  {
    matrix[N1, N1] K = cov_exp_quad(x1, alpha, rho);
    real sq_sigma = square(sigma);

    // diagonal elements
    for (n1 in 1:N1)
      K[n1, n1] = K[n1, n1] + sq_sigma;

    L_K = cholesky_decompose(K);
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();

  y1 ~ multi_normal_cholesky(mu, L_K);
}
generated quantities {
  vector[N2] f2;
  vector[N2] y2;

  f2 = gp_pred_rng(x2, y1, x1, alpha, rho, sigma, delta);
  for (n2 in 1:N2)
    y2[n2] = normal_rng(f2[n2], sigma);
}</code></pre>
</div>
</div>
<div id="multiple-output-gaussian-processes" class="section level3 unnumbered">
<h3>Multiple-output Gaussian processes</h3>
<p>Suppose we have observations <span class="math inline">\(y_i \in \mathbb{R}^M\)</span> observed at
<span class="math inline">\(x_i \in \mathbb{R}^K\)</span>. One can model the data like so:
<span class="math display">\[ \begin{aligned}
  y_i &amp; \sim \mathsf{multivariate\ normal}(f(x_i), \mathbf{I}_M \sigma^2) \\
  f(x) &amp; \sim \mathsf{GP}(m(x), K(x | \theta, \phi)) \\
  K(x &amp; | \theta) \in \mathbb{R}^{M \times M}, \, f(x), \, m(x) \in \mathbb{R}^M
\end{aligned} \]</span>
where the <span class="math inline">\(K(x, x^\prime | \theta, \phi)_{[m, m^\prime]}\)</span> entry defines the
covariance between <span class="math inline">\(f_m(x)\)</span> and <span class="math inline">\(f_{m^\prime}(x^\prime)(x)\)</span>. This construction
of Gaussian processes allows us to learn the covariance between the output
dimensions of <span class="math inline">\(f(x)\)</span>. If we parameterize our kernel <span class="math inline">\(K\)</span>:
<span class="math display">\[ \begin{aligned} K(x, x^\prime | \theta, \phi)_{[m, m^\prime]} = k(x, x^\prime |
\theta) k(m, m^\prime | \phi) \end{aligned} \]</span>
then our finite dimensional generative model for the above is:
<span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{Matrixnormalal}(m(x), K(x | \alpha, \rho), C(\phi)) \\
  y_{i, m} &amp; \sim \mathsf{normal}(f_{i,m}, \sigma) \\
  f &amp; \in \mathbb{R}^{N \times M}
\end{aligned} \]</span>
where <span class="math inline">\(K(x | \alpha, \rho)\)</span> is the exponentiated quadratic kernel we’ve used
throughout this chapter, and <span class="math inline">\(C(\phi)\)</span> is a positive-definite matrix,
parameterized by some vector <span class="math inline">\(\phi\)</span>.</p>
<p>The matrix normal distribution has two covariance matrices: <span class="math inline">\(K(x | \alpha, \rho)\)</span> to encode column covariance, and <span class="math inline">\(C(\phi)\)</span> to define row
covariance. The salient features of the matrix normal are that the rows
of the matrix <span class="math inline">\(f\)</span> are distributed:
<span class="math display">\[ \begin{aligned} f_{[n,]} \sim \mathsf{multivariate\ normal}(m(x)_{[n,]}, K(x | \alpha,
\rho)_{[n,n]} C(\phi)) \end{aligned} \]</span> and that the columns of the matrix <span class="math inline">\(f\)</span> are
distributed: <span class="math display">\[ \begin{aligned} f_{[,m]} \sim \mathsf{multivariate\ normal}(m(x)_{[,m]}, K(x
  | \alpha, \rho) C(\phi)_{[m,m]}) \end{aligned} \]</span>
This also means means that <span class="math inline">\(\mathbb{E}\left[f^T f\right]\)</span> is equal to
<span class="math inline">\(\text{trace}(K(x | \alpha, \rho)) * C\)</span>, whereas <span class="math inline">\(\mathbb{E}\left[ff^T\right]\)</span>
is <span class="math inline">\(\text{trace}(C) * K(x | \alpha, \rho)\)</span>. We can derive this using
properties of expectation and the matrix normal density.</p>
<p>We should set <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(1.0\)</span> because the parameter is not identified unless
we constrain <span class="math inline">\(\text{trace}(C) = 1\)</span>. Otherwise, we can multiply <span class="math inline">\(\alpha\)</span> by a scalar <span class="math inline">\(d\)</span> and
<span class="math inline">\(C\)</span> by <span class="math inline">\(1/d\)</span> and our likelihood will not change.</p>
<p>We can generate a random variable <span class="math inline">\(f\)</span> from a matrix normal density in
<span class="math inline">\(\mathbb{R}^{N \times M}\)</span> using the following algorithm:</p>
<p><span class="math display">\[ \begin{aligned}
  \eta_{i,j} &amp; \sim \mathsf{normal}(0, 1) \, \forall i,j \\
  f &amp; = L_{K(x | 1.0, \rho)} \, \eta \, L_C(\phi)^T \\
  f &amp; \sim \mathsf{MatrixNormal}(0, K(x | 1.0, \rho), C(\phi)) \\
  \eta &amp; \in \mathbb{R}^{N \times M} \\
  L_C(\phi) &amp; = \text{cholesky\_decompose}(C(\phi)) \\
  L_{K(x | 1.0, \rho)} &amp; = \text{cholesky\_decompose}(K(x | 1.0, \rho))
\end{aligned} \]</span></p>
<p>This can be implemented in Stan using a latent-variable GP formulation. We’ve used
<span class="math inline">\(\mathsf{LkjCorr}\)</span> for <span class="math inline">\(C(\phi)\)</span>, but any positive-definite matrix will do.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; D;
  real x[N];
  matrix[N, D] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  vector&lt;lower=0&gt;[D] alpha;
  real&lt;lower=0&gt; sigma;
  cholesky_factor_corr[D] L_Omega;
  matrix[N, D] eta;
}
model {
  matrix[N, D] f;
  {
    matrix[N, N] K = cov_exp_quad(x, 1.0, rho);
    matrix[N, N] L_K;

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta
        * diag_pre_multiply(alpha, L_Omega)&#39;;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(3);
  to_vector(eta) ~ std_normal();

  to_vector(y) ~ normal(to_vector(f), sigma);
}
generated quantities {
  matrix[D, D] Omega;
  Omega = L_Omega * L_Omega&#39;;
}</code></pre>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-RasmussenWilliams:2006">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. MIT Press.</p>
</div>
<div id="ref-Neal:1997">
<p>Neal, Radford M. 1997. “Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification.” 9702. University of Toronto, Department of Statistics.</p>
</div>
<div id="ref-Neal:1996">
<p>Neal, Radford M. 1996a. <em>Bayesian Learning for Neural Networks</em>. Lecture Notes in Statistics 118. New York: Springer.</p>
</div>
<div id="ref-PiironenVehtari:2016">
<p>Piironen, Juho, and Aki Vehtari. 2016. “Projection Predictive Model Selection for Gaussian Processes.” In <em>Machine Learning for Signal Processing (Mlsp), 2016 Ieee 26th International Workshop on</em>. IEEE.</p>
</div>
<div id="ref-zhang-gp:2004">
<p>Zhang, Hao. 2004. “Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics.” <em>Journal of the American Statistical Association</em> 99 (465). Taylor &amp; Francis: 250–61.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>Gaussian processes can be extended to covariance functions producing positive semi-definite matrices, but Stan does not support inference in the resulting models because the resulting distribution does not have unconstrained support.<a href="gaussian-processes-chapter.html#fnref30" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="directions-rotations-and-hyperspheres.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
