<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="truncated-or-censored-data.html">
<link rel="next" href="measurement-error-and-meta-analysis.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="7" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression Models</a></li>
<li class="chapter" data-level="8" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>8</b> Time-Series Models</a></li>
<li class="chapter" data-level="9" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>9</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="10" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>10</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="11" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>11</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="12" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>12</b> Finite Mixtures</a></li>
<li class="chapter" data-level="13" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>13</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="14" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>14</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="15" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>15</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="16" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>16</b> Clustering Models</a></li>
<li class="chapter" data-level="17" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>17</b> Gaussian Processes</a></li>
<li class="chapter" data-level="18" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>18</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="19" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>19</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="20" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>20</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="21" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>21</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="22" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>22</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="23" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>23</b> User-Defined Functions</a></li>
<li class="chapter" data-level="24" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>24</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="25" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>25</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="26" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>26</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="27" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>27</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="28" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>28</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="29" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>29</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="30" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>30</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>31</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="32" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>32</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mixture-modeling.chapter" class="section level1">
<h1><span class="header-section-number">12</span> Finite Mixtures</h1>
<p>Finite mixture models of an outcome assume that the outcome is drawn
from one of several distributions, the identity of which is controlled
by a categorical mixing distribution. Mixture models typically have
multimodal densities with modes near the modes of the mixture
components. Mixture models may be parameterized in several ways, as
described in the following sections. Mixture models may be used
directly for modeling data with multimodal distributions, or they may
be used as priors for other parameters.</p>
<div id="clustering-mixture.section" class="section level2">
<h2><span class="header-section-number">12.1</span> Relation to Clustering</h2>
<p>Clustering models, as discussed in the <a href="clustering-chapter.html#clustering.chapter">clustering
chapter</a>, are just a particular class of mixture
models that have been widely applied to clustering in the engineering
and machine-learning literature. The normal mixture model discussed
in this chapter reappears in multivariate form as the statistical
basis for the <span class="math inline">\(K\)</span>-means algorithm; the latent Dirichlet allocation
model, usually applied to clustering problems, can be viewed as a
mixed-membership multinomial mixture model.</p>
</div>
<div id="latent-discrete-parameterization" class="section level2">
<h2><span class="header-section-number">12.2</span> Latent Discrete Parameterization</h2>
<p>One way to parameterize a mixture model is with a latent categorical
variable indicating which mixture component was responsible for the
outcome. For example, consider <span class="math inline">\(K\)</span> normal distributions with locations
<span class="math inline">\(\mu_k \in \mathbb{R}\)</span> and scales <span class="math inline">\(\sigma_k \in (0,\infty)\)</span>. Now consider
mixing them in proportion <span class="math inline">\(\lambda\)</span>, where <span class="math inline">\(\lambda_k \geq 0\)</span> and
<span class="math inline">\(\sum_{k=1}^K \lambda_k = 1\)</span> (i.e., <span class="math inline">\(\lambda\)</span> lies in the unit <span class="math inline">\(K\)</span>-simplex).
For each outcome <span class="math inline">\(y_n\)</span> there is a latent variable <span class="math inline">\(z_n\)</span> in
<span class="math inline">\(\{ 1,\ldots,K \}\)</span> with a categorical distribution parameterized
by <span class="math inline">\(\lambda\)</span>,</p>
<p><span class="math display">\[
z_n \sim \mathsf{Categorical}(\lambda).
\]</span></p>
<p>The variable <span class="math inline">\(y_n\)</span> is distributed according to the parameters
of the mixture component <span class="math inline">\(z_n\)</span>,
<span class="math display">\[
y_n \sim \mathsf{normal}(\mu_{z[n]},\sigma_{z[n]}).
\]</span></p>
<p>This model is not directly supported by Stan because it involves
discrete parameters <span class="math inline">\(z_n\)</span>, but Stan can sample <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>
by summing out the <span class="math inline">\(z\)</span> parameter as described in the next section.</p>
</div>
<div id="summing-out-the-responsibility-parameter" class="section level2">
<h2><span class="header-section-number">12.3</span> Summing out the Responsibility Parameter</h2>
<p>To implement the normal mixture model outlined in the previous
section in Stan, the discrete parameters can be summed out of the
model. If <span class="math inline">\(Y\)</span> is a mixture of <span class="math inline">\(K\)</span> normal distributions with
locations <span class="math inline">\(\mu_k\)</span> and scales <span class="math inline">\(\sigma_k\)</span> with mixing proportions
<span class="math inline">\(\lambda\)</span> in the unit <span class="math inline">\(K\)</span>-simplex, then
<span class="math display">\[
p_Y(y | \lambda, \mu, \sigma)
\ = \
\sum_{k=1}^K \lambda_k \, \mathsf{normal}(y \, | \, \mu_k, \sigma_k).
\]</span></p>
<div id="log-sum-of-exponentials-linear-sums-on-the-log-scale" class="section level3 unnumbered">
<h3>Log Sum of Exponentials: Linear Sums on the Log Scale</h3>
<p>The log sum of exponentials function is used to define mixtures on the
log scale. It is defined for two inputs by</p>
<p><span class="math display">\[
\mbox{log\_sum\_exp}(a, b) = \log (\exp(a) + \exp(b)).
\]</span></p>
<p>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are probabilities on the log scale, then <span class="math inline">\(\exp(a) + \exp(b)\)</span> is their sum on the linear scale, and the outer log converts
the result back to the log scale; to summarize, log_sum_exp does
linear addition on the log scale. The reason to use Stan’s built-in
<code>log_sum_exp</code> function is that it can prevent underflow and overflow
in the exponentiation, by calculating the result as</p>
<p><span class="math display">\[
\log\left( \exp(a) + \exp(b)\right)
= c
  + \log \left( \exp(a - c) + \exp(b - c) \right),
\]</span></p>
<p>where <span class="math inline">\(c = \max(a, b)\)</span>. In this evaluation, one of the terms, <span class="math inline">\(a - c\)</span>
or <span class="math inline">\(b - c\)</span>, is zero and the other is negative, thus eliminating the
possibility of overflow or underflow in the leading term while
extracting the most arithmetic precision possible by pulling the
<code>max(a, b)</code> out of the log-exp round trip.</p>
<p>For example, the mixture of <span class="math inline">\(\mathsf{normal}(-1, 2)\)</span> with
<span class="math inline">\(\mathsf{normal}(3, 1)\)</span>, with mixing proportion <span class="math inline">\(\lambda = [0.3,0.7]^{\top}\)</span>, can be implemented in Stan as follows.</p>
<pre><code>parameters {
  real y;
}
model {
  target += log_sum_exp(log(0.3) + normal_lpdf(y | -1, 2),
                        log(0.7) + normal_lpdf(y | 3, 1));
}</code></pre>
<p>The log probability term is derived by taking</p>
<p><span class="math display">\[
\log p(y | \lambda,\mu,\sigma)  = \log\!\left( 0.3 * \mathsf{normal}(y|-1,2) \, + \,
  0.7 *
  \mathsf{normal}(y|3,1) \, \right)
\\
 = \log(
                 \exp(\log(0.3 * \mathsf{normal}(y|-1,2))) 
                 + \exp(\log(0.7 * \mathsf{normal}(y|3,1))) )
\\
 = \mbox{log\_sum\_exp}(
                         \log(0.3) + \log \mathsf{normal}(y|-1,2),
                         \log(0.7) + \log \mathsf{normal}(y|3,1) ).
\]</span></p>
</div>
<div id="dropping-uniform-mixture-ratios" class="section level3 unnumbered">
<h3>Dropping uniform mixture ratios</h3>
<p>If a two-component mixture has a mixing ratio of 0.5, then the mixing
ratios can be dropped, because</p>
<pre><code>neg_log_half = -log(0.5);
for (n in 1:N)
  target += log_sum_exp(neg_log_half
                        + normal_lpdf(y[n] | mu[1], sigma[1]),
                        neg_log_half
            + normal_lpdf(y[n] | mu[2], sigma[2]));</code></pre>
<p>then the <span class="math inline">\(-\log 0.5\)</span> term isn’t contributing to the proportional
density, and the above can be replaced with the more efficient version</p>
<pre><code>for (n in 1:N)
  target += log_sum_exp(normal_lpdf(y[n] | mu[1], sigma[1]),
                        normal_lpdf(y[n] | mu[2], sigma[2]));</code></pre>
<p>The same result holds if there are <span class="math inline">\(K\)</span> components and the mixing
simplex <span class="math inline">\(\lambda\)</span> is symmetric, i.e.,</p>
<p><span class="math display">\[
\lambda = \left( \frac{1}{K},   \ldots, \frac{1}{K} \right).
\]</span></p>
<p>The result follows from the identity</p>
<p><span class="math display">\[
\mbox{log\_sum\_exp}(c + a, c + b)
\ = \
c + \mbox{log\_sum\_exp(a, b)}
\]</span></p>
<p>and the fact that adding a constant <span class="math inline">\(c\)</span> to the log density accumulator
has no effect because the log density is only specified up to an
additive constant in the first place. There is nothing specific to
the normal distribution here; constants may always be dropped from the
target.</p>
</div>
<div id="recovering-posterior-mixture-proportions" class="section level3 unnumbered">
<h3>Recovering posterior mixture proportions</h3>
<p>The posterior <span class="math inline">\(p(z_n \mid y_n, \mu, \sigma)\)</span> over the mixture indicator <span class="math inline">\(z_n \in 1:K\)</span> is often of interest as <span class="math inline">\(p(z_n = k \mid y, \mu, \sigma)\)</span> is the
posterior probability that that observation <span class="math inline">\(y_n\)</span> was generated by
mixture component <span class="math inline">\(k\)</span>. The posterior can be computed via Bayes’s rule,
<span class="math display">\[
\begin{array}{rcl}
\mbox{Pr}(z_n = k \mid y_n, \mu, \sigma, \lambda)
&amp; \propto &amp;
p(y_n \mid z_n = k, \mu, \sigma) p(z_n = k \mid \lambda)
\\
&amp; = &amp; \mathsf{normal}(y_n \mid \mu_k, \sigma_k) \cdot \lambda_k.
\end{array}
\]</span></p>
<p>The normalization can be done via summation, because <span class="math inline">\(z_n \in 1{:}K\)</span> only
takes on finitely many values. In detail,
<span class="math display">\[
p(z_n = k \mid y_n, \mu, \sigma, \lambda)
\ = \
\frac{p(y_n \mid z_n = k, \mu, \sigma) \cdot p(z_n = k \mid \lambda)}
     {\sum_{k&#39; = 1}^K p(y_n \mid z_n = k&#39;, \mu, \sigma)
                    \cdot p(z_n = k&#39; \mid \lambda)}.
\]</span></p>
<p>On the log scale, the normalized probability is computed as
<span class="math display">\[
\begin{array}{l}
\log \mbox{Pr}(z_n = k \mid y_n, \mu, \sigma, \lambda)
\\
\mbox { } \ \ \ = \log p(y_n \mid z_n = k, \mu, \sigma)
  + \log \mbox{Pr}(z_n = k \mid \lambda)
  - \mathrm{log\_sum\_exp}_{k&#39; = 1}^K
      \log p(y_n \mid z_n = k&#39;, \mu, \sigma)
      + \log p(z_n = k&#39; \mid \lambda).
\end{array}
\]</span>
This can be coded up directly in Stan; the change-point model in the
<a href="latent-discrete-chapter.html#change-point.section">change point section</a> provides an example.</p>
</div>
<div id="estimating-parameters-of-a-mixture" class="section level3 unnumbered">
<h3>Estimating Parameters of a Mixture</h3>
<p>Given the scheme for representing mixtures, it may be moved to an
estimation setting, where the locations, scales, and mixture
components are unknown. Further generalizing to a number of mixture
components specified as data yields the following model.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;          // number of mixture components
  int&lt;lower=1&gt; N;          // number of data points
  real y[N];               // observations
}
parameters {
  simplex[K] theta;          // mixing proportions
  ordered[K] mu;             // locations of mixture components
  vector&lt;lower=0&gt;[K] sigma;  // scales of mixture components
}
model {
  vector[K] log_theta = log(theta);  // cache log calculation
  sigma ~ lognormal(0, 2);
  mu ~ normal(0, 10);
  for (n in 1:N) {
    vector[K] lps = log_theta;
    for (k in 1:K)
      lps[k] += normal_lpdf(y[n] | mu[k], sigma[k]);
    target += log_sum_exp(lps);
  }
}</code></pre>
<p>The model involves <code>K</code> mixture components and <code>N</code> data
points. The mixing proportion parameter <code>theta</code> is declared to be
a unit <span class="math inline">\(K\)</span>-simplex, whereas the component location parameter <code>mu</code>
and scale parameter <code>sigma</code> are both defined to be
<code>K</code>-vectors.</p>
<p>The location parameter <code>mu</code> is declared to be an ordered vector
in order to identify the model. This will not affect inferences that
do not depend on the ordering of the components as long as the prior
for the components <code>mu[k]</code> is symmetric, as it is here (each
component has an independent <span class="math inline">\(\mathsf{normal}(0, 10)\)</span> prior). It
would even be possible to include a hierarchical prior for the components.</p>
<p>The values in the scale array <code>sigma</code> are constrained to be
non-negative, and have a weakly informative prior given in the model
chosen to avoid zero values and thus collapsing components.</p>
<p>The model declares a local array variable <code>lps</code> to be size
<code>K</code> and uses it to accumulate the log contributions from the
mixture components. The main action is in the loop over data points
<code>n</code>. For each such point, the log of <span class="math inline">\(\theta_k * \mathsf{normal}(y_n \, | \, \mu_k,\sigma_k)\)</span> is calculated and added to the
array <code>lpps</code>. Then the log probability is incremented with the log
sum of exponentials of those values.</p>
</div>
</div>
<div id="vectorizing-mixtures" class="section level2">
<h2><span class="header-section-number">12.4</span> Vectorizing Mixtures</h2>
<p>There is (currently) no way to vectorize mixture models at the
observation level in Stan. This section is to warn users away from
attempting to vectorize naively, as it results in a different model.
A proper mixture at the observation level is defined as follows, where
we assume that <code>lambda</code>, <code>y[n]</code>, <code>mu[1], mu[2]</code>, and
<code>sigma[1], sigma[2]</code> are all scalars and <code>lambda</code> is between
0 and 1.</p>
<pre><code>for (n in 1:N) {
  target += log_sum_exp(log(lambda)
                          + normal_lpdf(y[n] | mu[1], sigma[1]),
                        log1m(lambda)
                          + normal_lpdf(y[n] | mu[2], sigma[2]));</code></pre>
<p>or equivalently</p>
<pre><code>for (n in 1:N)
  target += log_mix(lambda,
                    normal_lpdf(y[n] | mu[1], sigma[1]),
                    normal_lpdf(y[n] | mu[2], sigma[2]));</code></pre>
<p>This definition assumes that each observation <span class="math inline">\(y_n\)</span> may have arisen
from either of the mixture components. The density is
<span class="math display">\[
p(y \, | \, \lambda, \mu, \sigma)
= \prod_{n=1}^N (\lambda * \mathsf{normal}(y_n \, | \, \mu_1, \sigma_1)
                 + (1 - \lambda) * \mathsf{normal}(y_n \, | \, \mu_2, \sigma_2).
\]</span></p>
<p>Contrast the previous model with the following (erroneous) attempt to
vectorize the model.</p>
<pre><code>target += log_sum_exp(log(lambda)
                        + normal_lpdf(y | mu[1], sigma[1]),
                      log1m(lambda)
                        + normal_lpdf(y | mu[2], sigma[2]));</code></pre>
<p>or equivalently,</p>
<pre><code>target += log_mix(lambda,
                  normal_lpdf(y | mu[1], sigma[1]),
                  normal_lpdf(y | mu[2], sigma[2]));</code></pre>
<p>This second definition implies that the entire sequence <span class="math inline">\(y_1, \ldots, y_n\)</span> of
observations comes form one component or the other, defining a
different density,
<span class="math display">\[
p(y \, | \, \lambda, \mu, \sigma)
= \lambda * \prod_{n=1}^N \mbox{normal}(y_n \, | \, \mu_1, \sigma_1)
+ (1 - \lambda) * \prod_{n=1}^N \mbox{normal}(y_n \, | \, \mu_2, \sigma_2).
\]</span></p>
</div>
<div id="mixture-inference.section" class="section level2">
<h2><span class="header-section-number">12.5</span> Inferences Supported by Mixtures</h2>
<p>In many mixture models, the mixture components are underlyingly
exchangeable in the model and thus not identifiable. This arises if
the parameters of the mixture components have exchangeable priors and
the mixture ratio gets a uniform prior so that the parameters of the
mixture components are also exchangeable in the likelihood.</p>
<p>We have finessed this basic problem by ordering the parameters. This
will allow us in some cases to pick out mixture components either
ahead of time or after fitting (e.g., male vs. female, or Democrat
vs. Republican).</p>
<p>In other cases, we do not care about the actual identities of the
mixture components and want to consider inferences that are
independent of indexes. For example, we might only be interested
in posterior predictions for new observations.</p>
<div id="mixtures-with-unidentifiable-components" class="section level3 unnumbered">
<h3>Mixtures with Unidentifiable Components</h3>
<p>As an example, consider the normal mixture from the previous section,
which provides an exchangeable prior on the pairs of parameters
<span class="math inline">\((\mu_1, \sigma_1)\)</span> and <span class="math inline">\((\mu_2, \sigma_2)\)</span>,</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mu_1, \mu_2 &amp; \sim &amp; \mathsf{normal}(0, 10)
\\[8pt]
\sigma_1, \sigma_2 &amp; \sim &amp; \mathsf{Halfnormal}(0, 10)
\end{array}
\]</span></p>
<p>The prior on the mixture ratio is uniform,</p>
<p><span class="math display">\[
\lambda \sim \mathsf{Uniform}(0, 1),
\]</span></p>
<p>so that with the likelihood</p>
<p><span class="math display">\[
p(y_n \, | \, \mu, \sigma)
= \lambda \, \mathsf{normal}(y_n \, | \, \mu_1, \sigma_1)
+ (1 - \lambda) \, \mathsf{normal}(y_n \, | \, \mu_2, \sigma_2),
\]</span></p>
<p>the joint distribution <span class="math inline">\(p(y, \mu, \sigma, \lambda)\)</span> is exchangeable
in the parameters <span class="math inline">\((\mu_1, \sigma_1)\)</span> and <span class="math inline">\((\mu_2, \sigma_2)\)</span> with
<span class="math inline">\(\lambda\)</span> flipping to <span class="math inline">\(1 - \lambda\)</span>.^[Imposing a constraint such as <span class="math inline">\(\theta &lt; 0.5\)</span> will resolve the symmetry, but fundamentally changes the model and its posterior inferences.]</p>
</div>
<div id="inference-under-label-switching" class="section level3 unnumbered">
<h3>Inference under Label Switching</h3>
<p>In cases where the mixture components are not identifiable, it can be
difficult to diagnose convergence of sampling or optimization
algorithms because the labels will switch, or be permuted, in
different MCMC chains or different optimization runs. Luckily,
posterior inferences which do not refer to specific component labels
are invariant under label switching and may be used directly. This
subsection considers a pair of examples.</p>
<div id="predictive-likelihood" class="section level4 unnumbered">
<h4>Predictive likelihood</h4>
<p>Predictive likelihood for a new observation <span class="math inline">\(\tilde{y}\)</span> given the
complete parameter vector <span class="math inline">\(\theta\)</span> will be</p>
<p><span class="math display">\[
p(\tilde{y} \, | \, y)
=
\int_{\theta}
p(\tilde{y} \, | \, \theta)
\, p(\theta | y)
\, \mbox{d}\theta.
\]</span></p>
<p>The normal mixture example from the previous section, with <span class="math inline">\(\theta = (\mu, \sigma, \lambda)\)</span>, shows that the likelihood returns the same
density under label switching and thus the predictive inference is
sound. In Stan, that predictive inference can be done either by
computing <span class="math inline">\(p(\tilde{y} \, | \, y)\)</span>, which is more efficient
statistically in terms of effective sample size, or simulating draws
of <span class="math inline">\(\tilde{y}\)</span>, which is easier to plug into other inferences. Both
approaches can be coded directly in the generated quantities block of
the program. Here’s an example of the direct (non-sampling) approach.</p>
<pre><code>data {
  int&lt;lower = 0&gt; N_tilde;
  vector[N_tilde] y_tilde;
  ...
generated quantities {
  vector[N_tilde] log_p_y_tilde;
  for (n in 1:N_tilde)
    log_p_y_tilde[n]
      = log_mix(lambda,
                normal_lpdf(y_tilde[n] | mu[1], sigma[1])
                normal_lpdf(y_tilde[n] | mu[2], sigma[2]));
}</code></pre>
<p>It is a bit of a bother afterwards, because the logarithm function
isn’t linear and hence doesn’t distribute through averages (Jensen’s
inequality shows which way the inequality goes). The right thing to
do is to apply <code>log_sum_exp</code> of the posterior draws of
<code>log_p_y_tilde</code>. The average log predictive density is then
given by subtracting <code>log(N_new)</code>.</p>
</div>
<div id="clustering-and-similarity" class="section level4 unnumbered">
<h4>Clustering and similarity</h4>
<p>Often a mixture model will be applied to a clustering problem and
there might be two data items <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> for which there is a
question of whether they arose from the same mixture component. If we
take <span class="math inline">\(z_i\)</span> and <span class="math inline">\(z_j\)</span> to be the component responsibility discrete
variables, then the quantity of interest is <span class="math inline">\(z_i = z_j\)</span>, which can be
summarized as an event probability</p>
<p><span class="math display">\[
\mbox{Pr}[z_i = z_j \, | \, y]
=
\int_{\theta}
\frac{\sum_{k=0}^1 p(z_i=k, z_j = k, y_i, y_j \, | \, \theta)}
     {\sum_{k=0}^1 \sum_{m=0}^1 p(z_i = k, z_j = m, y_i, y_j \, | \,
       \theta)}
\
p(\theta \, | \, y)
\
\mbox{d}\theta.
\]</span></p>
<p>As with other event probabilities, this can be calculated in the
generated quantities block either by sampling <span class="math inline">\(z_i\)</span> and <span class="math inline">\(z_j\)</span> and
using the indicator function on their equality, or by computing the
term inside the integral as a generated quantity. As with predictive
likelihood, working in expectation is more statistically efficient than
sampling.</p>
</div>
</div>
</div>
<div id="zero-inflated.section" class="section level2">
<h2><span class="header-section-number">12.6</span> Zero-Inflated and Hurdle Models</h2>
<p>Zero-inflated and hurdle models both provide mixtures of a Poisson and
Bernoulli probability mass function to allow more flexibility in
modeling the probability of a zero outcome. Zero-inflated models, as
defined by <span class="citation">Lambert (<a href="#ref-Lambert:1992">1992</a>)</span>, add additional probability mass to
the outcome of zero. Hurdle models, on the other hand, are formulated
as pure mixtures of zero and non-zero outcomes.</p>
<p>Zero inflation and hurdle models can be formulated for discrete
distributions other than the Poisson. Zero inflation does not work
for continuous distributions in Stan because of issues with
derivatives; in particular, there is no way to add a point mass to a
continuous distribution, such as zero-inflating a normal as a
regression coefficient prior.</p>
<div id="zero-inflation" class="section level3 unnumbered">
<h3>Zero Inflation</h3>
<p>Consider the following example for zero-inflated Poisson
distributions. It uses a parameter <code>theta</code> here there is a
probability <span class="math inline">\(\theta\)</span> of drawing a zero, and a probability <span class="math inline">\(1 - \theta\)</span>
of drawing from <span class="math inline">\(\mathsf{Poisson}(\lambda)\)</span> (now <span class="math inline">\(\theta\)</span> is being
used for mixing proportions because <span class="math inline">\(\lambda\)</span> is the traditional
notation for a Poisson mean parameter). The probability function is
thus
<span class="math display">\[
p(y_n|\theta,\lambda)
=
\left\{
\begin{array}{ll}
\theta + (1 - \theta) * \mathsf{Poisson}(0|\lambda) &amp; \mbox{ if } y_n = 0, \mbox{ and}
\\[3pt]
(1-\theta) * \mathsf{Poisson}(y_n|\lambda) &amp; \mbox{ if } y_n &gt; 0.
\end{array}
\right.
\]</span></p>
<p>The log probability function can be implemented directly in Stan as follows.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  int&lt;lower=0&gt; y[N];
}
parameters {
  real&lt;lower=0, upper=1&gt; theta;
  real&lt;lower=0&gt; lambda;
}
model {
  for (n in 1:N) {
    if (y[n] == 0)
      target += log_sum_exp(bernoulli_lpmf(1 | theta),
                            bernoulli_lpmf(0 | theta)
                              + poisson_lpmf(y[n] | lambda));
    else
      target += bernoulli_lpmf(0 | theta)
                  + poisson_lpmf(y[n] | lambda);
  }
}</code></pre>
<p>The <code>log_sum_exp(lp1,lp2)</code> function adds the log probabilities
on the linear scale; it is defined to be equal to <code>log(exp(lp1) +   exp(lp2))</code>, but is more arithmetically stable and faster.</p>
<div id="optimizing-the-zero-inflated-poisson-model" class="section level4 unnumbered">
<h4>Optimizing the zero-inflated Poisson model</h4>
<p>The code given above to compute the zero-inflated Poisson
redundantly calculates all of the Bernoulli terms and also
<code>poisson_lpmf(0 | lambda)</code> every time the first condition
body executes. The use of the redundant terms is conditioned on
<code>y</code>, which is known when the data are read in. This allows
the transformed data block to be used to compute some more convenient
terms for expressing the log density each iteration.</p>
<p>The number of zero cases is computed and handled separately.
Then the nonzero cases are collected into their own array for
vectorization. The number of zeros is required to declare
<code>y_nonzero</code>, so it must be computed in a function.</p>
<pre><code>functions {
  int num_zeros(int[] y) {
    int sum = 0;
    for (n in 1:size(y))
      sum += (y[n] == 0);
    return sum;
  }
}
...
transformed data {
  int&lt;lower = 0&gt; N_zero = num_zeros(y);
  int&lt;lower = 1&gt; y_nonzero[N - N_zero];
  int N_nonzero = 0;
  for (n in 1:N) {
    if (y[n] == 0) continue;
    N_nonzero += 1;
    y_nonzero[N_nonzero] = y[n];
  }
}
...
model {
  ...
   target
     += N_zero
          * log_sum_exp(bernoulli_lpmf(1 | theta),
                        bernoulli_lpmf(0 | theta)
                          + poisson_lpmf(0 | lambda));
   target += N_nonzero * bernoulli_lpmf(0 | theta);
   target += poisson_lpmf(y_nonzero | lambda);
...</code></pre>
<p>The boundary conditions of all zeros and no zero outcomes is handled
appropriately; in the vectorized case, if <code>y_nonzero</code> is empty,
<code>N_nonzero</code> will be zero, and the last two target increment
terms will add zeros.</p>
</div>
</div>
<div id="hurdle-models" class="section level3 unnumbered">
<h3>Hurdle Models</h3>
<p>The hurdle model is similar to the zero-inflated model, but more
flexible in that the zero outcomes can be deflated as well as
inflated. The probability mass function for the hurdle likelihood is
defined by</p>
<p><span class="math display">\[
p(y|\theta,\lambda)
=
\begin{cases}
\ \theta &amp; \mbox{if } y = 0, \mbox{ and}
\\
\ (1 - \theta)
  \
   \frac{\displaystyle \mathsf{Poisson}(y | \lambda)}
        {\displaystyle  1 - \mathsf{PoissonCDF}(0 | \lambda)}
&amp; \mbox{if } y &gt; 0,
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\mathsf{PoissonCDF}\)</span> is the cumulative distribution function for
the Poisson distribution. The hurdle model is even more straightforward to
program in Stan, as it does not require an explicit mixture.</p>
<pre><code>   if (y[n] == 0)
      1 ~ bernoulli(theta);
    else {
      0 ~ bernoulli(theta);
      y[n] ~ poisson(lambda) T[1, ];
    }</code></pre>
<p>The Bernoulli statements are just shorthand for adding <span class="math inline">\(\log \theta\)</span>
and <span class="math inline">\(\log (1 - \theta)\)</span> to the log density. The <code>T[1,]</code> after the
Poisson indicates that it is truncated below at 1; see the <a href="truncated-or-censored-data.html#truncation.section">truncation
section</a> for more about truncation and the
<a href="#poisson.section">Poisson regresison section</a> for the specifics of
the Poisson CDF. The net effect is equivalent to the direct
definition of the log likelihood.</p>
<pre><code>   if (y[n] == 0)
      target += log(theta);
    else
      target += log1m(theta) + poisson_lpmf(y[n] | lambda)
                - poisson_lccdf(0 | lambda));</code></pre>
<p>Julian King pointed out that because
<span class="math display">\[
\log \left( 1 - \mathsf{PoissonCDF}(0 | \lambda) \right)
\ = \ \log \left( 1 - \mathsf{Poisson}(0 | \lambda) \right)
\ = \ \log(1 - \exp(-\lambda))
\]</span>
the CCDF in the else clause can be replaced with a simpler expression.</p>
<pre><code>      target += log1m(theta) + poisson_lpmf(y[n] | lambda)
                - log1m_exp(-lambda));</code></pre>
<p>The resulting code is about 15% faster than the code with the CCDF.</p>
<p>This is an example where collecting counts ahead of time can also
greatly speed up the execution speed without changing the density.
For data size <span class="math inline">\(N=200\)</span> and parameters <span class="math inline">\(\theta=0.3\)</span> and <span class="math inline">\(\lambda = 8\)</span>,
the speedup is a factor of 10; it will be lower for smaller <span class="math inline">\(N\)</span> and
greater for larger <span class="math inline">\(N\)</span>; it will also be greater for larger <span class="math inline">\(\theta\)</span>.</p>
<p>To achieve this speedup, it helps to have a function to count the
number of non-zero entries in an array of integers,</p>
<pre><code>functions {
  int num_zero(int[] y) {
    int nz = 0;
    for (n in 1:size(y))
      if (y[n] == 0)
        nz += 1;
    return nz;
  }
}</code></pre>
<p>Then a transformed data block can be used to store the sufficient
statistics,</p>
<pre><code>transformed data {
  int&lt;lower=0, upper=N&gt; N0 = num_zero(y);
  int&lt;lower=0, upper=N&gt; Ngt0 = N - N0;
  int&lt;lower=1&gt; y_nz[N - num_zero(y)];
  {
    int pos = 1;
    for (n in 1:N) {
      if (y[n] != 0) {
        y_nz[pos] = y[n];
        pos += 1;
      }
    }
  }
}</code></pre>
<p>The model block can then be reduced to three statements.</p>
<pre><code>model {
  N0 ~ binomial(N, theta);
  y_nz ~ poisson(lambda);
  target += -Ngt0 * log1m_exp(-lambda);
}</code></pre>
<p>The first statement accounts for the Bernoulli contribution to both
the zero and non-zero counts. The second line is the Poisson
contribution from the non-zero counts, which is now vectorized.
Finally, the normalization for the truncation is a single line, so
that the expression for the log CCDF at 0 isn’t repeated. Also note
that the negation is applied to the constant <code>Ngt0</code>; whenever
possible, leave subexpressions constant because then gradients need
not be propagated until a non-constant term is encountered.</p>
</div>
</div>
<div id="priors-and-effective-data-size-in-mixture-models" class="section level2">
<h2><span class="header-section-number">12.7</span> Priors and Effective Data Size in Mixture Models</h2>
<p>Suppose we have a two-component mixture model with mixing rate
<span class="math inline">\(\lambda \in (0, 1)\)</span>. Because the likelihood for the mixture
components is proportionally weighted by the mixture weights, the
effective data size used to estimate each of the mixture components
will also be weighted as a fraction of the overall data size. Thus
although there are <span class="math inline">\(N\)</span> observations, the mixture components will be
estimated with effective data sizes of <span class="math inline">\(\theta \, N\)</span> and <span class="math inline">\((1 - \theta) \, N\)</span> for the two components for some <span class="math inline">\(\theta \in (0, 1)\)</span>. The
effective weighting size is determined by posterior responsibility,
not simply by the mixing rate <span class="math inline">\(\lambda\)</span>.</p>
<div id="comparison-to-model-averaging" class="section level3 unnumbered">
<h3>Comparison to Model Averaging</h3>
<p>In contrast to mixture models, which create mixtures at the
observation level, model averaging creates mixtures over the
posteriors of models separately fit with the entire data set. In this
situation, the priors work as expected when fitting the models
independently, with the posteriors being based on the complete observed
data <span class="math inline">\(y\)</span>.</p>
<p>If different models are expected to account for different
observations, we recommend building mixture models directly. If the
models being mixed are similar, often a single expanded model will
capture the features of both and may be used on its own for
inferential purposes (estimation, decision making, prediction, etc.).
For example, rather than fitting an intercept-only regression and a
slope-only regression and averaging their predictions, even as a
mixture model, we would recommend building a single regression with
both a slope and an intercept. Model complexity, such as having more
predictors than data points, can be tamed using appropriately
regularizing priors. If computation becomes a bottleneck, the only
recourse can be model averaging, which can be calculated after fitting
each model independently (see <span class="citation">Hoeting et al. (<a href="#ref-HoetingEtAl:1999">1999</a>)</span> and
<span class="citation">Gelman et al. (<a href="#ref-GelmanEtAl:2013">2013</a>)</span> for theoretical and computational details).</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Lambert:1992">
<p>Lambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing.” <em>Technometrics</em> 34 (1).</p>
</div>
<div id="ref-HoetingEtAl:1999">
<p>Hoeting, Jennifer A., David Madigan, Adrian E Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” <em>Statistical Science</em> 14 (4): 382–417.</p>
</div>
<div id="ref-GelmanEtAl:2013">
<p>Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. London: Chapman &amp;Hall/CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="truncated-or-censored-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="measurement-error-and-meta-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
