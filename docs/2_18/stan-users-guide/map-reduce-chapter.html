<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Stan User’s Guide</title>
  <meta name="description" content="Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Stan User’s Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stan User’s Guide" />
  
  <meta name="twitter:description" content="Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="optimization-chapter.html">
<link rel="next" href="appendices-part.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan User's Guide</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 1. Example Models</i></a></li>
<li class="chapter" data-level="1" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>1</b> Regression Models</a></li>
<li class="chapter" data-level="2" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>2</b> Time-Series Models</a></li>
<li class="chapter" data-level="3" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>3</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="4" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>4</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="5" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="6" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>6</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="7" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>7</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="8" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>8</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="9" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>9</b> Clustering Models</a></li>
<li class="chapter" data-level="10" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>10</b> Gaussian Processes</a></li>
<li class="chapter" data-level="11" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>11</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="12" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>12</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="13" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>13</b> Ordinary Differential Equations</a></li>
<li><a href="part-2-programming-techniques.html#part-2.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 2. Programming Techniques</i></a></li>
<li class="chapter" data-level="14" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>14</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="15" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>15</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="16" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>16</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="17" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>17</b> User-Defined Functions</a></li>
<li class="chapter" data-level="18" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>18</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="19" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>19</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="20" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>20</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="21" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>21</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="22" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>22</b> Map-Reduce</a></li>
<li><a href="appendices-part.html#appendices.part"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="23" data-path="stan-program-style-guide.html"><a href="stan-program-style-guide.html"><i class="fa fa-check"></i><b>23</b> Stan Program Style Guide</a></li>
<li class="chapter" data-level="24" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i><b>24</b> Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan User’s Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="map-reduce.chapter" class="section level1">
<h1><span class="header-section-number">22</span> Map-Reduce</h1>
<p>Map-reduce allows large calculations (e.g., log likelihoods) to be
broken into components which may be calculated modularly (e.g., data
blocks) and combined (e.g., by summation and incrementing the target
log density).</p>
<div id="overview-of-map-reduce" class="section level2">
<h2><span class="header-section-number">22.1</span> Overview of Map-Reduce</h2>
<p>A <em>map function</em> is a higher-order function that applies an
argument function to every member of some collection, returning a
collection of the results. For example, mapping the square function,
<span class="math inline">\(f(x) = x^2\)</span>, over the vector <span class="math inline">\([3, 5, 10]\)</span> produces the vector
<span class="math inline">\([9, 25, 100]\)</span>. In other words, map applies the square function
elementwise.</p>
<p>The output of mapping a sequence is often fed into a reduction.
A <em>reduction function</em> takes an arbitrarily long sequence of
inputs and returns a single output. Examples of reduction functions
are summation (with the return being a single value) or sorting (with
the return being a sorted sequence). The combination of mapping and
reducing is so common it has its own name, <em>map-reduce</em>.</p>
</div>
<div id="map-function" class="section level2">
<h2><span class="header-section-number">22.2</span> Map Function</h2>
<p>In order to generalize the form of functions and results that are
possible and accomodate both parameters (which need derivatives) and
data values (which don’t), Stan’s map function operates on more than
just a sequence of inputs.</p>
<div id="map-function-signature" class="section level3 unnumbered">
<h3>Map Function Signature</h3>
<p>Stan’s map function has the following signature</p>
<pre><code>vector map_rect((vector, vector, real[], int[]):vector f,
                vector phi, vector[] thetas,
                data real[ , ] x_rs, data int[ , ] x_is);</code></pre>
<p>The arrays <code>thetas</code> of parametrs, <code>x_rs</code> of real data, and
<code>x_is</code> of integer data have the suffix “<code>s</code>” to indicate they
are arrays. These arrays must all be the same size, as they will be
mapped in parallel by the function <code>f</code>. The value of <code>phi</code>
is reused in each mapped operation.</p>
<p>The <code>_rect</code> suffix in the name arises because the data
structures it takes as arguments are rectangular. In order to deal
with ragged inputs, ragged inputs must be padded out to recangular
form.</p>
<p>The last two arguments are two dimensional arrays of real and integer
data values. These argument types are marked with the <code>data</code>
qualifier to indicate that they must only contain variables
originating in the data or transformed data blocks. This will allow
such data to be pinned to a processor on which it is being processed
to reduce communication overhead.</p>
<p>The notation <code>(vector, vector, real[], int[]):vector</code> indicates
that the function argument <code>f</code> must have the following signature.</p>
<pre><code>vector f(vector phi, vector theta,
         data real[] x_r, data int[] x_i);</code></pre>
<p>Although <code>f</code> will often return a vector of size one, the built-in
flexiblity allows general multivariate functions to be mapped, even
raggedly.</p>
</div>
<div id="map-function-semantics" class="section level3 unnumbered">
<h3>Map Function Semantics</h3>
<p>Stan’s map function applies the function <code>f</code> to the shared
parameters along with one element each of the job parameters, real
data, and integer data arrays. Each of the arguments <code>theta</code>,
<code>x_r</code>, and <code>x_i</code> must be arrays of the same size. If the
arrays are all size <code>N</code>, the result is defined as follows.</p>
<pre><code>map_rect(f, phi, thetas, xs, ns)
= f(phi, thetas[1], xs[1], ns[1]) . f(phi, thetas[2], xs[2], ns[2])
  . ... . f(phi, thetas[N], xs[N], ns[N])</code></pre>
<p>The dot operators in the notation above are meant to indicate
concatenation (implemented as <code>append_row</code> in Stan). The output
of each application of <code>f</code> is a vector, and the sequence of
<code>N</code> vectors is concatenated together to return a single vector.</p>
</div>
</div>
<div id="example-mapping-logistic-regression" class="section level2">
<h2><span class="header-section-number">22.3</span> Example: Mapping Logistic Regression</h2>
<p>An example should help to clarify both the syntax and semantics of the
mapping operation and how it may be combined with reductions built
into Stan to provide a map-reduce implementation.</p>
<div id="unmapped-logistic-regression" class="section level3 unnumbered">
<h3>Unmapped Logistic Regression</h3>
<p>Consider the following simple logistic regression model, which is
coded unconventionally to accomodate direct translation to a mapped
implementation.</p>
<pre><code>data {
  int y[12];
  real x[12];
}
parameters {
  vector[2] beta;
}
model {
  beta ~ std_normal();
  y ~ bernoulli_logit(beta[1] + beta[2] * to_vector(x));
}</code></pre>
<p>The program is unusual in that it (a) hardcodes the data size, which
is not required by the map function but is just used here for
simplicity, (b) represents the predictors as a real array even though
it needs to be used as a vector, and (c) represents the regression
coefficients (intercept and slope) as a vector even though they’re
used individually. The <code>bernoulli_logit</code> distribution is used
because the argument is on the logit scale—it implicitly applies the
inverse logit function to map the argument to a probability.</p>
</div>
<div id="mapped-logistic-regression" class="section level3 unnumbered">
<h3>Mapped Logistic Regression</h3>
<p>The unmapped logistic regression model described in the previous
subsection may be implemented using Stan’s rectangular mapping
functionality as follows.</p>
<pre><code>functions {
  vector lr(vector beta, vector theta, real[] x, int[] y) {
    real lp = bernoulli_logit_lpmf(y | beta[1] + to_vector(x) * beta[2]);
    return [lp]&#39;;
  }
}
data {
  int y[12];
  real x[12];
}
transformed data {
  // K = 3 shards
  int ys[3, 4] = { y[1:4], y[5:8], y[9:12] };
  real xs[3, 4] = { x[1:4], x[5:8], x[9:12] };
  vector[0] theta[3];
}
parameters {
  vector[2] beta;
}
model {
  beta ~ std_normal();
  target += sum(map_rect(lr, beta, theta, xs, ys));
}</code></pre>
<p>The first piece of the code is the actual function to compute the
logistic regression. The argument <code>beta</code> will contain the
regression coefficients (intercept and slope), as before. The second
argument <code>theta</code> of job-specific parameters is not used, but
nevertheless must be present. The modeled data <code>y</code> is passed as
an array of integers and the predictors <code>x</code> as an array of real
values. The function body then computes the log probability mass of <code>y</code> and
assigns it to the local variable <code>lp</code>. This variable is then
used in <code>[lp]'</code> to construct a row vector and then transpose it
to a vector to return.</p>
<p>The data are taken in as before. There is an additional transformed
data block that breaks the data up into three shards.<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<p>The value <code>3</code> is also hard coded; a more practical program would
allow the number of shards to be controlled. There are three parallel
arrays defined here, each of size three, corresponding to the number
of shards. The array <code>ys</code> contains the modeled data variables;
each element of the array <code>ys</code> is an array of size four. The
second array <code>xs</code> is for the predictors, and each element of it
is also of size four. These contained arrays are the same size
because the predictors <code>x</code> stand in a one-to-one relationship
with the modeled data <code>y</code>. The final array <code>theta</code> is also
of size three; its elements are empty vectors, because there are no
shard-specific parameters.</p>
<p>The parameters and the prior are as before. The likelihood is now
coded using map-reduce. The function <code>lr</code> to compute the log
probability mass is mapped over the data <code>xs</code> and <code>ys</code>,
which contain the original predictors and outcomes broken into shards.
The parameters <code>beta</code> are in the first argument because they are
shared across shards. There are no shard-specific parameters, so
the array of job-specific parameters <code>theta</code> contains only empty
vectors.</p>
</div>
</div>
<div id="example-hierarchical-logistic-regression" class="section level2">
<h2><span class="header-section-number">22.4</span> Example: Hierarchical Logistic Regression</h2>
<p>Consider a hierarchical model of American presidential voting behavior
based on state of residence.<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a></p>
<p>Each of the fifty states <span class="math inline">\(k \in 1{:}50\)</span> will have its own slope
<span class="math inline">\(\beta_k\)</span> and intercept <span class="math inline">\(\alpha_k\)</span> to model the log odds of voting for
the Republican candidate as a function of income. Suppose there are
<span class="math inline">\(N\)</span> voters and with voter <span class="math inline">\(n \in 1{:}N\)</span> being in state <span class="math inline">\(s[n]\)</span> with
income <span class="math inline">\(x_n\)</span>. The likelihood for the vote <span class="math inline">\(y_n \in \{ 0, 1 \}\)</span> is</p>
<p><span class="math display">\[
y_n \sim \mathsf{Bernoulli}
\left(
  \mathrm{logit}^{-1}\left( \alpha_{s[n]} +  \beta_{s[n]} \, x_n \right)
\right).
\]</span></p>
<p>The slopes and intercepts get hierarchical priors,</p>
<p><span class="math display">\[
\alpha_k \sim \mathsf{normal}(\mu_{\alpha}, \sigma_{\alpha})
\\
\beta_k \sim \mathsf{normal}(\mu_{\beta}, \sigma_{\beta})
\]</span></p>
<div id="unmapped-implementation" class="section level3 unnumbered">
<h3>Unmapped Implementation</h3>
<p>This model can be coded up in Stan directly as follows.</p>
<pre><code>data {
  int&lt;lower = 0&gt; K;
  int&lt;lower = 0&gt; N;
  int&lt;lower = 1, upper = K&gt; kk[N];
  vector[N] x;
  int&lt;lower = 0, upper = 1&gt; y[N];
}
parameters {
  matrix[K,2] beta;
  vector[2] mu;
  vector&lt;lower=0&gt;[2] sigma;
}
model {
  mu ~ normal(0, 2);
  sigma ~ normal(0, 2);
  for (i in 1:2)
    beta[ , i] ~ normal(mu[i], sigma[i]);
  y ~ bernoulli_logit(beta[kk, 1] + beta[kk, 2] .* x);
}</code></pre>
<p>For this model the vector of predictors <code>x</code> is coded as a vector,
corresponding to how it is used in the likelihood.
The priors for <code>mu</code> and <code>sigma</code> are vectorized. The priors
on the two components of <code>beta</code> (intercept and slope,
respectively) are stored in a <span class="math inline">\(K \times 2\)</span> matrix.</p>
<p>The likelihood is also
vectorized using multi-indexing with index <code>kk</code> for the states
and elementwise multiplication (<code>.*</code>) for the income <code>x</code>.
The vectorized likelihood works out to the same thing as the following
less efficient looped form.</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli_logit(beta[kk[n], 1] + beta[kk[n], 2] * x[n]);</code></pre>
</div>
<div id="mapped-implementation" class="section level3 unnumbered">
<h3>Mapped Implementation</h3>
<p>The mapped version of the model will map over the states <code>K</code>.
This means the group-level parameters, real data, and integer-data
must be arrays of the same size.</p>
<p>The mapped implementation requires a function to be mapped. The
following function evaluates both the likelihood for the data observed
for a group as well as the prior for the group-specific parameters
(the name <code>bl_glm</code> derives from the fact that it’s a generalized
linear model with a Bernoulli likelihood and logistic link function).</p>
<pre><code>functions {
 vector bl_glm(vector mu_sigma, vector beta,
               real[] x, int[] y) {
   vector[2] mu = mu_sigma[1:2];
   vector[2] sigma = mu_sigma[3:4];
   real lp = normal_lpdf(beta | mu, sigma);
   real ll = bernoulli_logit_lpmf(y | beta[1] + beta[2] * to_vector(x));
   return [lp + ll]&#39;;
 }
}</code></pre>
<p>The shared parameter <code>mu_sigma</code> contains the locations
(<code>mu_sigma[1:2]</code>) and scales (<code>mu_sigma[3:4]</code>) of the
priors, which are extracted in the first two lines of the program.
The variable <code>lp</code> is assigned the log density of the prior on
<code>beta</code>. The vector <code>beta</code> is of size two, as are the
vectors <code>mu</code> and <code>sigma</code>, so everything lines up for the
vectorization. Next, the variable <code>ll</code> is assigned to the log
likelihood contribution for the group. Here <code>beta[1]</code> is the
intercept of the regression and <code>beta[2]</code> the slope. The
predictor array <code>x</code> needs to be converted to a vector allow the
multiplication.</p>
<p>The data block is identical to that of the previous program, but
repeated here for convenience. A transformed data block computes the
data structures needed for the mapping by organizing the data into
arrays indexed by group.</p>
<pre><code>data {
  int&lt;lower = 0&gt; K;
  int&lt;lower = 0&gt; N;
  int&lt;lower = 1, upper = K&gt; kk[N];
  vector[N] x;
  int&lt;lower = 0, upper = 1&gt; y[N];
}
transformed data {
  int&lt;lower = 0&gt; J = N / K;
  real x_r[K, J];
  int&lt;lower = 0, upper = 1&gt; x_i[K, J];
  {
    int pos = 1;
    for (k in 1:K) {
      int end = pos + J - 1;
      x_r[k] = to_array_1d(x[pos:end]);
      x_i[k] = to_array_1d(y[pos:end]);
      pos += J;
    }
  }
}</code></pre>
<p>The integer <code>J</code> is set to the number of observations per group.<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a></p>
<p>The real data array <code>x_r</code> holds the predictors and the integer
data array <code>x_i</code> holds the outcomes. The grouped data arrays
are constructed by slicing the predictor vector <code>x</code> (and
converting it to an array) and slicing the outcome array <code>y</code>.</p>
<p>Given the transformed data with groupings, the parameters are the same
as the previous program. The model has the same priors for the
hyperparameters <code>mu</code> and <code>sigma</code>, but moves the prior for
<code>beta</code> and the likelihood to the mapped function.</p>
<pre><code>parameters {
  vector[2] beta[K];
  vector[2] mu;
  vector&lt;lower=0&gt;[2] sigma;
}
model {
  mu ~ normal(0, 2);
  sigma ~ normal(0, 2);
  target += sum(map_rect(bl_glm, append_row(mu, sigma), beta, x_r, x_i));
}</code></pre>
<p>The model as written here computes the priors for each group’s
parameters along with the likelihood contribution for the group. An
alternative mapping would leave the prior in the model block and only
map the likelihood. In a serial setting this shouldn’t make much of a
difference, but with parallelization, there is reduced communication
(the prior’s parameters need not be transmitted) and also reduced
parallelization with the version that leaves the prior in the model
block.</p>
</div>
</div>
<div id="ragged-inputs-and-outputs" class="section level2">
<h2><span class="header-section-number">22.5</span> Ragged Inputs and Outputs</h2>
<p>The previous examples included rectangular data structures and single
outputs. Despite the name, this is not technically required by
<code>map_rect</code>.</p>
<div id="ragged-inputs" class="section level3 unnumbered">
<h3>Ragged Inputs</h3>
<p>If each group has a different number of observations, then the
rectangular data structures for predictors and outcomes will need to
be padded out to be rectangular. In addition, the size of the ragged
structure will need to be passed as integer data. This holds for
shards with varying numbers of parameters as well as varying numbers
of data points.</p>
</div>
<div id="ragged-outputs" class="section level3 unnumbered">
<h3>Ragged Outputs</h3>
<p>The output of each mapped function is concatenated to produce the
output of <code>map_rect</code>. When every shard returns a singleton
(size one) array, the result is the same size as the number of shards
and is easy to deal with downstream. If functions return longer
arrays, they can still be structured using the <code>to_matrix</code>
function if they are rectangular.</p>
<p>If the outputs are of varying sizes, then there will have to be some way
to convert it back to a usable form based on the input, because there
is no way to directly return sizes or a ragged structure.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="42">
<li id="fn42"><p>The term
“shard” is borrowed from databases, where it refers to a slice of the
rows of a database. That is exactly what it is here if we think of
rows of a dataframe. Stan’s shards are more general in that they need
not correspond to rows of a dataframe.<a href="map-reduce-chapter.html#fnref42" class="footnote-back">↩</a></p></li>
<li id="fn43"><p>This example is a simplified form of the model
described in <span class="citation">(Gelman and Hill <a href="#ref-GelmanHill:2007">2007</a>, Section 14.2)</span><a href="map-reduce-chapter.html#fnref43" class="footnote-back">↩</a></p></li>
<li id="fn44"><p>This makes the strong assumption that each group has the same number of observations!<a href="map-reduce-chapter.html#fnref44" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimization-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendices-part.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
