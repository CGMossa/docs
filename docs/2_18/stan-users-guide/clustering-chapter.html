<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Stan User’s Guide</title>
  <meta name="description" content="Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Stan User’s Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stan User’s Guide" />
  
  <meta name="twitter:description" content="Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="sparse-ragged-chapter.html">
<link rel="next" href="gaussian-processes-chapter.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan User's Guide</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 1. Example Models</i></a></li>
<li class="chapter" data-level="1" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>1</b> Regression Models</a></li>
<li class="chapter" data-level="2" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>2</b> Time-Series Models</a></li>
<li class="chapter" data-level="3" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>3</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="4" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>4</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="5" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="6" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>6</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="7" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>7</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="8" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>8</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="9" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>9</b> Clustering Models</a></li>
<li class="chapter" data-level="10" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>10</b> Gaussian Processes</a></li>
<li class="chapter" data-level="11" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>11</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="12" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>12</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="13" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>13</b> Ordinary Differential Equations</a></li>
<li><a href="part-2-programming-techniques.html#part-2.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 2. Programming Techniques</i></a></li>
<li class="chapter" data-level="14" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>14</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="15" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>15</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="16" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>16</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="17" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>17</b> User-Defined Functions</a></li>
<li class="chapter" data-level="18" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>18</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="19" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>19</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="20" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>20</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="21" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>21</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="22" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>22</b> Map-Reduce</a></li>
<li><a href="appendices-part.html#appendices.part"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="23" data-path="stan-program-style-guide.html"><a href="stan-program-style-guide.html"><i class="fa fa-check"></i><b>23</b> Stan Program Style Guide</a></li>
<li class="chapter" data-level="24" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i><b>24</b> Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan User’s Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering.chapter" class="section level1">
<h1><span class="header-section-number">9</span> Clustering Models</h1>
<p>Unsupervised methods for organizing data into groups are collectively
referred to as clustering. This chapter describes the implementation
in Stan of two widely used statistical clustering models, soft
<span class="math inline">\(K\)</span>-means and latent Dirichlet allocation (LDA). In addition, this
chapter includes naive Bayesian classification, which can be viewed as
a form of clustering which may be supervised. These models are
typically expressed using discrete parameters for cluster assignments.
Nevertheless, they can be implemented in Stan like any other mixture
model by marginalizing out the discrete parameters (see
the <a href="mixture-modeling-chapter.html#mixture-modeling.chapter">mixture modeling chapter</a>).</p>
<div id="relation-to-finite-mixture-models" class="section level2">
<h2><span class="header-section-number">9.1</span> Relation to Finite Mixture Models</h2>
<p>As mentioned in the <a href="mixture-modeling-chapter.html#clustering-mixture.section">clustering section</a>,
clustering models and finite mixture models are really just two sides
of the same coin. The ``soft&quot; <span class="math inline">\(K\)</span>-means model described in the next
section is a normal mixture model (with varying assumptions about
covariance in higher dimensions leading to variants of <span class="math inline">\(K\)</span>-means).
Latent Dirichlet allocation is a mixed-membership multinomial mixture.</p>
</div>
<div id="soft-k-means" class="section level2">
<h2><span class="header-section-number">9.2</span> Soft <span class="math inline">\(K\)</span>-Means</h2>
<p><span class="math inline">\(K\)</span>-means clustering is a method of clustering data represented as
<span class="math inline">\(D\)</span>-dimensional vectors. Specifically, there will be <span class="math inline">\(N\)</span> items to be
clustered, each represented as a vector <span class="math inline">\(y_n \in \mathbb{R}^D\)</span>. In the
“soft” version of <span class="math inline">\(K\)</span>-means, the assignments to clusters will be
probabilistic.</p>
<div id="geometric-hard-k-means-clustering" class="section level3 unnumbered">
<h3>Geometric Hard <span class="math inline">\(K\)</span>-Means Clustering</h3>
<p><span class="math inline">\(K\)</span>-means clustering is typically described geometrically in terms of
the following algorithm, which assumes the number of clusters <span class="math inline">\(K\)</span> and
data vectors <span class="math inline">\(y\)</span> as input.</p>
<ol style="list-style-type: decimal">
<li>For each <span class="math inline">\(n\)</span> in <span class="math inline">\(1:N\)</span>, randomly assign vector <span class="math inline">\(y_n\)</span> to a cluster in <span class="math inline">\(1{:}K\)</span>;</li>
<li>Repeat
<ol style="list-style-type: decimal">
<li>For each cluster <span class="math inline">\(k\)</span> in <span class="math inline">\(1{:}K\)</span>, compute the cluster centroid <span class="math inline">\(\mu_k\)</span> by averaging the vectors assigned to that cluster;</li>
<li>For each <span class="math inline">\(n\)</span> in <span class="math inline">\(1:N\)</span>, reassign <span class="math inline">\(y_n\)</span> to the cluster <span class="math inline">\(k\)</span> for which the (Euclidean) distance from <span class="math inline">\(y_n\)</span> to <span class="math inline">\(\mu_k\)</span> is smallest;</li>
<li>If no vectors changed cluster, return the cluster assignments.</li>
</ol></li>
</ol>
<p>This algorithm is guaranteed to terminate.</p>
</div>
<div id="soft-k-means-clustering" class="section level3 unnumbered">
<h3>Soft <span class="math inline">\(K\)</span>-Means Clustering</h3>
<p>Soft <span class="math inline">\(K\)</span>-means clustering treats the cluster assignments as
probability distributions over the clusters. Because of the
connection between Euclidean distance and multivariate normal models
with a fixed covariance, soft <span class="math inline">\(K\)</span>-means can be expressed (and coded in
Stan) as a multivariate normal mixture model.</p>
<p>In the full generative model, each data point <span class="math inline">\(n\)</span> in <span class="math inline">\(1{:}N\)</span> is assigned
a cluster <span class="math inline">\(z_n \in 1{:}K\)</span> with symmetric uniform probability,</p>
<p><span class="math display">\[
z_n \sim \mathsf{Categorical}({\bf 1}/K),
\]</span>
where <span class="math inline">\({\bf 1}\)</span> is the unit vector of <span class="math inline">\(K\)</span> dimensions, so that <span class="math inline">\({\bf  1}/K\)</span> is the symmetric <span class="math inline">\(K\)</span>-simplex. Thus the model assumes that
each data point is drawn from a hard decision about cluster
membership. The softness arises only from the uncertainty about which
cluster generated a data point.</p>
<p>The data points themselves are generated from a multivariate normal
distribution whose parameters are determined by the cluster assignment
<span class="math inline">\(z_n\)</span>,
<span class="math display">\[
y_n \sim  \mathsf{normal}(\mu_{z[n]},\Sigma_{z[n]})
\]</span></p>
<p>The sample implementation in this section assumes a fixed unit
covariance matrix shared by all clusters <span class="math inline">\(k\)</span>,
<span class="math display">\[
\Sigma_k = \mbox{diag\_matrix}({\bf 1}),
\]</span>
so that the log multivariate normal can be implemented directly up to a proportion
by
<span class="math display">\[
\mbox{normal}\left( y_n | \mu_k, \mbox{diag\_matrix}({\bf 1}) \right)
\propto \exp \left (- \frac{1}{2} \sum_{d=1}^D \left( \mu_{k,d} - y_{n,d}
  \right)^2 \right).
\]</span>
The spatial perspective on <span class="math inline">\(K\)</span>-means arises by noting that the inner
term is just half the negative Euclidean distance from the cluster
mean <span class="math inline">\(\mu_k\)</span> to the data point <span class="math inline">\(y_n\)</span>.</p>
</div>
<div id="stan-implementation-of-soft-k-means" class="section level3 unnumbered">
<h3>Stan Implementation of Soft <span class="math inline">\(K\)</span>-Means</h3>
<p>Consider the following Stan program for implementing <span class="math inline">\(K\)</span>-means
clustering.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;  // number of data points
  int&lt;lower=1&gt; D;  // number of dimensions
  int&lt;lower=1&gt; K;  // number of clusters
  vector[D] y[N];  // observations
}
transformed data {
  real&lt;upper=0&gt; neg_log_K;
  neg_log_K = -log(K);
}
parameters {
  vector[D] mu[K]; // cluster means
}
transformed parameters {
  real&lt;upper=0&gt; soft_z[N, K]; // log unnormalized clusters
  for (n in 1:N)
    for (k in 1:K)
      soft_z[n, k] = neg_log_K
                     - 0.5 * dot_self(mu[k] - y[n]);
}
model {
  // prior
  for (k in 1:K)
    mu[k] ~ std_normal();

  // likelihood
  for (n in 1:N)
    target += log_sum_exp(soft_z[n]));
}</code></pre>
<p>There is an independent standard normal prior on the centroid parameters;
this prior could be swapped with other priors, or even a hierarchical
model to fit an overall problem scale and location.</p>
<p>The only parameter is <code>mu</code>, where <code>mu[k]</code> is the centroid for cluster
<span class="math inline">\(k\)</span>. The transformed parameters <code>soft_z[n]</code> contain the log of the
unnormalized cluster assignment probabilities. The vector <code>soft_z[n]</code>
can be converted back to a normalized simplex using the softmax
function (see the functions reference manual), either externally or
within the model’s generated quantities block.</p>
</div>
<div id="generalizing-soft-k-means" class="section level3 unnumbered">
<h3>Generalizing Soft <span class="math inline">\(K\)</span>-Means</h3>
<p>The multivariate normal distribution with unit covariance matrix
produces a log probability density proportional to Euclidean distance
(i.e., <span class="math inline">\(L_2\)</span> distance). Other distributions relate to other
geometries. For instance, replacing the normal distribution with the
double exponential (Laplace) distribution produces a clustering model
based on <span class="math inline">\(L_1\)</span> distance (i.e., Manhattan or taxicab
distance).</p>
<p>Within the multivariate normal version of <span class="math inline">\(K\)</span>-means, replacing the
unit covariance matrix with a shared covariance matrix amounts to
working with distances defined in a space transformed by the inverse
covariance matrix.</p>
<p>Although there is no global spatial analog, it is common to see soft
<span class="math inline">\(K\)</span>-means specified with a per-cluster covariance matrix. In this
situation, a hierarchical prior may be used for the covariance matrices.</p>
</div>
</div>
<div id="the-difficulty-of-bayesian-inference-for-clustering" class="section level2">
<h2><span class="header-section-number">9.3</span> The Difficulty of Bayesian Inference for Clustering</h2>
<p>Two problems make it pretty much impossible to perform full Bayesian
inference for clustering models, the lack of parameter identifiability
and the extreme multimodality of the posteriors. There is additional
discussion related to the non-identifiability due to label switching
in the <a href="problematic-posteriors-chapter.html#label-switching-problematic.section">label switching
section</a>.</p>
<div id="non-identifiability" class="section level3 unnumbered">
<h3>Non-Identifiability</h3>
<p>Cluster assignments are not identified — permuting the cluster mean
vectors <code>mu</code> leads to a model with identical likelihoods. For
instance, permuting the first two indexes in <code>mu</code> and the first
two indexes in each <code>soft_z[n]</code> leads to an identical likelihood
(and prior).</p>
<p>The lack of identifiability means that the cluster parameters
cannot be compared across multiple Markov chains. In fact, the only
parameter in soft <span class="math inline">\(K\)</span>-means is not identified, leading to problems in
monitoring convergence. Clusters can even fail to be identified
within a single chain, with indices swapping if the chain is long
enough or the data are not cleanly separated.</p>
</div>
<div id="multimodality" class="section level3 unnumbered">
<h3>Multimodality</h3>
<p>The other problem with clustering models is that their posteriors are
highly multimodal. One form of multimodality is the
non-identifiability leading to index swapping. But even without
the index problems the posteriors are highly multimodal.</p>
<p>Bayesian inference fails in cases of high multimodality because there
is no way to visit all of the modes in the posterior in appropriate
proportions and thus no way to evaluate integrals involved in
posterior predictive inference.</p>
<p>In light of these two problems, the advice often given in fitting
clustering models is to try many different initializations and select
the sample with the highest overall probability. It is also popular
to use optimization-based point estimators such as expectation
maximization or variational Bayes, which can be much more efficient
than sampling-based approaches.</p>
</div>
</div>
<div id="naive-bayes-classification-and-clustering" class="section level2">
<h2><span class="header-section-number">9.4</span> Naive Bayes Classification and Clustering</h2>
<p>Naive Bayes is a kind of mixture model that can be used for
classification or for clustering (or a mix of both), depending on
which labels for items are observed.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<p>Multinomial mixture models are referred to as “naive Bayes” because
they are often applied to classification problems where the
multinomial independence assumptions are clearly false.</p>
<p>Naive Bayes classification and clustering can be applied to any data
with multinomial structure. A typical example of this is natural
language text classification and clustering, which is used an example
in what follows.</p>
<p>The observed data consists of a sequence of <span class="math inline">\(M\)</span> documents made up of
bags of words drawn from a vocabulary of <span class="math inline">\(V\)</span> distinct words. A
document <span class="math inline">\(m\)</span> has <span class="math inline">\(N_m\)</span> words, which are indexed as <span class="math inline">\(w_{m,1}, \ldots, w_{m,N[m]} \in 1{:}V\)</span>. Despite the ordered indexing of words in a
document, this order is not part of the model, which is clearly
defective for natural human language data. A number of topics (or
categories) <span class="math inline">\(K\)</span> is fixed.</p>
<p>The multinomial mixture model generates a single category <span class="math inline">\(z_m \in 1{:}K\)</span> for each document <span class="math inline">\(m \in 1{:}M\)</span> according to a categorical
distribution,
<span class="math display">\[
z_m \sim \mathsf{Categorical}(\theta).
\]</span>
The <span class="math inline">\(K\)</span>-simplex parameter <span class="math inline">\(\theta\)</span> represents the prevalence of each
category in the data.</p>
<p>Next, the words in each document are generated conditionally
independently of each other and the words in other documents based on
the category of the document, with word <span class="math inline">\(n\)</span> of document <span class="math inline">\(m\)</span> being
generated as
<span class="math display">\[
w_{m,n} \sim \mathsf{Categorical}(\phi_{z[m]}).
\]</span>
The parameter <span class="math inline">\(\phi_{z[m]}\)</span> is a <span class="math inline">\(V\)</span>-simplex representing the
probability of each word in the vocabulary in documents of category
<span class="math inline">\(z_m\)</span>.</p>
<p>The parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are typically given symmetric
Dirichlet priors. The prevalence <span class="math inline">\(\theta\)</span> is sometimes fixed to
produce equal probabilities for each category <span class="math inline">\(k \in 1:K\)</span>.</p>
<div id="coding-ragged-arrays" class="section level3 unnumbered">
<h3>Coding Ragged Arrays</h3>
<p>The specification for naive Bayes in the previous sections have used a ragged
array notation for the words <span class="math inline">\(w\)</span>. Because Stan does not support
ragged arrays, the models are coded using an alternative strategy that
provides an index for each word in a global list of words. The data
is organized as follows, with the word arrays laid out in a column and each
assigned to its document in a second column.</p>

<p>The relevant variables for the program are <code>N</code>, the total number
of words in all the documents, the word array <code>w</code>, and the
document identity array <code>doc</code>.</p>
</div>
<div id="estimation-with-category-labeled-training-data" class="section level3 unnumbered">
<h3>Estimation with Category-Labeled Training Data</h3>
<p>A naive Bayes model for estimating the simplex parameters given
training data with documents of known categories can be coded in Stan
as follows</p>
<pre><code>data {
  // training data
  int&lt;lower=1&gt; K;               // num topics
  int&lt;lower=1&gt; V;               // num words
  int&lt;lower=0&gt; M;               // num docs
  int&lt;lower=0&gt; N;               // total word instances
  int&lt;lower=1,upper=K&gt; z[M];    // topic for doc m
  int&lt;lower=1,upper=V&gt; w[N];    // word n
  int&lt;lower=1,upper=M&gt; doc[N];  // doc ID for word n
  // hyperparameters
  vector&lt;lower=0&gt;[K] alpha;     // topic prior
  vector&lt;lower=0&gt;[V] beta;      // word prior
}
parameters {
  simplex[K] theta;   // topic prevalence
  simplex[V] phi[K];  // word dist for topic k
}
model {
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    z[m] ~ categorical(theta);
  for (n in 1:N)
    w[n] ~ categorical(phi[z[doc[n]]]);
}</code></pre>
<p>The topic identifiers <span class="math inline">\(z_m\)</span> are declared as data and the
latent category assignments are included as part of the likelihood
function.</p>
</div>
<div id="estimation-without-category-labeled-training-data" class="section level3 unnumbered">
<h3>Estimation without Category-Labeled Training Data</h3>
<p>Naive Bayes models can be used in an unsupervised fashion to cluster
multinomial-structured data into a fixed number <span class="math inline">\(K\)</span> of categories.
The data declaration includes the same variables as the model in the
previous section excluding the topic labels <code>z</code>. Because
<code>z</code> is discrete, it needs to be summed out of the model
calculation. This is done for naive Bayes as for other mixture
models. The parameters are the same up to the priors, but the
likelihood is now computed as the marginal document probability
<span class="math display">\[
\begin{array}{l}
\log p(w_{m,1},\ldots,w_{m,N_m}|\theta,\phi)
\\[2pt]
\ \ \ = \
\log \sum_{k=1}^K
\left( \mathsf{Categorical}(k|\theta)
        * \prod_{n=1}^{N_m} \mathsf{Categorical}(w_{m,n}|\phi_k)
\right)
\\[6pt]
\ \ \ = \
\log \sum_{k=1}^K \exp \left(
\log \mathsf{Categorical}(k|\theta)
+ \sum_{n=1}^{N_m} \log \mathsf{Categorical}(w_{m,n}|\phi_k)
\right).
\end{array}
\]</span></p>
<p>The last step shows how the <code>log_sum_exp</code> function can be used
to stabilize the numerical calculation and return a result on the log
scale.</p>
<pre><code>model {
  real gamma[M, K];
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    for (k in 1:K)
      gamma[m, k] = categorical_lpmf(k | theta);
  for (n in 1:N)
    for (k in 1:K)
      gamma[doc[n], k] = gamma[doc[n], k]
                         + categorical_lpmf(w[n] | phi[k]);
  for (m in 1:M)
    target += log_sum_exp(gamma[m]);
}</code></pre>
<p>The local variable <code>gamma[m, k]</code> represents the value
<span class="math display">\[
\gamma_{m,k} = \log \mathsf{Categorical}(k|\theta)
+ \sum_{n=1}^{N_m} \log \mathsf{Categorical}(w_{m,n}|\phi_k).
\]</span></p>
<p>Given <span class="math inline">\(\gamma\)</span>, the posterior probability that document
<span class="math inline">\(m\)</span> is assigned category <span class="math inline">\(k\)</span> is
<span class="math display">\[
\mbox{Pr}[z_m = k|w,\alpha,\beta]
=
\exp \left(
\gamma_{m,k}
- \log \sum_{k=1}^K \exp \left( \gamma_{m,k} \right)
\right).
\]</span></p>
<p>If the variable <code>gamma</code> were declared and defined in the
transformed parameter block, its sampled values would be saved by
Stan. The normalized posterior probabilities could also be defined as
generated quantities.</p>
</div>
<div id="full-bayesian-inference-for-naive-bayes" class="section level3 unnumbered">
<h3>Full Bayesian Inference for Naive Bayes</h3>
<p>Full Bayesian posterior predictive inference for the naive Bayes model
can be implemented in Stan by combining the models for labeled and
unlabeled data. The estimands include both the model parameters and
the posterior distribution over categories for the unlabeled data. The
model is essentially a missing data model assuming the unknown
category labels are missing completely at random; see
<span class="citation">Gelman et al. (<a href="#ref-GelmanEtAl:2013">2013</a>)</span> and <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill:2007">2007</a>)</span> for more
information on missing data imputation. The model is also an instance
of semisupervised learning because the unlabeled data contributes to
the parameter estimations.</p>
<p>To specify a Stan model for performing full Bayesian inference, the
model for labeled data is combined with the model for unlabeled data.
A second document collection is declared as data, but without the
category labels, leading to new variables <code>M2</code> <code>N2</code>,
<code>w2</code>, and <code>doc2</code>. The number of categories and number of
words, as well as the hyperparameters are shared and only declared
once. Similarly, there is only one set of parameters. Then the model
contains a single set of statements for the prior, a set of statements
for the labeled data, and a set of statements for the unlabeled data.</p>
</div>
<div id="prediction-without-model-updates" class="section level3 unnumbered">
<h3>Prediction without Model Updates</h3>
<p>An alternative to full Bayesian inference involves estimating a model
using labeled data, then applying it to unlabeled data without
updating the parameter estimates based on the unlabeled data. This
behavior can be implemented by moving the definition of <code>gamma</code>
for the unlabeled documents to the generated quantities block.
Because the variables no longer contribute to the log probability,
they no longer jointly contribute to the estimation of the model
parameters.</p>
</div>
</div>
<div id="latent-dirichlet-allocation" class="section level2">
<h2><span class="header-section-number">9.5</span> Latent Dirichlet Allocation</h2>
<p>Latent Dirichlet allocation (LDA) is a mixed-membership multinomial
clustering model <span class="citation">Blei, Ng, and Jordan (<a href="#ref-BleiNgJordan:2003">2003</a>)</span> that generalized naive
Bayes. Using the topic and document terminology common in discussions of
LDA, each document is modeled as having a mixture of topics, with each
word drawn from a topic based on the mixing proportions.</p>
<div id="the-lda-model" class="section level3 unnumbered">
<h3>The LDA Model</h3>
<p>The basic model assumes each document is generated independently based
on fixed hyperparameters. For document <span class="math inline">\(m\)</span>, the first step is to draw a topic
distribution simplex <span class="math inline">\(\theta_m\)</span> over the <span class="math inline">\(K\)</span> topics,</p>
<p><span class="math display">\[
\theta_m \sim \mathsf{Dirichlet}(\alpha).
\]</span></p>
<p>The prior hyperparameter <span class="math inline">\(\alpha\)</span> is fixed to a <span class="math inline">\(K\)</span>-vector of positive
values. Each word in the document is generated independently
conditional on the distribution <span class="math inline">\(\theta_m\)</span>. First, a topic
<span class="math inline">\(z_{m,n} \in 1{:}K\)</span> is drawn for the word based on the
document-specific topic-distribution,
<span class="math display">\[
z_{m,n} \sim \mathsf{Categorical}(\theta_m).
\]</span></p>
<p>Finally, the word <span class="math inline">\(w_{m,n}\)</span> is drawn according to the word distribution
for topic <span class="math inline">\(z_{m,n}\)</span>,
<span class="math display">\[
w_{m,n} \sim \mathsf{Categorical}(\phi_{z[m,n]}).
\]</span>
The distributions <span class="math inline">\(\phi_k\)</span> over words for topic <span class="math inline">\(k\)</span> are also given a
Dirichlet prior,
<span class="math display">\[
\phi_k \sim \mathsf{Dirichlet}(\beta)
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a fixed <span class="math inline">\(V\)</span>-vector of positive values.</p>
</div>
<div id="summing-out-the-discrete-parameters" class="section level3 unnumbered">
<h3>Summing out the Discrete Parameters</h3>
<p>Although Stan does not (yet) support discrete sampling, it is possible
to calculate the marginal distribution over the continuous parameters
by summing out the discrete parameters as in other mixture models.
The marginal posterior of the topic and word variables is</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(\theta,\phi|w,\alpha,\beta)
&amp; \propto &amp;
p(\theta|\alpha) p(\phi|\beta) p(w|\theta,\phi)
\\[4pt]
&amp; = &amp;
\prod_{m=1}^M p(\theta_m|\alpha)
*
\prod_{k=1}^K p(\phi_k|\beta)
*
\prod_{m=1}^M \prod_{n=1}^{M[n]} p(w_{m,n}|\theta_m,\phi).
\end{array}
\]</span></p>
<p>The inner word-probability term is defined by summing out the
topic assignments,</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(w_{m,n}|\theta_m,\phi)
&amp; = &amp;
\sum_{z=1}^K p(z,w_{m,n}|\theta_m,\phi).
\\[4pt]
&amp; = &amp;
\sum_{z=1}^K p(z|\theta_m) p(w_{m,n}|\phi_z).
\end{array}
\]</span></p>
<p>Plugging the distributions in and converting to the log scale provides a
formula that can be implemented directly in Stan,
<span class="math display">\[
\begin{array}{l}
\log p(\theta,\phi|w,\alpha,\beta)
\\[6pt]
{ } \ \
\begin{array}{l}
{ } = \sum_{m=1}^M \log \mathsf{Dirichlet}(\theta_m|\alpha)
\ + \
\sum_{k=1}^K \log \mathsf{Dirichlet}(\phi_k|\beta)
\\[6pt]
{ } \ \ \ \ \
+ \sum_{m=1}^M \sum_{n=1}^{N[m]} \log \left(
\sum_{z=1}^K
  \mathsf{Categorical}(z|\theta_m)
   * \mathsf{Categorical}(w_{m,n}|\phi_z)
 \right)
\end{array}
\end{array}
\]</span></p>
</div>
<div id="implementation-of-lda" class="section level3 unnumbered">
<h3>Implementation of LDA</h3>
<p>Applying the marginal derived in the last section to the data
structure described in this section leads to the following Stan
program for LDA.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;               // num topics
  int&lt;lower=2&gt; V;               // num words
  int&lt;lower=1&gt; M;               // num docs
  int&lt;lower=1&gt; N;               // total word instances
  int&lt;lower=1,upper=V&gt; w[N];    // word n
  int&lt;lower=1,upper=M&gt; doc[N];  // doc ID for word n
  vector&lt;lower=0&gt;[K] alpha;     // topic prior
  vector&lt;lower=0&gt;[V] beta;      // word prior
}
parameters {
  simplex[K] theta[M];   // topic dist for doc m
  simplex[V] phi[K];     // word dist for topic k
}
model {
  for (m in 1:M)
    theta[m] ~ dirichlet(alpha);  // prior
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);     // prior
  for (n in 1:N) {
    real gamma[K];
    for (k in 1:K)
      gamma[k] = log(theta[doc[n], k]) + log(phi[k, w[n]]);
    target += log_sum_exp(gamma);  // likelihood;
  }
}</code></pre>
<p>As in the other mixture models, the log-sum-of-exponents function is
used to stabilize the numerical arithmetic.</p>
</div>
<div id="correlated-topic-model" class="section level3 unnumbered">
<h3>Correlated Topic Model</h3>
<p>To account for correlations in the distribution of topics for
documents, <span class="citation">Blei and Lafferty (<a href="#ref-BleiLafferty:2007">2007</a>)</span> introduced a variant of LDA in
which the Dirichlet prior on the per-document topic distribution is
replaced with a multivariate logistic normal distribution.</p>
<p>The authors treat the prior as a fixed hyperparameter. They use an
<span class="math inline">\(L_1\)</span>-regularized estimate of covariance, which is equivalent to the
maximum a posteriori estimate given a double-exponential prior. Stan
does not (yet) support maximum a posteriori estimation, so the mean and
covariance of the multivariate logistic normal must be specified as
data.</p>
<div id="fixed-hyperparameter-correlated-topic-model" class="section level4 unnumbered">
<h4>Fixed Hyperparameter Correlated Topic Model</h4>
<p>The Stan model in the previous section can be modified to implement
the correlated topic model by replacing the Dirichlet topic prior
<code>alpha</code> in the data declaration with the mean and covariance of
the multivariate logistic normal prior.</p>
<pre><code>data {
  ... data as before without alpha ...
  vector[K] mu;          // topic mean
  cov_matrix[K] Sigma;   // topic covariance
}</code></pre>
<p>Rather than drawing the simplex parameter <code>theta</code> from a
Dirichlet, a parameter <code>eta</code> is drawn from a multivariate normal
distribution and then transformed using softmax into a simplex.</p>
<pre><code>parameters {
  simplex[V] phi[K];  // word dist for topic k
  vector[K] eta[M];   // topic dist for doc m
}
transformed parameters {
  simplex[K] theta[M];
  for (m in 1:M)
    theta[m] = softmax(eta[m]);
}
model {
  for (m in 1:M)
    eta[m] ~ multi_normal(mu, Sigma);
  ... model as before w/o prior for theta ...
}</code></pre>
</div>
<div id="full-bayes-correlated-topic-model" class="section level4 unnumbered">
<h4>Full Bayes Correlated Topic Model</h4>
<p>By adding a prior for the mean and covariance, Stan supports full
Bayesian inference for the correlated topic model. This requires
moving the declarations of topic mean <code>mu</code> and covariance <code>Sigma</code>
from the data block to the parameters block and providing them with
priors in the model. A relatively efficient and interpretable prior
for the covariance matrix <code>Sigma</code> may be encoded as follows.</p>
<pre><code>... data block as before, but without alpha ...
parameters {
  vector[K] mu;              // topic mean
  corr_matrix[K] Omega;      // correlation matrix
  vector&lt;lower=0&gt;[K] sigma;  // scales
  vector[K] eta[M];          // logit topic dist for doc m
  simplex[V] phi[K];         // word dist for topic k
}
transformed parameters {
  ... eta as above ...
  cov_matrix[K] Sigma;       // covariance matrix
  for (m in 1:K)
    Sigma[m, m] = sigma[m] * sigma[m] * Omega[m, m];
  for (m in 1:(K-1)) {
    for (n in (m+1):K) {
      Sigma[m, n] = sigma[m] * sigma[n] * Omega[m, n];
      Sigma[n, m] = Sigma[m, n];
    }
  }
}
model {
  mu ~ normal(0, 5);      // vectorized, diffuse
  Omega ~ lkj_corr(2.0);  // regularize to unit correlation
  sigma ~ cauchy(0, 5);   // half-Cauchy due to constraint
  ... words sampled as above ...
}</code></pre>
<p>The <span class="math inline">\(\mathsf{LkjCorr}\)</span> distribution with shape <span class="math inline">\(\alpha &gt; 0\)</span> has support
on correlation matrices (i.e., symmetric positive definite with unit
diagonal). Its density is defined by
<span class="math display">\[
\mathsf{LkjCorr}(\Omega|\alpha) \propto \mbox{det}(\Omega)^{\alpha - 1}
\]</span>
With a scale of <span class="math inline">\(\alpha = 2\)</span>, the weakly informative prior favors a
unit correlation matrix. Thus the compound effect of this prior on
the covariance matrix <span class="math inline">\(\Sigma\)</span> for the multivariate logistic normal is
a slight concentration around diagonal covariance matrices with scales
determined by the prior on <code>sigma</code>.</p>

</div>
</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-GelmanEtAl:2013">
<p>Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. London: Chapman &amp;Hall/CRC Press.</p>
</div>
<div id="ref-GelmanHill:2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel-Hierarchical Models</em>. Cambridge, United Kingdom: Cambridge University Press.</p>
</div>
<div id="ref-BleiNgJordan:2003">
<p>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3: 993–1022.</p>
</div>
<div id="ref-BleiLafferty:2007">
<p>Blei, David M., and John D. Lafferty. 2007. “A Correlated Topic Model of <em>Science</em>.” <em>The Annals of Applied Statistics</em> 1 (1): 17–37.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>For clustering, the non-identifiability problems for all mixture models present a problem, whereas there is no such problem for classification. Despite the difficulties with full Bayesian inference for clustering, researchers continue to use it, often in an exploratory data analysis setting rather than for predictive modeling.<a href="clustering-chapter.html#fnref21" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sparse-ragged-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gaussian-processes-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
