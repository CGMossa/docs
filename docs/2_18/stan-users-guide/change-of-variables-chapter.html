<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Stan User’s Guide</title>
  <meta name="description" content="Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Stan User’s Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stan User’s Guide" />
  
  <meta name="twitter:description" content="Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="problematic-posteriors-chapter.html">
<link rel="next" href="optimization-chapter.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan User's Guide</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 1. Example Models</i></span></a></li>
<li class="chapter" data-level="1" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>1</b> Regression Models</a></li>
<li class="chapter" data-level="2" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>2</b> Time-Series Models</a></li>
<li class="chapter" data-level="3" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>3</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="4" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>4</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="5" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="6" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>6</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="7" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>7</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="8" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>8</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="9" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>9</b> Clustering Models</a></li>
<li class="chapter" data-level="10" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>10</b> Gaussian Processes</a></li>
<li class="chapter" data-level="11" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>11</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="12" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>12</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="13" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>13</b> Ordinary Differential Equations</a></li>
<li><a href="part-2-programming-techniques.html#part-2.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 2. Programming Techniques</i></a></li>
<li class="chapter" data-level="14" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>14</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="15" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>15</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="16" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>16</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="17" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>17</b> User-Defined Functions</a></li>
<li class="chapter" data-level="18" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>18</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="19" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>19</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="20" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>20</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="21" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>21</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="22" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>22</b> Map-Reduce</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan User’s Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="change-of-variables.chapter" class="section level1">
<h1><span class="header-section-number">20</span> Reparameterization and Change of Variables</h1>
<p>Stan supports a direct encoding of reparameterizations.
Stan also supports changes of variables by directly incrementing the
log probability accumulator with the log Jacobian of the transform.</p>
<div id="theoretical-and-practical-background" class="section level2">
<h2><span class="header-section-number">20.1</span> Theoretical and Practical Background</h2>
<p>A Bayesian posterior is technically a probability <em>measure</em>,
which is a parameterization-invariant, abstract mathematical object.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a></p>
<p>Stan’s modeling language, on the other hand, defines a probability
<em>density</em>, which is a non-unique, parameterization-dependent
function in <span class="math inline">\(\mathbb{R}^N \rightarrow \mathbb{R}^{+}\)</span>. In practice, this
means a given model can be represented different ways in Stan, and
different representations have different computational performances.</p>
<p>As pointed out by <span class="citation">Gelman (<a href="#ref-Gelman:2004">2004</a>)</span> in a paper discussing the
relation between parameterizations and Bayesian modeling, a change of
parameterization often carries with it suggestions of how the model
might change, because we tend to use certain natural classes of prior
distributions. Thus, it’s not <em>just</em> that we have a fixed
distribution that we want to sample from, with reparameterizations
being computational aids. In addition, once we reparameterize and add
prior information, the model itself typically changes, often in useful
ways.</p>
</div>
<div id="reparameterizations" class="section level2">
<h2><span class="header-section-number">20.2</span> Reparameterizations</h2>
<p>Reparameterizations may be implemented directly using the transformed
parameters block or just in the model block.</p>
<div id="beta-and-dirichlet-priors" class="section level3 unnumbered">
<h3>Beta and Dirichlet Priors</h3>
<p>The beta and Dirichlet distributions may both be reparameterized from
a vector of counts to use a mean and total count.</p>
<div id="beta-distribution" class="section level4 unnumbered">
<h4>Beta Distribution</h4>
<p>For example, the Beta distribution is parameterized by two positive
count parameters <span class="math inline">\(\alpha, \beta &gt; 0\)</span>. The following example
illustrates a hierarchical Stan model with a vector of parameters
<code>theta</code> are drawn i.i.d. for a Beta distribution whose
parameters are themselves drawn from a hyperprior distribution.</p>
<pre><code>parameters {
  real&lt;lower = 0&gt; alpha;
  real&lt;lower = 0&gt; beta;
  ...
model {
  alpha ~ ...
  beta ~ ...
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
  ...</code></pre>
<p>It is often more natural to specify hyperpriors in terms of
transformed parameters. In the case of the Beta, the obvious choice
for reparameterization is in terms of a mean parameter
<span class="math display">\[
\phi = \alpha / (\alpha + \beta)
\]</span>
and total count parameter
<span class="math display">\[
\lambda = \alpha + \beta.
\]</span>
Following @[GelmanEtAl:2013, Chapter 5] the mean
gets a uniform prior and the count parameter a Pareto prior with
<span class="math inline">\(p(\lambda) \propto \lambda^{-2.5}\)</span>.</p>
<pre><code>parameters {
  real&lt;lower=0,upper=1&gt; phi;
  real&lt;lower=0.1&gt; lambda;
  ...
transformed parameters {
  real&lt;lower=0&gt; alpha = lambda * phi;
  real&lt;lower=0&gt; beta = lambda * (1 - phi);
  ...
model {
  phi ~ beta(1, 1); // uniform on phi, could drop
  lambda ~ pareto(0.1, 1.5);
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
  ...</code></pre>
<p>The new parameters, <code>phi</code> and <code>lambda</code>, are declared in the
parameters block and the parameters for the Beta distribution,
<code>alpha</code> and <code>beta</code>, are declared and defined in the
transformed parameters block. And If their values are not of interest,
they could instead be defined as local variables in the model as
follows.</p>
<pre><code>model {
  real alpha = lambda * phi
  real beta = lambda * (1 - phi);
...
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
...
}</code></pre>
<p>With vectorization, this could be expressed more compactly and
efficiently as follows.</p>
<pre><code>model {
  theta ~ beta(lambda * phi, lambda * (1 - phi));
...
}</code></pre>
<p>If the variables <code>alpha</code> and <code>beta</code> are of interest, they
can be defined in the transformed parameter block and then used in the
model.</p>
</div>
<div id="jacobians-not-necessary" class="section level4 unnumbered">
<h4>Jacobians not Necessary</h4>
<p>Because the transformed parameters are being used, rather than given a
distribution, there is no need to apply a Jacobian adjustment for the
transform. For example, in the beta distribution example,
<code>alpha</code> and <code>beta</code> have the correct posterior distribution.</p>
</div>
<div id="dirichlet-priors" class="section level4 unnumbered">
<h4>Dirichlet Priors</h4>
<p>The same thing can be done with a Dirichlet, replacing the mean for
the Beta, which is a probability value, with a simplex. Assume there
are <span class="math inline">\(K &gt; 0\)</span> dimensions being considered (<span class="math inline">\(K=1\)</span> is trivial and <span class="math inline">\(K=2\)</span>
reduces to the beta distribution case). The traditional prior is</p>
<pre><code>parameters {
  vector[K] alpha;
  simplex[K] theta[N];
  ...
model {
  alpha ~ ...;
  for (n in 1:N)
    theta[n] ~ dirichlet(alpha);
}</code></pre>
<p>This provides essentially <span class="math inline">\(K\)</span> degrees of freedom, one for each
dimension of <code>alpha</code>, and it is not obvious how to specify a
reasonable prior for <code>alpha</code>.</p>
<p>An alternative coding is to use the mean, which is a simplex, and a
total count.</p>
<pre><code>parameters {
  simplex[K] phi;
  real&lt;lower=0&gt; kappa;
  simplex[K] theta[N];
  ...
transformed parameters {
  vector[K] alpha = kappa * phi;
  ...
}
model {
  phi ~ ...;
  kappa ~ ...;
  for (n in 1:N)
    theta[n] ~ dirichlet(alpha);</code></pre>
<p>Now it is much easier to formulate priors, because <code>phi</code> is the
expected value of <code>theta</code> and <code>kappa</code> (minus <code>K</code>) is
the strength of the prior mean measured in number of prior observations.</p>
</div>
</div>
<div id="transforming-unconstrained-priors-probit-and-logit" class="section level3 unnumbered">
<h3>Transforming Unconstrained Priors: Probit and Logit</h3>
<p>If the variable <span class="math inline">\(u\)</span> has a <span class="math inline">\(\mathsf{Uniform}(0, 1)\)</span> distribution, then
<span class="math inline">\(\mbox{logit}(u)\)</span> is distributed as <span class="math inline">\(\mathsf{Logistic}(0, 1)\)</span>. This
is because inverse logit is the cumulative distribution function (cdf)
for the logistic distribution, so that the logit function itself is
the inverse cdf and thus maps a uniform draw in <span class="math inline">\((0, 1)\)</span> to a
logistically-distributed quantity.</p>
<p>Things work the same way for the probit case: if <span class="math inline">\(u\)</span> has a
<span class="math inline">\(\mathsf{Uniform}(0, 1)\)</span> distribution, then <span class="math inline">\(\Phi^{-1}(u)\)</span> has a
<span class="math inline">\(\mathsf{normal}(0, 1)\)</span> distribution. The other way around, if <span class="math inline">\(v\)</span>
has a <span class="math inline">\(\mathsf{normal}(0, 1)\)</span> distribution, then <span class="math inline">\(\Phi(v)\)</span> has a
<span class="math inline">\(\mathsf{Uniform}(0, 1)\)</span> distribution.</p>
<p>In order to use the probit and logistic as priors on variables
constrained to <span class="math inline">\((0, 1)\)</span>, create an unconstrained variable and
transform it appropriately. For comparison, the following Stan
program fragment declares a <span class="math inline">\((0, 1)\)</span>-constrained parameter
<code>theta</code> and gives it a beta prior, then uses it as a parameter in
a distribution (here using <code>foo</code> as a placeholder).</p>
<pre><code>parameters {
  real&lt;lower = 0, upper = 1&gt; theta;
...
model {
  theta ~ beta(a, b);
  ...
  y ~ foo(theta);
...</code></pre>
<p>If the variables <code>a</code> and <code>b</code> are one, then this imposes
a uniform distribution <code>theta</code>. If <code>a</code> and <code>b</code> are
both less than one, then the density on <code>theta</code> has a U shape,
whereas if they are both greater than one, the density of <code>theta</code>
has an inverted-U or more bell-like shape.</p>
<p>Roughly the same result can be achieved with unbounded parameters that
are probit or inverse-logit-transformed. For example,</p>
<pre><code>parameters {
  real theta_raw;
...
transformed parameters {
  real&lt;lower = 0, upper = 1&gt; theta = inv_logit(theta_raw);
...
model {
  theta_raw ~ logistic(mu, sigma);
  ...
  y ~ foo(theta);
...</code></pre>
<p>In this model, an unconstrained parameter <code>theta_raw</code> gets a
logistic prior, and then the transformed parameter <code>theta</code> is
defined to be the inverse logit of <code>theta_raw</code>. In this
parameterization, <code>inv_logit(mu)</code> is the mean of the implied
prior on <code>theta</code>. The prior distribution on <code>theta</code> will be
flat if <code>sigma</code> is one and <code>mu</code> is zero, and will be
U-shaped if <code>sigma</code> is larger than one and bell shaped if
<code>sigma</code> is less than one.</p>
<p>When moving from a variable in <span class="math inline">\((0, 1)\)</span> to a simplex, the same trick
may be performed using the softmax function, which is a multinomial
generalization of the inverse logit function. First, consider a
simplex parameter with a Dirichlet prior.</p>
<pre><code>parameters {
  simplex[K] theta;
...
model {
  theta ~ dirichlet(a);
  ...
  y ~ foo(theta);</code></pre>
<p>Now <code>a</code> is a vector with <code>K</code> rows, but it has the same shape
properties as the pair <code>a</code> and <code>b</code> for a beta; the beta
distribution is just the distribution of the first component of a
Dirichlet with parameter vector <span class="math inline">\([a b]^{\top}\)</span>. To formulate an
unconstrained prior, the exact same strategy works as for the beta.</p>
<pre><code>parameters {
  vector[K] theta_raw;
...
transformed parameters {
  simplex[K] theta = softmax(theta_raw);
...
model {
  theta_raw ~ multi_normal_cholesky(mu, L_Sigma);</code></pre>
<p>The multivariate normal is used for convenience and efficiency with
its Cholesky-factor parameterization. Now the mean is controlled by
<code>softmax(mu)</code>, but we have additional control of covariance
through <code>L_Sigma</code> at the expense of having on the order of <span class="math inline">\(K^2\)</span>
parameters in the prior rather than order <span class="math inline">\(K\)</span>. If no covariance is
desired, the number of parameters can be reduced back to <span class="math inline">\(K\)</span> using a
vectorized normal distribution as follows.</p>
<pre><code>  theta_raw ~ normal(mu, sigma);</code></pre>
<p>where either or both of <code>mu</code> and <code>sigma</code> can be vectors.</p>
</div>
</div>
<div id="changes-of-variables" class="section level2">
<h2><span class="header-section-number">20.3</span> Changes of Variables</h2>
<p>Changes of variables are applied when the transformation of a
parameter is characterized by a distribution. The standard textbook
example is the lognormal distribution, which is the distribution of a
variable <span class="math inline">\(y &gt; 0\)</span> whose logarithm <span class="math inline">\(\log y\)</span> has a normal distribution.
The distribution is being assigned to <span class="math inline">\(\log y\)</span>.</p>
<p>The change of variables requires an adjustment to the probability to
account for the distortion caused by the transform. For this to work,
univariate changes of variables must be monotonic and differentiable
everywhere in their support.</p>
<p>For univariate changes of variables, the resulting probability must be
scaled by the absolute derivative of the transform.</p>
<p>In the case of log normals, if <span class="math inline">\(y\)</span>’s logarithm is normal with mean
<span class="math inline">\(\mu\)</span> and deviation <span class="math inline">\(\sigma\)</span>, then the distribution of <span class="math inline">\(y\)</span> is given by
<span class="math display">\[
p(y)
\ = \
\mathsf{normal}(\log y| \mu, \sigma) \  \left| \frac{d}{dy} \log y \right|
\ = \
\mathsf{normal}(\log y| \mu, \sigma) \frac{1}{y}.
\]</span>
Stan works on the log scale to prevent underflow, where
<span class="math display">\[
\log p(y)
=
\log \mathsf{normal}(\log y| \mu, \sigma)
- \log y.
\]</span></p>
<p>In Stan, the change of variables can be applied in the sampling
statement. To adjust for the curvature, the log probability
accumulator is incremented with the log absolute derivative of the
transform. The lognormal distribution can thus be implemented
directly in Stan as follows.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a></p>
<pre><code>parameters {
  real&lt;lower=0&gt; y;
  ...
model {
  log(y) ~ normal(mu, sigma);
  target += -log(y);
  ...</code></pre>
<p>It is important, as always, to declare appropriate constraints on
parameters; here <code>y</code> is constrained to be positive.</p>
<p>It would be slightly more efficient to define a local variable for the
logarithm, as follows.</p>
<pre><code>model {
  real log_y;
  log_y = log(y);
  log_y ~ normal(mu, sigma);
  target += -log_y;
  ...</code></pre>
<p>If <code>y</code> were declared as data instead of as a parameter, then the
adjustment can be ignored because the data will be constant and Stan
only requires the log probability up to a constant.</p>
<div id="change-of-variables-vs.transformations" class="section level3 unnumbered">
<h3>Change of Variables vs. Transformations</h3>
<p>This section illustrates the difference between a change of variables
and a simple variable transformation. A transformation samples a
parameter, then transforms it, whereas a change of variables
transforms a parameter, then samples it. Only the latter requires a
Jacobian adjustment.</p>
<p>It does not matter whether the probability function is
expressed using a sampling statement, such as</p>
<pre><code>log(y) ~ normal(mu, sigma);</code></pre>
<p>or as an increment to the log probability function, as in</p>
<pre><code>target += normal_lpdf(log(y) | mu, sigma);</code></pre>
<div id="jacobian-adjustment.section" class="section level4">
<h4><span class="header-section-number">20.3.0.1</span> Gamma and Inverse Gamma Distribution {-}</h4>
<p>Like the log normal, the inverse gamma distribution is a distribution
of variables whose inverse has a gamma distribution. This section
contrasts two approaches, first with a transform, then with a change
of variables.</p>
<p>The transform based approach to sampling <code>y_inv</code> with an inverse
gamma distribution can be coded as follows.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; y;
}
transformed parameters {
  real&lt;lower=0&gt; y_inv;
  y_inv = 1 / y;
}
model {
  y ~ gamma(2,4);
}</code></pre>
<p>The change-of-variables approach to sampling <code>y_inv</code> with an
inverse gamma distribution can be coded as follows.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; y_inv;
}
transformed parameters {
  real&lt;lower=0&gt; y;
  y = 1 / y_inv;  // change variables
}
model {
  y ~ gamma(2,4);
  target +=  -2 * log(y_inv);  //  Jacobian adjustment;
}</code></pre>
<p>The Jacobian adjustment is the log of the absolute derivative of the
transform, which in this case is</p>
<p><span class="math display">\[
\log \left| \frac{d}{du} \left( \frac{1}{u} \right) \right|
\ = \
\log | - u^{-2} |
\ = \
\log u^{-2}
\ = \
 -2 \log u.
\]</span></p>
</div>
</div>
<div id="multivariate-changes-of-variables" class="section level3 unnumbered">
<h3>Multivariate Changes of Variables</h3>
<p>In the case of a multivariate transform, the log of the Jacobian of
the transform must be added to the log probability accumulator. In
Stan, this can be coded as follows in the general case where the
Jacobian is not a full matrix.</p>
<pre><code>parameters {
  vector[K] u;      // multivariate parameter
   ...
transformed parameters {
  vector[K] v;     // transformed parameter
  matrix[K, K] J;   // Jacobian matrix of transform
  ... compute v as a function of u ...
  ... compute J[m, n] = d.v[m] / d.u[n] ...
  target += log(fabs(determinant(J)));
  ...
model {
  v ~ ...;
  ...</code></pre>
<p>If the Jacobian is known analytically, it will be more
efficient to apply it directly than to call the determinant function,
which is neither efficient nor particularly stable numerically.</p>
<p>In many cases, the Jacobian matrix will be triangular, so that only
the diagonal elements will be required for the determinant
calculation. Triangular Jacobians arise when each element <code>v[k]</code>
of the transformed parameter vector only depends on elements
<code>u[1]</code>, , <code>u[k]</code> of the parameter vector. For
triangular matrices, the determinant is the product of the diagonal
elements, so the transformed parameters block of the above model can
be simplified and made more efficient by recoding as follows.</p>
<pre><code>transformed parameters {
  ...
  vector[K] J_diag;  // diagonals of Jacobian matrix
  ...
  ... compute J[k, k] = d.v[k] / d.u[k] ...
  target += sum(log(J_diag));
  ...</code></pre>
</div>
</div>
<div id="vectors-with-varying-bounds" class="section level2">
<h2><span class="header-section-number">20.4</span> Vectors with Varying Bounds</h2>
<p>Stan only allows a single lower and upper bound to be declared in the
constraints for a container data type. But suppose we have a vector
of parameters and a vector of lower bounds? Then the transforms are
calculated and their log Jacobians added to the log density accumulator;
the Jacobian calculations are described in detail in the reference
manual chapter on constrained parameter transforms.</p>
<div id="varying-lower-bounds" class="section level3 unnumbered">
<h3>Varying Lower Bounds</h3>
<p>For example, suppose there is a vector parameter <span class="math inline">\(\alpha\)</span> with a
vector <span class="math inline">\(L\)</span> of lower bounds. The simplest way to deal with this if <span class="math inline">\(L\)</span>
is a constant is to shift a lower-bounded parameter.</p>
<pre><code>data {
  int N;
  vector[N] L;  // lower bounds
  ...
parameters {
  vector&lt;lower=0&gt;[N] alpha_raw;
  ...
transformed parameters {
  vector[N] alpha = L + alpha_raw;
  ...</code></pre>
<p>The Jacobian for adding a constant is one, so its log drops out of the
log density.</p>
<p>Even if the lower bound is a parameter rather than data, there is no
Jacobian required, because the transform from <span class="math inline">\((L, \alpha_{\mathrm  raw})\)</span> to <span class="math inline">\((L + \alpha_{\mathrm raw}, \alpha_{\mathrm raw}\)</span> produces
a Jacobian derivative matrix with a unit determinant.</p>
<p>It’s also possible implement the transform by directly transforming
an unconstrained parameter and accounting for the Jacobian.</p>
<pre><code>data {
  int N;
  vector[N] L;  // lower bounds
  ...
parameters {
  vector[N] alpha_raw;
  ...
transformed parameters {
  vector[N] alpha = L + exp(alpha_raw);
  ...
model {
  target += sum(alpha_raw);  // log Jacobian
  ...</code></pre>
<p>The adjustment in the the log Jacobian determinant of the transform
mapping <span class="math inline">\(\alpha_{\mathrm{raw}}\)</span> to
<span class="math inline">\(\alpha = L + \exp(\alpha_{\mathrm{raw}})\)</span>. The details are simple in
this case because the Jacobian is diagonal; see the reference manual
chapter on constrained parameter transforms for full details. Here
<span class="math inline">\(L\)</span> can even be a vector containing parameters that don’t depend on
<span class="math inline">\(\alpha_{\mathrm{raw}}\)</span>; if the bounds do depend on
<span class="math inline">\(\alpha_{\mathrm{raw}}\)</span> then a revised Jacobian needs to be calculated
taking into account the dependencies.</p>
</div>
<div id="varying-upper-and-lower-bounds" class="section level3 unnumbered">
<h3>Varying Upper and Lower Bounds</h3>
<p>Suppose there are lower and upper bounds that vary by parameter.
These can be applied to shift and rescale a parameter constrained to
<span class="math inline">\((0, 1)\)</span>.</p>
<pre><code>data {
  int N;
  vector[N] L;  // lower bounds
  vector[N] U;  // upper bounds
  ...
parameters {
  vector&lt;lower=0, upper=1&gt;[N] alpha_raw;
  ...
transformed parameters {
  vector[N] alpha = L + (U - L) .* alpha_raw;</code></pre>
<p>The expression <code>U - L</code> is multiplied by <code>alpha_raw</code>
elementwise to produce a vector of variables in <span class="math inline">\((0, U-L)\)</span>, then
adding <span class="math inline">\(L\)</span> results in a variable ranging between <span class="math inline">\((L, U)\)</span>.</p>
<p>In this case, it is important that <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> are constants,
otherwise a Jacobian would be required when multiplying by <span class="math inline">\(U - L\)</span>.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Gelman:2004">
<p>Gelman, Andrew. 2004. “Parameterization and Bayesian Modeling.” <em>Journal of the American Statistical Association</em> 99: 537–45.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="36">
<li id="fn36"><p>This is in contrast to (penalized) maximum likelihood estimates, which are not parameterization invariant.<a href="change-of-variables-chapter.html#fnref36" class="footnote-back">↩</a></p></li>
<li id="fn37"><p>This example is for illustrative purposes only; the recommended way to implement the lognormal distribution in Stan is with the built-in <code>lognormal</code> probability function; see the functions reference manual for details.<a href="change-of-variables-chapter.html#fnref37" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="problematic-posteriors-chapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimization-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
