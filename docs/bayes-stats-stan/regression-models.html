<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="example-models-part.html">
<link rel="next" href="time-series-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-overview.html#part-1-overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Overview</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>2</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="3" data-path="simple-examples.html"><a href="simple-examples.html"><i class="fa fa-check"></i><b>3</b> Simple Examples</a></li>
<li class="chapter" data-level="4" data-path="bayesian-workflow-1.html"><a href="bayesian-workflow-1.html"><i class="fa fa-check"></i><b>4</b> Bayesian Workflow</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="5" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>5</b> Regression Models</a></li>
<li class="chapter" data-level="6" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>6</b> Time-Series Models</a></li>
<li class="chapter" data-level="7" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>7</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="8" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>8</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="9" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>9</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="10" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>10</b> Finite Mixtures</a></li>
<li class="chapter" data-level="11" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>11</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="12" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>12</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="13" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>13</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="14" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>14</b> Clustering Models</a></li>
<li class="chapter" data-level="15" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>15</b> Gaussian Processes</a></li>
<li class="chapter" data-level="16" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>16</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="17" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>17</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="18" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>18</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="19" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>19</b> Modeling as Software Development</a></li>
<li class="chapter" data-level="20" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>20</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="21" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>21</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="22" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>22</b> User-Defined Functions</a></li>
<li class="chapter" data-level="23" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>23</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="24" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>24</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="25" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>25</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="26" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>26</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="27" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>27</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="28" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>28</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="29" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>29</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="30" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>30</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>31</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-models" class="section level1">
<h1><span class="header-section-number">5</span> Regression Models</h1>
<p>Stan supports regression models from simple linear regressions to
multilevel generalized linear models.</p>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">5.1</span> Linear Regression</h2>
<p>The simplest linear regression model is the following, with a single
predictor and a slope and intercept coefficient, and normally
distributed noise. This model can be written using standard
regression notation as</p>
<p><span class="math display">\[
y_n = \alpha + \beta x_n + \epsilon_n
\ \ \ \mbox{where} \ \ \
\epsilon_n \sim \mathsf{normal}(0,\sigma).
\]</span>
This is equivalent to the following sampling involving the
residual,
<span class="math display">\[
y_n - (\alpha + \beta X_n) \sim \mathsf{normal}(0,\sigma),
\]</span>
and reducing still further, to
<span class="math display">\[
y_n \sim \mathsf{normal}(\alpha + \beta X_n, \, \sigma).
\]</span></p>
<p>This latter form of the model is coded in Stan as follows.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}</code></pre>
<p>There are <code>N</code> observations, each with predictor <code>x[n]</code> and
outcome <code>y[n]</code>. The intercept and slope parameters are
<code>alpha</code> and <code>beta</code>. The model assumes a normally
distributed noise term with scale <code>sigma</code>. This model has
improper priors for the two regression coefficients.</p>
<div id="vectorization-section" class="section level3 unnumbered">
<h3>Matrix Notation and Vectorization</h3>
<p>The sampling statement in the previous model is vectorized, with</p>
<pre><code>  y ~ normal(alpha + beta * x, sigma);</code></pre>
<p>providing the same model as the unvectorized version,</p>
<pre><code>  for (n in 1:N)
    y[n] ~ normal(alpha + beta * x[n], sigma);</code></pre>
<p>In addition to being more concise, the vectorized form is much faster.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>In general, Stan allows the arguments to distributions such as
<code>normal</code> to be vectors. If any of the other arguments are vectors or
arrays, they have to be the same size. If any of the other arguments
is a scalar, it is reused for each vector entry. See <a href="regression-models.html#vectorization-section">the
vectorization section</a> for more information on
vectorization of probability functions.</p>
<p>The other reason this works is that Stan’s arithmetic operators are
overloaded to perform matrix arithmetic on matrices. In this case,
because <code>x</code> is of type <code>vector</code> and <code>beta</code> of type
<code>real</code>, the expression <code>beta * x</code> is of type <code>vector</code>.
Because Stan supports vectorization, a regression model with more than
one predictor can be written directly using matrix notation.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;   // number of data items
  int&lt;lower=0&gt; K;   // number of predictors
  matrix[N, K] x;   // predictor matrix
  vector[N] y;      // outcome vector
}
parameters {
  real alpha;           // intercept
  vector[K] beta;       // coefficients for predictors
  real&lt;lower=0&gt; sigma;  // error scale
}
model {
  y ~ normal(x * beta + alpha, sigma);  // likelihood
}</code></pre>
<p>The constraint <code>lower=0</code> in the declaration of <code>sigma</code>
constrains the value to be greater than or equal to 0. With no prior
in the model block, the effect is an improper prior on non-negative
real numbers. Although a more informative prior may be added, improper
priors are acceptable as long as they lead to proper posteriors.</p>
<p>In the model above, <code>x</code> is an <span class="math inline">\(N \times K\)</span> matrix of predictors
and <code>beta</code> a <span class="math inline">\(K\)</span>-vector of coefficients, so <code>x * beta</code> is an
<span class="math inline">\(N\)</span>-vector of predictions, one for each of the <span class="math inline">\(N\)</span> data items. These
predictions line up with the outcomes in the <span class="math inline">\(N\)</span>-vector <code>y</code>, so
the entire model may be written using matrix arithmetic as shown. It
would be possible to include a column of ones the data matrix <code>x</code> and
remove the <code>alpha</code> parameter.</p>
<p>The sampling statement in the model above is just a more efficient,
vector-based approach to coding the model with a loop, as in the
following statistically equivalent model.</p>
<pre><code>model {
  for (n in 1:N)
    y[n] ~ normal(x[n] * beta, sigma);
}</code></pre>
<p>With Stan’s matrix indexing scheme, <code>x[n]</code> picks out row <code>n</code>
of the matrix <code>x</code>; because <code>beta</code> is a column vector,
the product <code>x[n] * beta</code> is a scalar of type <code>real</code>.</p>
<div id="intercepts-as-inputs" class="section level4 unnumbered">
<h4>Intercepts as Inputs</h4>
<p>In the model formulation</p>
<pre><code>  y ~ normal(x * beta, sigma);</code></pre>
<p>there is no longer an intercept coefficient <code>alpha</code>. Instead, we
have assumed that the first column of the input matrix <code>x</code> is a
column of 1 values. This way, <code>beta[1]</code> plays the role of the
intercept. If the intercept gets a different prior than the slope
terms, then it would be clearer to break it out. It is also slightly
more efficient in its explicit form with the intercept variable
singled out because there’s one fewer multiplications; it should not
make that much of a difference to speed, though, so the choice should
be based on clarity.</p>
</div>
</div>
</div>
<div id="QR-reparameterization.section" class="section level2">
<h2><span class="header-section-number">5.2</span> The QR Reparameterization</h2>
<p>In the previous example, the linear predictor can be written as <span class="math inline">\(\eta = x \beta\)</span>, where <span class="math inline">\(\eta\)</span> is a <span class="math inline">\(N\)</span>-vector of predictions, <span class="math inline">\(x\)</span> is a <span class="math inline">\(N \times K\)</span> matrix, and <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(K\)</span>-vector of coefficients.
Presuming <span class="math inline">\(N \geq K\)</span>, we can exploit the fact that any design matrix,
<span class="math inline">\(x\)</span> can be decomposed using the thin QR decomposition into an
orthogonal matrix <span class="math inline">\(Q\)</span> and an upper-triangular matrix <span class="math inline">\(R\)</span>, i.e. <span class="math inline">\(x = Q R\)</span>. See the reference manual for a definition of the QR-decoposition.
The functions <code>qr_Q</code> and <code>qr_R</code> implement the fat QR decomposition so
here we thin it by including only <span class="math inline">\(K\)</span> columns in <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> rows in
<span class="math inline">\(R\)</span> (see the reference manual for more information on the <code>qr_Q</code> and
<code>qr_R</code> functions). Also, in practice, it is best to write <span class="math inline">\(x = Q^\ast R^\ast\)</span> where <span class="math inline">\(Q^\ast = Q * \sqrt{n - 1}\)</span> and <span class="math inline">\(R^\ast = \frac{1}{\sqrt{n - 1}} R\)</span>. Thus, we can equivalently write <span class="math inline">\(\eta = x \beta = Q R \beta = Q^\ast R^\ast \beta\)</span>. If we let <span class="math inline">\(\theta = R^\ast \beta\)</span>, then we have <span class="math inline">\(\eta = Q^\ast \theta\)</span> and <span class="math inline">\(\beta = R^{\ast^{-1}} \theta\)</span>. In that case, the previous Stan program becomes</p>
<pre><code>data {
  int&lt;lower=0&gt; N;   // number of data items
  int&lt;lower=0&gt; K;   // number of predictors
  matrix[N, K] x;   // predictor matrix
  vector[N] y;      // outcome vector
}
transformed data {
  matrix[N, K] Q_ast;
  matrix[K, K] R_ast;
  matrix[K, K] R_ast_inverse;
  // thin and scale the QR decomposition
  Q_ast = qr_Q(x)[, 1:K] * sqrt(N - 1);
  R_ast = qr_R(x)[1:K, ] / sqrt(N - 1);
  R_ast_inverse = inverse(R_ast);
}
parameters {
  real alpha;           // intercept
  vector[K] theta;      // coefficients on Q_ast
  real&lt;lower=0&gt; sigma;  // error scale
}
model {
  y ~ normal(Q_ast * theta + alpha, sigma);  // likelihood
}
generated quantities {
  vector[K] beta;
  beta = R_ast_inverse * theta; // coefficients on x
}</code></pre>
<p>Since this Stan program generates equivalent predictions for <span class="math inline">\(y\)</span> and
the same posterior distribution for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> as
the previous Stan program, many wonder why the version with this QR
reparameterization performs so much better in practice, often both in
terms of wall time and in terms of effective sample size. The
reasoning is threefold:</p>
<ol style="list-style-type: decimal">
<li><p>The columns of <span class="math inline">\(Q^\ast\)</span> are orthogonal whereas the columns of
<span class="math inline">\(x\)</span> generally are not. Thus, it is easier for a Markov Chain to move
around in <span class="math inline">\(\theta\)</span>-space than in <span class="math inline">\(\beta\)</span>-space.</p></li>
<li><p>The columns of <span class="math inline">\(Q^\ast\)</span> have the same scale whereas the columns
of <span class="math inline">\(x\)</span> generally do not. Thus, a Hamiltonian Monte Carlo algorithm
can move around the parameter space with a smaller number of larger
steps</p></li>
<li><p>Since the covariance matrix for the columns of <span class="math inline">\(Q\ast\)</span> is an
identity matrix, <span class="math inline">\(\theta\)</span> typically has a reasonable scale if the
units of <span class="math inline">\(y\)</span> are also reasonable. This also helps HMC move
efficiently without compromising numerical accuracy.</p></li>
</ol>
<p>Consequently, this QR reparameterization is recommended for linear and
generalized linear models in Stan whenever <span class="math inline">\(K &gt; 1\)</span> and you do not have
an informative prior on the location of <span class="math inline">\(\beta\)</span>. It can also be
worthwhile to subtract the mean from each column of <span class="math inline">\(x\)</span> before
obtaining the QR decomposition, which does not affect the posterior
distribution of <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\beta\)</span> but does affect <span class="math inline">\(\alpha\)</span> and
allows you to interpret <span class="math inline">\(\alpha\)</span> as the expectation of <span class="math inline">\(y\)</span> in a linear
model.</p>
</div>
<div id="regression-priors.section" class="section level2">
<h2><span class="header-section-number">5.3</span> Priors for Coefficients and Scales</h2>
<p>See our <a href="prior-distributions-and-models-for-data.html#general-priors.section">general discussion of priors</a> for tips on priors for parameters in regression models.</p>
<p>Later sections discuss <a href="regression-models.html#hierarchical-priors.section">univariate
hierarchical priors</a> and <a href="regression-models.html#multivariate-hierarchical-priors.section">multivariate
hierarchical priors</a>, as well
as <a href="regression-models.html#priors-for-identification.section">priors used to identify models</a>.</p>
<p>However, as described in <a href="regression-models.html#QR-reparameterization.section">QR-reparameterization
section</a>, if you do not have an
informative prior on the <em>location</em> of the regression coefficients,
then you are better off reparameterizing your model so that the
regression coefficients are a generated quantity. In that case, it
usually does not matter much what prior is used on on the
reparameterized regression coefficients and almost any weakly
informative prior that scales with the outcome will do.</p>
</div>
<div id="robust-noise-models" class="section level2">
<h2><span class="header-section-number">5.4</span> Robust Noise Models</h2>
<p>The standard approach to linear regression is to model the noise
term <span class="math inline">\(\epsilon\)</span> as having a normal distribution. From Stan’s
perspective, there is nothing special about normally distributed
noise. For instance, robust regression can be accommodated by giving
the noise term a Student-<span class="math inline">\(t\)</span> distribution. To code this in Stan, the
sampling distribution is changed to the following.</p>
<pre><code>data {
  ...
  real&lt;lower=0&gt; nu;
}
...
model {
  y ~ student_t(nu, alpha + beta * x, sigma);
}</code></pre>
<p>The degrees of freedom constant <code>nu</code> is specified as data.</p>
</div>
<div id="logistic-probit-regression.section" class="section level2">
<h2><span class="header-section-number">5.5</span> Logistic and Probit Regression</h2>
<p>For binary outcomes, either of the closely related logistic or probit
regression models may be used. These generalized linear models vary
only in the link function they use to map linear predictions in
<span class="math inline">\((-\infty,\infty)\)</span> to probability values in <span class="math inline">\((0,1)\)</span>. Their respective
link functions, the logistic function and the standard normal cumulative distribution
function, are both sigmoid functions (i.e., they are both S-shaped).</p>
<p>A logistic regression model with one predictor and an intercept is coded as
follows.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
  int&lt;lower=0,upper=1&gt; y[N];
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ bernoulli_logit(alpha + beta * x);
}</code></pre>
<p>The noise parameter is built into the Bernoulli formulation here
rather than specified directly.</p>
<p>Logistic regression is a kind of generalized linear model with binary
outcomes and the log odds (logit) link function, defined by</p>
<p><span class="math display">\[
\mbox{logit}(v) = \log \left( \frac{v}{1-v} \right).
\]</span></p>
<p>The inverse of the link function appears in the model.</p>
<p><span class="math display">\[
\mbox{logit}^{-1}(u) = \frac{1}{1 + \exp(-u)}.
\]</span></p>
<p>The model formulation above uses the logit-parameterized version of
the Bernoulli distribution, which is defined by</p>
<p><span class="math display">\[
\mathsf{BernoulliLogit}(y|\alpha)
=
\mathsf{Bernoulli}(y \mid \mbox{logit}^{-1}(\alpha)).
\]</span></p>
<p>The formulation is also vectorized in the sense that <code>alpha</code> and
<code>beta</code> are scalars and <code>x</code> is a vector, so that <code>alpha   + beta * x</code> is a vector. The vectorized formulation is equivalent
to the less efficient version</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli_logit(alpha + beta * x[n]);</code></pre>
<p>Expanding out the Bernoulli logit, the model is equivalent to the more
explicit, but less efficient and less arithmetically stable</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli(inv_logit(alpha + beta * x[n]));</code></pre>
<p>Other link functions may be used in the same way. For example, probit
regression uses the cumulative normal distribution function, which is
typically written as</p>
<p><span class="math display">\[
\Phi(x) = \int_{-\infty}^x \mathsf{normal}(y|0,1) \, dy.
\]</span></p>
<p>The cumulative standard normal distribution function <span class="math inline">\(\Phi\)</span> is implemented
in Stan as the function <code>Phi</code>. The probit regression model
may be coded in Stan by replacing the logistic model’s sampling
statement with the following.</p>
<pre><code>        y[n] ~ bernoulli(Phi(alpha + beta * x[n]));</code></pre>
<p>A fast approximation to the cumulative standard normal distribution
function <span class="math inline">\(\Phi\)</span> is implemented in Stan as the function
<code>Phi_approx</code>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
The approximate probit regression model may
be coded with the following.</p>
<pre><code>        y[n] ~ bernoulli(Phi_approx(alpha + beta * x[n]));</code></pre>
</div>
<div id="multi-logit.section" class="section level2">
<h2><span class="header-section-number">5.6</span> Multi-Logit Regression</h2>
<p>Multiple outcome forms of logistic regression can be coded directly in
Stan. For instance, suppose there are <span class="math inline">\(K\)</span> possible outcomes for each
output variable <span class="math inline">\(y_n\)</span>. Also suppose that there is a <span class="math inline">\(D\)</span>-dimensional
vector <span class="math inline">\(x_n\)</span> of predictors for <span class="math inline">\(y_n\)</span>. The multi-logit model with
<span class="math inline">\(\mathsf{normal}(0,5)\)</span> priors on the coefficients is coded as follows.</p>
<pre><code>data {
  int K;
  int N;
  int D;
  int y[N];
  matrix[N, D] x;
}
parameters {
  matrix[D, K] beta;
}
model {
  matrix[N, K] x_beta = x * beta;

  to_vector(beta) ~ normal(0, 2);

  for (n in 1:N)
    y[n] ~ categorical_logit(x_beta[n]);
}</code></pre>
<p>The prior on <code>beta</code> is coded in vectorized form. As of Stan 2.18, the
categorical-logit distribution is not vectorized for parameter
arguments, so the loop is required. The matrix multiplication is
pulled out to define a local variable for all of the predictors for
efficiency. Like the Bernoulli-logit, the categorical-logit
distribution applies softmax internally to convert an arbitrary vector
to a simplex,</p>
<p><span class="math display">\[
\mathsf{CategoricalLogit}(y \mid \alpha)
\ = \
\mathsf{Categorical}(y \mid \mathsf{softmax}(\alpha)),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathrm{softmax}(u) = \mathrm{exp}(u) / \mathrm{sum}(\mathrm{exp}(u)).
\]</span></p>
<p>The categorical distribution with log-odds (logit) scaled parameters
used above is equivalent to writing</p>
<pre><code>    y[n] ~ categorical(softmax(x[n] * beta));</code></pre>
<div id="constraints-on-data-declarations" class="section level4 unnumbered">
<h4>Constraints on Data Declarations</h4>
<p>The data block in the above model is defined without constraints on
sizes <code>K</code>, <code>N</code>, and <code>D</code> or on the outcome array
<code>y</code>. Constraints on data declarations provide error checking at
the point data are read (or transformed data are defined), which is
before sampling begins. Constraints on data declarations also make
the model author’s intentions more explicit, which can help with
readability. The above model’s declarations could be tightened to</p>
<pre><code>  int&lt;lower = 2&gt; K;
  int&lt;lower = 0&gt; N;
  int&lt;lower = 1&gt; D;
  int&lt;lower = 1, upper = K&gt; y[N];</code></pre>
<p>These constraints arise because the number of categories, <code>K</code>,
must be at least two in order for a categorical model to be useful.
The number of data items, <code>N</code>, can be zero, but not negative;
unlike R, Stan’s for-loops always move forward, so that a loop extent
of <code>1:N</code> when <code>N</code> is equal to zero ensures the loop’s body
will not be executed. The number of predictors, <code>D</code>, must be at
least one in order for <code>beta * x[n]</code> to produce an
appropriate argument for <code>softmax()</code>. The categorical outcomes
<code>y[n]</code> must be between <code>1</code> and <code>K</code> in order for the
discrete sampling to be well defined.</p>
<p>Constraints on data declarations are optional. Constraints on
parameters declared in the <code>parameters</code> block, on the other hand,
are <em>not</em> optional—they are required to ensure support for all
parameter values satisfying their constraints. Constraints on
transformed data, transformed parameters, and generated quantities are
also optional.</p>
</div>
<div id="identifiability" class="section level3 unnumbered">
<h3>Identifiability</h3>
<p>Because softmax is invariant under adding a constant to each component
of its input, the model is typically only identified if there is a
suitable prior on the coefficients.</p>
<p>An alternative is to use <span class="math inline">\((K-1)\)</span>-vectors by fixing one of them to be
zero. The <a href="missing-data-and-partially-known-parameters.html#partially-known-parameters.section">partially known parameters
section</a> discusses how to mix
constants and parameters in a vector. In the multi-logit case, the
parameter block would be redefined to use <span class="math inline">\((K - 1)\)</span>-vectors</p>
<pre><code>parameters {
  matrix[K - 1, D] beta_raw;
}</code></pre>
<p>and then these are transformed to parameters to use in the model.
First, a transformed data block is added before the parameters block
to define a row vector of zero values,</p>
<pre><code>transformed data {
  row_vector[D] zeros = rep_row_vector(0, D);
}</code></pre>
<p>which can then be appended to <code>beta_row</code> to produce the
coefficient matrix <code>beta</code>,</p>
<pre><code>transformed parameters {
  matrix[K, D] beta;
  beta = append_row(beta_raw, zeros);
}</code></pre>
<p>The <code>rep_row_vector(0, D)</code> call creates a row vector of size <code>D</code> with
all entries set to zero. The derived matrix <code>beta</code> is then defined to
be the result of appending the row-vector <code>zeros</code> as a new row at the
end of <code>beta_raw</code>; the row vector <code>zeros</code> is defined as transformed
data so that it doesn’t need to be constructed from scratch each time
it is used.</p>
<p>This is not the same model as using <span class="math inline">\(K\)</span>-vectors as parameters,
because now the prior only applies to <span class="math inline">\((K-1)\)</span>-vectors. In practice,
this will cause the maximum likelihood solutions to be different and
also the posteriors to be slightly different when taking priors
centered around zero, as is typical for regression coefficients.</p>
</div>
</div>
<div id="parameterizing-centered-vectors" class="section level2">
<h2><span class="header-section-number">5.7</span> Parameterizing Centered Vectors</h2>
<p>It is often convenient to define a parameter vector <span class="math inline">\(\beta\)</span> that is
centered in the sense of satisfying the sum-to-zero constraint,</p>
<p><span class="math display">\[
\sum_{k=1}^K \beta_k = 0.
\]</span></p>
<p>Such a parameter vector may be used to identify a multi-logit
regression parameter vector (see the <a href="regression-models.html#multi-logit.section">multi-logit
section</a> for details), or may be used for
ability or difficulty parameters (but not both) in an IRT model (see
the <a href="item-response-models.section">item-response model section</a> for
details).</p>
<div id="k-1-degrees-of-freedom" class="section level3 unnumbered">
<h3><span class="math inline">\(K-1\)</span> Degrees of Freedom</h3>
<p>There is more than one way to enforce a sum-to-zero constraint on a
parameter vector, the most efficient of which is to define the <span class="math inline">\(K\)</span>-th
element as the negation of the sum of the elements <span class="math inline">\(1\)</span> through <span class="math inline">\(K-1\)</span>.</p>
<pre><code>parameters {
  vector[K-1] beta_raw;
  ...
transformed parameters {
  vector[K] beta = append_row(beta_raw, -sum(beta_raw));
  ...</code></pre>
<p>Placing a prior on <code>beta_raw</code> in this parameterization leads to
a subtly different posterior than that resulting from the same prior
on <code>beta</code> in the original parameterization without the
sum-to-zero constraint. Most notably, a simple prior on each
component of <code>beta_raw</code> produces different results than putting
the same prior on each component of an unconstrained <span class="math inline">\(K\)</span>-vector
<code>beta</code>. For example, providing a <span class="math inline">\(\mathsf{normal}(0,5)\)</span> prior
on <code>beta</code> will produce a different posterior mode than placing
the same prior on <code>beta_raw</code>.</p>
<div id="marginal-distribution-of-sum-to-zero-components" class="section level4 unnumbered">
<h4>Marginal distribution of sum-to-zero components</h4>
<p>On the Stan forums, Aaron Goodman provided the following code to
produce a prior with standard normal marginals on the components of
<code>beta</code>,</p>
<pre><code>model {
  beta ~ normal(0, inv(sqrt(1 - inv(K))));
  ...</code></pre>
<p>The components are not independent, as they must sum zero. No
Jacobian is required because summation and negation are linear
operations (and thus have constant Jacobians).</p>
<p>To generate distributions with marginals other than standard normal,
the resulting <code>beta</code> may be scaled by some factor <code>sigma</code> and
translated to some new location <code>mu</code>.</p>
</div>
</div>
<div id="qr-decomposition" class="section level3 unnumbered">
<h3>QR Decomposition</h3>
<p>Aaron Goodman, on the Stan forums, also provided this approach, which
calculates a QR decomposition in the transformed data block, then uses
it to transform to a sum-to-zero parameter <code>x</code>,</p>
<pre><code>transformed data{
  matrix[K, K] A = diag_matrix(rep_vector(1,K));
  matrix[K, K-1] A_qr;
  for (i in 1:K-1) A[K,i] = -1;
  A[K,K] = 0;
  A_qr = qr_Q(A)[ , 1:(K-1)];
}
parameters {
  vector[K-1] beta_raw;
}
transformed parameters{
   vector[K] beta =  A_qr * beta_raw;
}
model {
  beta_raw ~ normal(0, inv(sqrt(1 - inv(K))));
}</code></pre>
<p>This produces a marginal standard normal distribution on the values of
<code>beta</code>, which will sum to zero by construction of the QR decomposition.</p>
</div>
<div id="translated-and-scaled-simplex" class="section level3 unnumbered">
<h3>Translated and Scaled Simplex</h3>
<p>An alternative approach that’s less efficient, but amenable to a
symmetric prior, is to offset and scale a simplex.</p>
<pre><code>parameters {
  simplex[K] beta_raw;
  real beta_scale;
  ...
transformed parameters {
  vector[K] beta;
  beta = beta_scale * (beta_raw - inv(K));
  ...</code></pre>
<p>Here <code>inv(K)</code> is just a short way to write <code>1.0~/~K</code>. Given
that <code>beta_raw</code> sums to 1 because it is a simplex, the
elementwise subtraction of <code>inv(K)</code> is guaranteed to sum to zero.
Because the magnitude of the elements of the simplex is bounded, a
scaling factor is required to provide <code>beta</code> with <span class="math inline">\(K\)</span> degrees of
freedom necessary to take on every possible value that sums to zero.</p>
<p>With this parameterization, a Dirichlet prior can be placed on
<code>beta_raw</code>, perhaps uniform, and another prior put on
<code>beta_scale</code>, typically for “shrinkage.”</p>
</div>
<div id="soft-centering" class="section level3 unnumbered">
<h3>Soft Centering</h3>
<p>Adding a prior such as <span class="math inline">\(\beta \sim \mathsf{normal}(0,\sigma)\)</span> will provide a kind
of soft centering of a parameter vector <span class="math inline">\(\beta\)</span> by preferring, all
else being equal, that <span class="math inline">\(\sum_{k=1}^K \beta_k = 0\)</span>. This approach is only
guaranteed to roughly center if <span class="math inline">\(\beta\)</span> and the elementwise addition <span class="math inline">\(\beta + c\)</span>
for a scalar constant <span class="math inline">\(c\)</span> produce the same likelihood (perhaps by
another vector <span class="math inline">\(\alpha\)</span> being transformed to <span class="math inline">\(\alpha - c\)</span>, as in the
IRT models). This is another way of achieving a symmetric prior.</p>
</div>
</div>
<div id="ordered-logistic.section" class="section level2">
<h2><span class="header-section-number">5.8</span> Ordered Logistic and Probit Regression</h2>
<p>Ordered regression for an outcome <span class="math inline">\(y_n \in \{ 1,\ldots, k \}\)</span> with
predictors <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> is determined by a single coefficient
vector <span class="math inline">\(\beta \in \mathbb{R}^D\)</span> along with a sequence of cutpoints <span class="math inline">\(c \in \mathbb{R}^{K-1}\)</span> sorted so that <span class="math inline">\(c_d &lt; c_{d+1}\)</span>. The discrete output is
<span class="math inline">\(k\)</span> if the linear predictor <span class="math inline">\(x_n \beta\)</span> falls between <span class="math inline">\(c_{k-1}\)</span> and
<span class="math inline">\(c_k\)</span>, assuming <span class="math inline">\(c_0 = -\infty\)</span> and <span class="math inline">\(c_K = \infty\)</span>. The noise term is
fixed by the form of regression, with examples for ordered logistic
and ordered probit models.</p>
<div id="ordered-logistic-regression" class="section level3 unnumbered">
<h3>Ordered Logistic Regression</h3>
<p>The ordered logistic model can be coded in Stan using the
<code>ordered</code> data type for the cutpoints and the built-in
<code>ordered_logistic</code> distribution.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; D;
  int&lt;lower=1,upper=K&gt; y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  for (n in 1:N)
    y[n] ~ ordered_logistic(x[n] * beta, c);
}</code></pre>
<p>The vector of cutpoints <code>c</code> is declared as <code>ordered[K-1]</code>,
which guarantees that <code>c[k]</code> is less than <code>c[k+1]</code>.</p>
<p>If the cutpoints were assigned independent priors, the constraint
effectively truncates the joint prior to support over points that
satisfy the ordering constraint. Luckily, Stan does not need to
compute the effect of the constraint on the normalizing term because
the probability is needed only up to a proportion.</p>
<div id="ordered-probit" class="section level4 unnumbered">
<h4>Ordered Probit</h4>
<p>An ordered probit model could be coded in exactly the same way by
swapping the cumulative logistic (<code>inv_logit</code>) for the cumulative
normal (<code>Phi</code>).</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; D;
  int&lt;lower=1,upper=K&gt; y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  vector[K] theta;
  for (n in 1:N) {
    real eta;
    eta = x[n] * beta;
    theta[1] = 1 - Phi(eta - c[1]);
    for (k in 2:(K-1))
      theta[k] = Phi(eta - c[k-1]) - Phi(eta - c[k]);
    theta[K] = Phi(eta - c[K-1]);
    y[n] ~ categorical(theta);
  }
}</code></pre>
<p>The logistic model could also be coded this way by replacing
<code>Phi</code> with <code>inv_logit</code>, though the built-in encoding based
on the softmax transform is more efficient and more numerically
stable. A small efficiency gain could be achieved by computing the
values <code>Phi(eta - c[k])</code> once and storing them for re-use.</p>
</div>
</div>
</div>
<div id="hierarchical-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.9</span> Hierarchical Logistic Regression</h2>
<p>The simplest multilevel model is a hierarchical model in which the
data are grouped into <span class="math inline">\(L\)</span> distinct categories (or levels). An extreme
approach would be to completely pool all the data and estimate a
common vector of regression coefficients <span class="math inline">\(\beta\)</span>. At the other
extreme, an approach with no pooling assigns each level <span class="math inline">\(l\)</span> its own
coefficient vector <span class="math inline">\(\beta_l\)</span> that is estimated separately from the
other levels. A hierarchical model is an intermediate solution where
the degree of pooling is determined by the data and a prior on the
amount of pooling.</p>
<p>Suppose each binary outcome <span class="math inline">\(y_n \in \{ 0, 1 \}\)</span> has an associated
level, <span class="math inline">\(ll_n \in \{ 1,\ldots,L \}\)</span>. Each outcome will also have
an associated predictor vector <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>. Each level <span class="math inline">\(l\)</span>
gets its own coefficient vector <span class="math inline">\(\beta_l \in \mathbb{R}^D\)</span>. The
hierarchical structure involves drawing the coefficients <span class="math inline">\(\beta_{l,d} \in \mathbb{R}\)</span> from a prior that is also estimated with the data. This
hierarchically estimated prior determines the amount of pooling. If
the data in each level are similar, strong pooling will be
reflected in low hierarchical variance. If the data in the levels are
dissimilar, weaker pooling will be reflected in higher hierarchical variance.</p>
<p>The following model encodes a hierarchical logistic regression model
with a hierarchical prior on the regression coefficients.</p>
<pre><code>data {
  int&lt;lower=1&gt; D;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; L;
  int&lt;lower=0,upper=1&gt; y[N];
  int&lt;lower=1,upper=L&gt; ll[N];
  row_vector[D] x[N];
}
parameters {
  real mu[D];
  real&lt;lower=0&gt; sigma[D];
  vector[D] beta[L];
}
model {
  for (d in 1:D) {
    mu[d] ~ normal(0, 100);
    for (l in 1:L)
      beta[l,d] ~ normal(mu[d], sigma[d]);
  }
  for (n in 1:N)
    y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}</code></pre>
<p>The standard deviation parameter <code>sigma</code> gets an implicit uniform
prior on <span class="math inline">\((0,\infty)\)</span> because of its declaration with a lower-bound
constraint of zero. Stan allows improper priors as long as the
posterior is proper. Nevertheless, it is usually helpful to have
informative or at least weakly informative priors for all parameters;
see the <a href="regression-models.html#regression-priors.section">regression priors section</a> for
recommendations on priors for regression coefficients and scales.</p>
<div id="optimizing-the-model" class="section level4 unnumbered">
<h4>Optimizing the Model</h4>
<p>Where possible, vectorizing sampling statements leads to faster log
probability and derivative evaluations. The speed boost is not
because loops are eliminated, but because vectorization allows sharing
subcomputations in the log probability and gradient calculations and
because it reduces the size of the expression tree required for
gradient calculations.</p>
<p>The first optimization vectorizes the for-loop over <code>D</code> as</p>
<pre><code>  mu ~ normal(0, 100);
  for (l in 1:L)
    beta[l] ~ normal(mu, sigma);</code></pre>
<p>The declaration of <code>beta</code> as an array of vectors means that the
expression <code>beta[l]</code> denotes a vector. Although <code>beta</code> could have
been declared as a matrix, an array of vectors (or a two-dimensional
array) is more efficient for accessing rows; see the <a href="matrices-vectors-and-arrays.html#indexing-efficiency.section">indexing
efficiency section</a> for more information
on the efficiency tradeoffs among arrays, vectors, and matrices.</p>
<p>This model can be further sped up and at the same time made more
arithmetically stable by replacing the application of inverse-logit
inside the Bernoulli distribution with the logit-parameterized
Bernoulli,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<pre><code>  for (n in 1:N)
    y[n] ~ bernoulli_logit(x[n] * beta[ll[n]]);</code></pre>
<p>Unlike in R or BUGS, loops, array access and assignments are fast in
Stan because they are translated directly to C++. In most cases, the
cost of allocating and assigning to a container is more than made up
for by the increased efficiency due to vectorizing the log probability
and gradient calculations. Thus the following version is faster than
the original formulation as a loop over a sampling statement.</p>
<pre><code>  {
    vector[N] x_beta_ll;
    for (n in 1:N)
      x_beta_ll[n] = x[n] * beta[ll[n]];
    y ~ bernoulli_logit(x_beta_ll);
  }</code></pre>
<p>The brackets introduce a new scope for the local variable
<code>x_beta_ll</code>; alternatively, the variable may be declared at the
top of the model block.</p>
<p>In some cases, such as the above, the local variable assignment leads
to models that are less readable. The recommended practice in such
cases is to first develop and debug the more transparent version of
the model and only work on optimizations when the simpler formulation
has been debugged.</p>
</div>
</div>
<div id="hierarchical-priors.section" class="section level2">
<h2><span class="header-section-number">5.10</span> Hierarchical Priors</h2>
<p>Priors on priors, also known as “hyperpriors,” should be treated the
same way as priors on lower-level parameters in that as much prior
information as is available should be brought to bear. Because
hyperpriors often apply to only a handful of lower-level parameters,
care must be taken to ensure the posterior is both proper and not
overly sensitive either statistically or computationally to wide tails
in the priors.</p>
<div id="bound-avoid-priors.subsection" class="section level3">
<h3><span class="header-section-number">5.10.1</span> Boundary-Avoiding Priors for MLE in Hierarchical Models {-}</h3>
<p>The fundamental problem with maximum likelihood estimation (MLE) in
the hierarchical model setting is that as the hierarchical variance
drops and the values cluster around the hierarchical mean, the overall
density grows without bound. As an illustration, consider a simple
hierarchical linear regression (with fixed prior mean) of <span class="math inline">\(y_n \in \mathbb{R}\)</span> on <span class="math inline">\(x_n \in \mathbb{R}^K\)</span>, formulated as</p>
<p><span class="math display">\[
\begin{array}{rcl}
y_n &amp; \sim &amp; \mathsf{normal}(x_n \beta, \sigma)
\\[3pt]
\beta_k &amp; \sim &amp; \mathsf{normal}(0,\tau)
\\[3pt]
\tau &amp; \sim &amp; \mathsf{Cauchy}(0,2.5)
\end{array}
\]</span></p>
<p>In this case, as <span class="math inline">\(\tau \rightarrow 0\)</span> and <span class="math inline">\(\beta_k \rightarrow 0\)</span>, the
posterior density <span class="math display">\[ p(\beta,\tau,\sigma|y,x) \propto
p(y|x,\beta,\tau,\sigma) \]</span> grows without bound. See the <a href="#funnel.figure">plot of
Neal’s funnel density</a>, which has similar behavior.</p>
<p>There is obviously no MLE estimate for <span class="math inline">\(\beta,\tau,\sigma\)</span> in such a
case, and therefore the model must be modified if posterior modes are
to be used for inference. The approach recommended by
<span class="citation">Chung et al. (<a href="#ref-ChungEtAl:2013">2013</a>)</span> is to use a gamma distribution as a prior, such
as</p>
<p><span class="math display">\[
\sigma \sim \mathsf{Gamma}(2, 1/A),
\]</span></p>
<p>for a reasonably large value of <span class="math inline">\(A\)</span>, such as <span class="math inline">\(A = 10\)</span>.</p>
</div>
</div>
<div id="item-response-models.section" class="section level2">
<h2><span class="header-section-number">5.11</span> Item-Response Theory Models</h2>
<p>Item-response theory (IRT) models the situation in which a number of
students each answer one or more of a group of test questions. The
model is based on parameters for the ability of the students, the
difficulty of the questions, and in more articulated models, the
discriminativeness of the questions and the probability of guessing
correctly; <span class="citation">(Gelman and Hill <a href="#ref-GelmanHill:2007">2007</a>, pps. 314–320)</span> for a textbook
introduction to hierarchical IRT models and <span class="citation">Curtis (<a href="#ref-Curtis:2010">2010</a>)</span> for
encodings of a range of IRT models in BUGS.</p>
<div id="data-declaration-with-missingness" class="section level3 unnumbered">
<h3>Data Declaration with Missingness</h3>
<p>The data provided for an IRT model may be declared as follows
to account for the fact that not every student is required to answer
every question.</p>
<pre><code>data {
  int&lt;lower=1&gt; J;              // number of students
  int&lt;lower=1&gt; K;              // number of questions
  int&lt;lower=1&gt; N;              // number of observations
  int&lt;lower=1,upper=J&gt; jj[N];  // student for observation n
  int&lt;lower=1,upper=K&gt; kk[N];  // question for observation n
  int&lt;lower=0,upper=1&gt; y[N];   // correctness for observation n
}</code></pre>
<p>This declares a total of <code>N</code> student-question pairs in the data
set, where each <code>n</code> in <code>1:N</code> indexes a binary observation
<code>y[n]</code> of the correctness of the answer of student <code>jj[n]</code>
on question <code>kk[n]</code>.</p>
<p>The prior hyperparameters will be hard coded in the rest of this
section for simplicity, though they could be coded as data in
Stan for more flexibility.</p>
</div>
<div id="pl-rasch-model" class="section level3 unnumbered">
<h3>1PL (Rasch) Model</h3>
<p>The 1PL item-response model, also known as the Rasch model, has one
parameter (1P) for questions and uses the logistic link function (L).</p>
<p>The model parameters are declared as follows.</p>
<pre><code>parameters {
  real delta;         // mean student ability
  real alpha[J];      // ability of student j - mean ability
  real beta[K];       // difficulty of question k
}</code></pre>
<p>The parameter <code>alpha[j]</code> is the ability coefficient for student
<code>j</code> and <code>beta[k]</code> is the difficulty coefficient for question
<code>k</code>. The non-standard parameterization used here also includes
an intercept term <code>delta</code>, which represents the average student’s
response to the average question.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The model itself is as follows.</p>
<pre><code>model {
  alpha ~ std_normal();         // informative true prior
  beta ~ std_normal();          // informative true prior
  delta ~ normal(0.75, 1);      // informative true prior
  for (n in 1:N)
    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);
}</code></pre>
<p>This model uses the logit-parameterized Bernoulli distribution, where</p>
<p><span class="math display">\[
\mathsf{BernoulliLogit}(y \mid \alpha)
\ = \
\mathsf{Bernoulli}(y \mid \mbox{logit}^{-1}(\alpha)).
\]</span></p>
<p>The key to understanding it is the term inside the
<code>bernoulli_logit</code> distribution, from which it follows that
<span class="math display">\[
\mbox{Pr}[y_n = 1] = \mbox{logit}^{-1}(\alpha_{jj[n]} - \beta_{kk[n]}
+ \delta).
\]</span></p>
<p>The model suffers from additive identifiability issues without the
priors. For example, adding a term <span class="math inline">\(\xi\)</span> to each <span class="math inline">\(\alpha_j\)</span> and
<span class="math inline">\(\beta_k\)</span> results in the same predictions. The use of priors for
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> located at 0 identifies the parameters; see
<span class="citation">Gelman and Hill (<a href="#ref-GelmanHill:2007">2007</a>)</span> for a discussion of identifiability issues and
alternative approaches to identification.</p>
<p>For testing purposes, the IRT 1PL model distributed with Stan uses
informative priors that match the actual data generation process used
to simulate the data in R (the simulation code is supplied in the same
directory as the models). This is unrealistic for most practical
applications, but allows Stan’s inferences to be validated. A simple
sensitivity analysis with fatter priors shows that the posterior is
fairly sensitive to the prior even with 400 students and 100 questions
and only 25% missingness at random. For real applications, the
priors should be fit hierarchically along with the other parameters,
as described in the next section.</p>
</div>
<div id="multilevel-2pl-model" class="section level3 unnumbered">
<h3>Multilevel 2PL Model</h3>
<p>The simple 1PL model described in the previous section is generalized
in this section with the addition of a discrimination parameter to
model how noisy a question is and by adding multilevel priors for the
question difficulty and discrimination parameters. The model
parameters are declared as follows.</p>
<pre><code>parameters {
  real mu_beta;                // mean question difficulty
  vector[J] alpha;             // ability for j - mean
  vector[K] beta;              // difficulty for k
  vector&lt;lower=0&gt;[K] gamma;    // discrimination of k
  real&lt;lower=0&gt; sigma_beta;    // scale of difficulties
  real&lt;lower=0&gt; sigma_gamma;   // scale of log discrimination
}</code></pre>
<p>The parameters should be clearer after the model definition.</p>
<pre><code>model {
  alpha ~ std_normal();
  beta ~ normal(0, sigma_beta);
  gamma ~ lognormal(0, sigma_gamma);
  mu_beta ~ cauchy(0, 5);
  sigma_beta ~ cauchy(0, 5);
  sigma_gamma ~ cauchy(0, 5);
  y ~ bernoulli_logit(gamma[kk] .* (alpha[jj] - (beta[kk] + mu_beta)));
}</code></pre>
<p>The <code>std_normal</code> function is used here, defined by</p>
<p><span class="math display">\[
\mathsf{Stdnormalal}(y)
\ = \
\mathsf{normal}(y \mid 0, 1).
\]</span></p>
<p>The sampling statement is also vectorized using elementwise
multiplication; it is equivalent to</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli_logit(gamma[kk[n]]
                         * (alpha[jj[n]] - (beta[kk[n]] + mu_beta));</code></pre>
<p>The 2PL model is similar to the 1PL model, with the additional parameter
<code>gamma[k]</code> modeling how discriminative question <code>k</code> is. If
<code>gamma[k]</code> is greater than 1, responses are more attenuated with
less chance of getting a question right at random. The parameter
<code>gamma[k]</code> is constrained to be positive, which prohibits there
being questions that are easier for students of lesser ability; such
questions are not unheard of, but they tend to be eliminated from most
testing situations where an IRT model would be applied.</p>
<p>The model is parameterized here with student abilities <code>alpha</code> being
given a standard normal prior. This is to identify both the scale and
the location of the parameters, both of which would be unidentified
otherwise; see the <a href="problematic-posteriors-chapter.html#problematic-posteriors.chapter">problematic posteriors
chapter</a> for further discussion of
identifiability. The difficulty and discrimination parameters <code>beta</code>
and <code>gamma</code> then have varying scales given hierarchically in this
model. They could also be given weakly informative non-hierarchical
priors, such as</p>
<pre><code>  beta ~ normal(0, 5);
  gamma ~ lognormal(0, 2);</code></pre>
<p>The point is that the <code>alpha</code> determines the scale and location
and <code>beta</code> and <code>gamma</code> are allowed to float.</p>
<p>The <code>beta</code> parameter is here given a non-centered
parameterization, with parameter <code>mu_beta</code> serving as the mean
<code>beta</code> location. An alternative would’ve been to take:</p>
<pre><code>  beta ~ normal(mu_beta, sigma_beta);</code></pre>
<p>and</p>
<pre><code>  y[n] ~ bernoulli_logit(gamma[kk[n]] * (alpha[jj[n]] - beta[kk[n]]));</code></pre>
<p>Non-centered parameterizations tend to be more efficient in
hierarchical models; see the <a href="optimization-chapter.html#reparameterization.section">reparameterization
section</a> for more information on
non-centered reparameterizations.</p>
<p>The intercept term <code>mu_beta</code> can’t itself be modeled
hierarchically, so it is given a weakly informative
<span class="math inline">\(\mathsf{Cauchy}(0,5)\)</span> prior. Similarly, the scale terms,
<code>sigma_beta</code>, and <code>sigma_gamma</code>, are given half-Cauchy
priors. As mentioned earlier, the scale and location for <code>alpha</code>
are fixed to ensure identifiability. The truncation in the
half-Cauchy prior is implicit; explicit truncation is not necessary
because the log probability need only be calculated up to a proportion
and the scale variables are constrained to <span class="math inline">\((0,\infty)\)</span> by their
declarations.</p>
</div>
</div>
<div id="priors-for-identification.section" class="section level2">
<h2><span class="header-section-number">5.12</span> Priors for Identifiability</h2>
<div id="location-and-scale-invariance" class="section level3 unnumbered">
<h3>Location and Scale Invariance</h3>
<p>One application of (hierarchical) priors is to identify the scale
and/or location of a group of parameters. For example, in the IRT
models discussed in the previous section, there is both a location and
scale non-identifiability. With uniform priors, the posteriors will
float in terms of both scale and location. See the <a href="problematic-posteriors-chapter.html#collinearity.section">collinearity
section</a> for a simple example of the problems
this poses for estimation.</p>
<p>The non-identifiability is resolved by providing a standard normal (i.e.,
<span class="math inline">\(\mathsf{normal}(0,1)\)</span>) prior on one group of coefficients, such as
the student abilities. With a standard normal prior on the student
abilities, the IRT model is identified in that the posterior will
produce a group of estimates for student ability parameters that have
a sample mean of close to zero and a sample variance of close to one.
The difficulty and discrimination parameters for the questions should
then be given a diffuse, or ideally a hierarchical prior, which will
identify these parameters by scaling and locating relative to the
student ability parameters.</p>
</div>
<div id="collinearity" class="section level3 unnumbered">
<h3>Collinearity</h3>
<p>Another case in which priors can help provide identifiability is in
the case of collinearity in a linear regression. In linear
regression, if two predictors are collinear (i.e, one is a linear
function of the other), then their coefficients will have a
correlation of 1 (or -1) in the posterior. This leads to
non-identifiability. By placing normal priors on the coefficients,
the maximum likelihood solution of two duplicated predictors (trivially
collinear) will be half the value than would be obtained by only
including one.</p>
</div>
<div id="separability" class="section level3 unnumbered">
<h3>Separability</h3>
<p>In a logistic regression, if a predictor is positive in cases of 1
outcomes and negative in cases of 0 outcomes, then the maximum
likelihood estimate for the coefficient for that predictor diverges to
infinity. This divergence can be controlled by providing a prior for
the coefficient, which will “shrink” the estimate back toward zero
and thus identify the model in the posterior.</p>
<p>Similar problems arise for sampling with improper flat priors. The
sampler will try to draw large values. By providing a prior,
the posterior will be concentrated around finite values, leading to
well-behaved sampling.</p>
</div>
</div>
<div id="multivariate-hierarchical-priors.section" class="section level2">
<h2><span class="header-section-number">5.13</span> Multivariate Priors for Hierarchical Models</h2>
<p>In hierarchical regression models (and other situations), several
individual-level variables may be assigned hierarchical priors. For
example, a model with multiple varying intercepts and slopes within
might assign them a multivariate prior.</p>
<p>As an example, the individuals might be people and the outcome income,
with predictors such as education level and age, and the groups might be states
or other geographic divisions. The effect of education level and age
as well as an intercept might be allowed to vary by state.
Furthermore, there might be state-level predictors, such as average
state income and unemployment level.</p>
<div id="multivariate-regression-example" class="section level3 unnumbered">
<h3>Multivariate Regression Example</h3>
<p><span class="citation">(Gelman and Hill <a href="#ref-GelmanHill:2007">2007</a>, Chapter 13, Chapter 17)</span> includes a discussion of a
hierarchical model with <span class="math inline">\(N\)</span> individuals organized into <span class="math inline">\(J\)</span> groups.
Each individual has a predictor row vector <span class="math inline">\(x_n\)</span> of size <span class="math inline">\(K\)</span>; to unify
the notation, they assume that <span class="math inline">\(x_{n,1} = 1\)</span> is a fixed “intercept”
predictor. To encode group membership, they assume individual <span class="math inline">\(n\)</span>
belongs to group <span class="math inline">\(jj[n] \in 1{:}J\)</span>. Each individual <span class="math inline">\(n\)</span> also has an
observed outcome <span class="math inline">\(y_n\)</span> taking on real values.</p>
<div id="likelihood" class="section level4 unnumbered">
<h4>Likelihood</h4>
<p>The model is a linear regression with slope and intercept coefficients
varying by group, so that <span class="math inline">\(\beta_j\)</span> is the coefficient <span class="math inline">\(K\)</span>-vector for
group <span class="math inline">\(j\)</span>. The likelihood function for individual <span class="math inline">\(n\)</span> is then just</p>
<p><span class="math display">\[
y_n \sim \mathsf{normal}(x_n \, \beta_{jj[n]}, \, \sigma)
\mbox{ for } n \in 1{:}N.
\]</span></p>
</div>
<div id="coefficient-prior" class="section level4 unnumbered">
<h4>Coefficient Prior</h4>
<p>Gelman and Hill model the coefficient vectors <span class="math inline">\(\beta_j\)</span> as being drawn
from a multivariate distribution with mean vector <span class="math inline">\(\mu\)</span> and
covariance matrix <span class="math inline">\(\Sigma\)</span>,</p>
<p><span class="math display">\[
\beta_j \sim \mathsf{multivariate\ normal}(\mu, \, \Sigma)
\mbox{ for } j \in 1{:}J.
\]</span></p>
<p>Below, we discuss the full model of Gelman and Hill, which uses
group-level predictors to model <span class="math inline">\(\mu\)</span>; for now, we assume <span class="math inline">\(\mu\)</span> is a
simple vector parameter.</p>
</div>
<div id="hyperpriors" class="section level4 unnumbered">
<h4>Hyperpriors</h4>
<p>For hierarchical modeling, the group-level mean vector <span class="math inline">\(\mu\)</span> and
covariance matrix <span class="math inline">\(\Sigma\)</span> must themselves be given priors. The
group-level mean vector can be given a reasonable weakly-informative
prior for independent coefficients, such as</p>
<p><span class="math display">\[
\mu_j \sim \mathsf{normal}(0,5).
\]</span>
If more is known about the expected coefficient values
<span class="math inline">\(\beta_{j, k}\)</span>, this information can be incorporated into the prior for
<span class="math inline">\(\mu_k\)</span>.</p>
<p>For the prior on the covariance matrix, Gelman and Hill suggest using
a scaled inverse Wishart. That choice was motivated primarily by
convenience as it is conjugate to the multivariate likelihood function
and thus simplifies Gibbs sampling</p>
<p>In Stan, there is no restriction to conjugacy for multivariate priors,
and we in fact recommend a slightly different approach. Like Gelman
and Hill, we decompose our prior into a scale and a matrix, but are
able to do so in a more natural way based on the actual variable
scales and a correlation matrix. Specifically, we define
<span class="math display">\[
\Sigma = \mbox{diag\_matrix}(\tau) \ \Omega \ \mbox{diag\_matrix}(\tau),
\]</span>
where <span class="math inline">\(\Omega\)</span> is a correlation matrix and <span class="math inline">\(\tau\)</span> is the vector of
coefficient scales. This mapping from scale vector <span class="math inline">\(\tau\)</span> and
correlation matrix <span class="math inline">\(\Omega\)</span> can be inverted, using
<span class="math display">\[
\tau_k = \sqrt{\, \Sigma_{k,k}}
\]</span>
and
<span class="math display">\[
\Omega_{i, j} = \frac{\Sigma_{i, j}}{\tau_i \, \tau_j}.
\]</span></p>
<p>The components of the scale vector <span class="math inline">\(\tau\)</span> can be given any reasonable
prior for scales, but we recommend something weakly informative like a
half-Cauchy distribution with a small scale, such as
<span class="math display">\[
\tau_k \sim \mathsf{Cauchy}(0, 2.5)
\mbox{ for } k \in 1{:}K \mbox{ constrained by } \tau_k &gt; 0.
\]</span>
As for the prior means, if there is information about the scale of
variation of coefficients across groups, it should be incorporated
into the prior for <span class="math inline">\(\tau\)</span>. For large numbers of exchangeable
coefficients, the components of <span class="math inline">\(\tau\)</span> itself (perhaps excluding the
intercept) may themselves be given a hierarchical prior.</p>
<p>Our final recommendation is to give the correlation matrix <span class="math inline">\(\Omega\)</span> an
LKJ prior with shape <span class="math inline">\(\eta \geq 1\)</span>,<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p><span class="math display">\[
\Omega \sim \mathsf{LKJCorr}(\eta).
\]</span></p>
<p>The LKJ correlation distribution is defined by</p>
<p><span class="math display">\[
\mathsf{LkjCorr}(\Sigma \mid \eta)
\ \propto \
\mathsf{det}(\Sigma)^{\eta - 1}.
\]</span></p>
<p>The basic behavior of the LKJ correlation distribution is similar to
that of a beta distribution. For <span class="math inline">\(\eta = 1\)</span>, the result is a uniform
distribution. Despite being the identity over correlation matrices, the
marginal distribution over the entries in that matrix (i.e., the
correlations) is not uniform between -1 and 1. Rather, it
concentrates around zero as the dimensionality increases due to the
complex constraints.</p>
<p>For <span class="math inline">\(\eta &gt; 1\)</span>, the density increasingly concentrates
mass around the unit matrix, i.e., favoring less correlation. For
<span class="math inline">\(\eta &lt; 1\)</span>, it increasingly concentrates mass in the other direction,
i.e., favoring more correlation.</p>
<p>The LKJ prior may thus be used to control the expected amount of
correlation among the parameters <span class="math inline">\(\beta_j\)</span>. For a discussion of
decomposing a covariance prior into a prior on correlation matrices
and an independent prior on scales, see <span class="citation">Barnard, McCulloch, and Meng (<a href="#ref-barnard-mcculloch-meng:2000">2000</a>)</span>.</p>
</div>
<div id="group-level-predictors-for-prior-mean" class="section level4 unnumbered">
<h4>Group-Level Predictors for Prior Mean</h4>
<p>To complete Gelman and Hill’s model, suppose each group <span class="math inline">\(j \in 1{:}J\)</span>
is supplied with an <span class="math inline">\(L\)</span>-dimensional row-vector of group-level
predictors <span class="math inline">\(u_j\)</span>. The prior mean for the <span class="math inline">\(\beta_j\)</span> can then itself be
modeled as a regression, using an <span class="math inline">\(L\)</span>-dimensional coefficient vector
<span class="math inline">\(\gamma\)</span>. The prior for the group-level coefficients then becomes
<span class="math display">\[
\beta_j \sim \mathsf{multivariate\ normal}(u_j \, \gamma, \Sigma)
\]</span></p>
<p>The group-level coefficients <span class="math inline">\(\gamma\)</span> may themselves be given
independent weakly informative priors, such as
<span class="math display">\[
\gamma_l \sim \mathsf{normal}(0,5).
\]</span>
As usual, information about the group-level means should be
incorporated into this prior.</p>
</div>
<div id="coding-the-model-in-stan" class="section level4 unnumbered">
<h4>Coding the Model in Stan</h4>
<p>The Stan code for the full hierarchical model with multivariate priors
on the group-level coefficients and group-level prior means follows
its definition.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;              // num individuals
  int&lt;lower=1&gt; K;              // num ind predictors
  int&lt;lower=1&gt; J;              // num groups
  int&lt;lower=1&gt; L;              // num group predictors
  int&lt;lower=1,upper=J&gt; jj[N];  // group for individual
  matrix[N, K] x;               // individual predictors
  row_vector[L] u[J];          // group predictors
  vector[N] y;                 // outcomes
}
parameters {
  corr_matrix[K] Omega;        // prior correlation
  vector&lt;lower=0&gt;[K] tau;      // prior scale
  matrix[L, K] gamma;           // group coeffs
  vector[K] beta[J];           // indiv coeffs by group
  real&lt;lower=0&gt; sigma;         // prediction error scale
}
model {
  tau ~ cauchy(0, 2.5);
  Omega ~ lkj_corr(2);
  to_vector(gamma) ~ normal(0, 5);
  {
    row_vector[K] u_gamma[J];
    for (j in 1:J)
      u_gamma[j] = u[j] * gamma;
    beta ~ multi_normal(u_gamma, quad_form_diag(Omega, tau));
  }
  for (n in 1:N)
    y[n] ~ normal(x[n] * beta[jj[n]], sigma);
}</code></pre>
<p>The hyperprior covariance matrix is defined implicitly through the a
quadratic form in the code because the correlation matrix <code>Omega</code> and
scale vector <code>tau</code> are more natural to inspect in the output; to
output <code>Sigma</code>, define it as a transformed parameter. The function
<code>quad_form_diag</code> is defined so that <code>quad_form_diag(Sigma, tau)</code> is
equivalent to <code>diag_matrix(tau) * Sigma * diag_matrix(tau)</code>, where
<code>diag_matrix(tau)</code> returns the matrix with <code>tau</code> on the diagonal and
zeroes off diagonal; the version using <code>quad_form_diag</code> should be
faster. For details on these and other matrix arithmtic operators and
functions, see the function reference manual.</p>
</div>
<div id="optimization-through-vectorization" class="section level4 unnumbered">
<h4>Optimization through Vectorization</h4>
<p>The code in the Stan program above can be sped up dramatically by replacing:</p>
<pre><code>  for (n in 1:N)
    y[n] ~ normal(x[n] * beta[jj[n]], sigma);</code></pre>
<p>with the vectorized form:</p>
<pre><code>  {
    vector[N] x_beta_jj;
    for (n in 1:N)
      x_beta_jj[n] = x[n] * beta[jj[n]];
    y ~ normal(x_beta_jj, sigma);
  }</code></pre>
<p>The outer brackets create a local scope in which to define the
variable <code>x_beta_jj</code>, which is then filled in a loop and used
to define a vectorized sampling statement. The reason this is such a
big win is that it allows us to take the log of sigma only once and it
greatly reduces the size of the resulting expression graph by packing
all of the work into a single density function.</p>
<p>Although it is tempting to redeclare <code>beta</code> and include a revised
model block sampling statement,</p>
<pre><code>parameters {
  matrix[J, K] beta;
...
model {
  y ~ normal(rows_dot_product(x, beta[jj]), sigma);
  ...</code></pre>
<p>this fails because it breaks the vectorization of sampling for
<code>beta</code>,<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<pre><code>  beta ~ multi_normal(...);</code></pre>
<p>which requires <code>beta</code> to be an array of vectors. Both
vectorizations are important, so the best solution is to just use the
loop above, because <code>rows_dot_product</code> cannot do much
optimization in and of itself because there are no shared computations.</p>
<p>The code in the Stan program above also builds up an array of vectors
for the outcomes and for the multivariate normal, which provides a
major speedup by reducing the number of linear systems that
need to be solved and differentiated.</p>
<pre><code>  {
    matrix[K, K] Sigma_beta;
    Sigma_beta = quad_form_diag(Omega, tau);
    for (j in 1:J)
      beta[j] ~ multi_normal((u[j] * gamma)&#39;, Sigma_beta);
  }</code></pre>
<p>In this example, the covariance matrix <code>Sigma_beta</code> is defined
as a local variable so as not to have to repeat the quadratic form
computation <span class="math inline">\(J\)</span> times. This vectorization can be combined with the
Cholesky-factor optimization in the next section.</p>
</div>
<div id="optimization-through-cholesky-factorization" class="section level4 unnumbered">
<h4>Optimization through Cholesky Factorization</h4>
<p>The multivariate normal density and LKJ prior on correlation matrices
both require their matrix parameters to be factored. Vectorizing, as
in the previous section, ensures this is only done once for each
density. An even better solution, both in terms of efficiency and
numerical stability, is to parameterize the model directly in terms of
Cholesky factors of correlation matrices using the multivariate
version of the non-centered parameterization. For the model in the
previous section, the program fragment to replace the full matrix
prior with an equivalent Cholesky factorized prior is as follows.</p>
<pre><code>data {
  matrix[J, L] u;
  ...
parameters {
  matrix[K, J] z;
  cholesky_factor_corr[K] L_Omega;
  ...
transformed parameters {
  matrix[J, K] beta;
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)&#39;;
}
model {
  to_vector(z) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2);
  ...</code></pre>
<p>The data variable <code>u</code> was originally an array of vectors, which
is efficient for access; here it is redeclared as a matrix in order to
use it in matrix arithmetic. The new parameter <code>L_Omega</code> is
the Cholesky factor of the original correlation matrix <code>Omega</code>,
so that</p>
<pre><code>Omega = L_Omega * L_Omega&#39;</code></pre>
<p>The prior scale vector <code>tau</code> is unchanged, and furthermore,
Pre-multiplying the Cholesky factor by the scale produces the Cholesky
factor of the final covariance matrix,</p>
<pre><code>  Sigma_beta
  = quad_form_diag(Omega, tau)
  = diag_pre_multiply(tau, L_Omega) * diag_pre_multiply(tau, L_Omega)&#39;</code></pre>
<p>where the diagonal pre-multiply compound operation is defined by</p>
<pre><code>diag_pre_multiply(a, b) = diag_matrix(a) * b</code></pre>
<p>The new variable <code>z</code> is declared as a matrix, the entries of
which are given independent standard normal priors; the <code>to_vector</code>
operation turns the matrix into a vector so that it can be used as a
vectorized argument to the univariate normal density. Multiplying the
Cholesky factor of the covariance matrix by <code>z</code> and adding the
mean <code>(u\,*\,gamma)'</code> produces a <code>beta</code> distributed as in
the original model.</p>
<p>Omitting the data declarations, which are the same as before, the
optimized model is as follows.</p>
<pre><code>parameters {
  matrix[K, J] z;
  cholesky_factor_corr[K] L_Omega;
  vector&lt;lower=0,upper=pi()/2&gt;[K] tau_unif;
  matrix[L, K] gamma;                         // group coeffs
  real&lt;lower=0&gt; sigma;                       // prediction error scale
}
transformed parameters {
  matrix[J, K] beta;
  vector&lt;lower=0&gt;[K] tau;     // prior scale
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]);
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)&#39;;
}
model {
  to_vector(z) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2);
  to_vector(gamma) ~ normal(0, 5);
  y ~ normal(rows_dot_product(beta[jj] , x), sigma);
}</code></pre>
<p>This model also reparameterizes the prior scale <code>tau</code> to avoid potential problems with the heavy tails of the Cauchy distribution. The statement <code>tau_unif ~ uniform(0,pi()/2)</code> can be omitted from the model block because stan increments the log posterior for parameters with uniform priors without it.</p>
</div>
</div>
</div>
<div id="prediction-forecasting-and-backcasting" class="section level2">
<h2><span class="header-section-number">5.14</span> Prediction, Forecasting, and Backcasting</h2>
<p>Stan models can be used for “predicting” the values of arbitrary
model unknowns. When predictions are about the future, they’re called
``forecasts;&quot; when they are predictions about the past, as in climate
reconstruction or cosmology, they are sometimes called “backcasts”
(or “aftcasts” or “hindcasts” or “antecasts,” depending on the
author’s feelings about the opposite of “fore”).</p>
<div id="programming-predictions" class="section level3 unnumbered">
<h3>Programming Predictions</h3>
<p>As a simple example, the following linear regression provides the same
setup for estimating the coefficients <code>beta</code> as in our first
example above, using <code>y</code> for the <code>N</code> observations and
<code>x</code> for the <code>N</code> predictor vectors. The model parameters and
model for observations are exactly the same as before.</p>
<p>To make predictions, we need to be given the number of predictions,
<code>N_new</code>, and their predictor matrix, <code>x_new</code>. The
predictions themselves are modeled as a parameter <code>y_new</code>. The
model statement for the predictions is exactly the same as for the
observations, with the new outcome vector <code>y_new</code> and prediction
matrix <code>x_new</code>.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=0&gt; N;
  matrix[N, K] x;
  vector[N] y;

  int&lt;lower=0&gt; N_new;
  matrix[N_new, K] x_new;
}
parameters {
  vector[K] beta;
  real&lt;lower=0&gt; sigma;

  vector[N_new] y_new;                  // predictions
}
model {
  y ~ normal(x * beta, sigma);          // observed model

  y_new ~ normal(x_new * beta, sigma);  // prediction model
}</code></pre>
</div>
<div id="predictions-as-generated-quantities" class="section level3 unnumbered">
<h3>Predictions as Generated Quantities</h3>
<p>Where possible, the most efficient way to generate predictions is to
use the generated quantities block. This provides proper Monte Carlo
(not Markov chain Monte Carlo) inference, which can have a much higher
effective sample size per iteration.</p>
<pre><code>...data as above...

parameters {
  vector[K] beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(x * beta, sigma);
}
generated quantities {
  vector[N_new] y_new;
  for (n in 1:N_new)
    y_new[n] = normal_rng(x_new[n] * beta, sigma);
}</code></pre>
<p>Now the data are just as before, but the parameter <code>y_new</code> is now
declared as a generated quantity, and the prediction model is
removed from the model and replaced by a pseudo-random draw from a
normal distribution.</p>
<div id="overflow-in-generated-quantities" class="section level4 unnumbered">
<h4>Overflow in Generated Quantities</h4>
<p>It is possible for values to overflow or underflow in generated
quantities. The problem is that if the result is NaN, then any
constraints placed on the variables will be violated. It is possible
to check a value assigned by an RNG and reject it if it overflows, but
this is both inefficient and leads to biased posterior estimates.
Instead, the conditions causing overflow, such as trying to generate a
negative binomial random variate with a mean of <span class="math inline">\(2^{31}\)</span>. These must
be intercepted and dealt with, typically be reparameterizing or
reimplementing the random number generator using real values rather
than integers, which are upper-bounded by <span class="math inline">\(2^{31} - 1\)</span> in Stan.</p>
</div>
</div>
</div>
<div id="multivariate-outcomes" class="section level2">
<h2><span class="header-section-number">5.15</span> Multivariate Outcomes</h2>
<p>Most regressions are set up to model univariate observations (be they
scalar, boolean, categorical, ordinal, or count). Even multinomial
regressions are just repeated categorical regressions. In contrast,
this section discusses regression when each observed value is
multivariate. To relate multiple outcomes in a regression setting,
their error terms are provided with covariance structure.</p>
<p>This section considers two cases, seemingly unrelated regressions for
continuous multivariate quantities and multivariate probit regression
for boolean multivariate quantities.</p>
<div id="seemingly-unrelated-regressions" class="section level3 unnumbered">
<h3>Seemingly Unrelated Regressions</h3>
<p>The first model considered is the “seemingly unrelated” regressions
(SUR) of econometrics where several linear regressions share
predictors and use a covariance error structure rather than
independent errors <span class="citation">(Zellner <a href="#ref-Zellner:1962">1962</a>; Greene <a href="#ref-Greene:2011">2011</a>)</span>.</p>
<p>The model is easy to write down as a regression,</p>
<p><span class="math display">\[
\begin{array}{rcl}
 y_n &amp; = &amp; x_n \, \beta + \epsilon_n
\\[4pt]
 \epsilon_n &amp; \sim &amp; \mathsf{multivariate\ normal}(0, \Sigma)
\end{array}
\]</span></p>
<p>where <span class="math inline">\(x_n\)</span> is a <span class="math inline">\(J\)</span>-row-vector of predictors (<span class="math inline">\(x\)</span> is an <span class="math inline">\((N \times J)\)</span> matrix), <span class="math inline">\(y_n\)</span> is a <span class="math inline">\(K\)</span>-vector of observations, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\((K \times J)\)</span> matrix of regression coefficients (vector <span class="math inline">\(\beta_k\)</span> holds
coefficients for outcome <span class="math inline">\(k\)</span>), and <span class="math inline">\(\Sigma\)</span> is covariance matrix
governing the error. As usual, the intercept can be rolled into <span class="math inline">\(x\)</span>
as a column of ones.</p>
<p>The basic Stan code is straightforward (though see below for more
optimized code for use with LKJ priors on correlation).</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=1&gt; J;
  int&lt;lower=0&gt; N;
  vector[J] x[N];
  vector[K] y[N];
}
parameters {
  matrix[K, J] beta;
  cov_matrix[K] Sigma;
}
model {
  vector[K] mu[N];
  for (n in 1:N)
    mu[n] = beta * x[n];
  y ~ multi_normal(mu, Sigma);
}</code></pre>
<p>For efficiency, the multivariate normal is vectorized by precomputing
the array of mean vectors and sharing the same covariance matrix.</p>
<p>Following the advice in the <a href="multivariate-hierarchical-priors.section">multivariate hierarchical priors
section</a>, we will place a
weakly informative normal prior on the regression coefficients, an LKJ
prior on the correlations and a half-Cauchy prior on standard
deviations. The covariance structure is parameterized in terms of
Cholesky factors for efficiency and arithmetic stability.</p>
<pre><code>...
parameters {
  matrix[K, J] beta;
  cholesky_factor_corr[K] L_Omega;
  vector&lt;lower=0&gt;[K] L_sigma;
}
model {
  vector[K] mu[N];
  matrix[K, K] L_Sigma;

  for (n in 1:N)
    mu[n] = beta * x[n];

  L_Sigma = diag_pre_multiply(L_sigma, L_Omega);

  to_vector(beta) ~ normal(0, 5);
  L_Omega ~ lkj_corr_cholesky(4);
  L_sigma ~ cauchy(0, 2.5);

  y ~ multi_normal_cholesky(mu, L_Sigma);
}</code></pre>
<p>The Cholesky factor of the covariance matrix is then reconstructed as
a local variable and used in the model by scaling the Cholesky factor
of the correlation matrices. The regression coefficients get a prior
all at once by converting the matrix <code>beta</code> to a vector.</p>
<p>If required, the full correlation or covariance matrices may be
reconstructed from their Cholesky factors in the generated quantities
block.</p>
</div>
<div id="multivariate-probit-regression" class="section level3 unnumbered">
<h3>Multivariate Probit Regression</h3>
<p>The multivariate probit model generates sequences of boolean variables
by applying a step function to the output of a seemingly unrelated
regression.</p>
<p>The observations <span class="math inline">\(y_n\)</span> are <span class="math inline">\(D\)</span>-vectors of boolean values (coded 0 for
false, 1 for true). The values for the observations <span class="math inline">\(y_n\)</span> are based
on latent values <span class="math inline">\(z_n\)</span> drawn from a seemingly unrelated regression
model (see the previous section),</p>
<p><span class="math display">\[
\begin{array}{rcl}
 z_n &amp; = &amp; x_n \, \beta + \epsilon_n
\\[4pt]
 \epsilon_n &amp; \sim &amp; \mathsf{multivariate\ normal}(0, \Sigma)
\end{array}
\]</span></p>
<p>These are then put through the step function to produce a <span class="math inline">\(K\)</span>-vector <span class="math inline">\(z_n\)</span>
of boolean values with elements defined by
<span class="math display">\[
y_{n, k} = \mathrm{I}(z_{n, k} &gt; 0),
\]</span>
where <span class="math inline">\(\mathrm{I}()\)</span> is the indicator function taking the value 1 if its
argument is true and 0 otherwise.</p>
<p>Unlike in the seemingly unrelated regressions case, here the
covariance matrix <span class="math inline">\(\Sigma\)</span> has unit standard deviations (i.e., it is a
correlation matrix). As with ordinary probit and logistic
regressions, letting the scale vary causes the model (which is defined
only by a cutpoint at 0, not a scale) to be unidentified (see
<span class="citation">Greene (<a href="#ref-Greene:2011">2011</a>)</span>).</p>
<p>Multivariate probit regression can be coded in Stan using the trick
introduced by <span class="citation">Albert and Chib (<a href="#ref-AlbertChib:1993">1993</a>)</span>, where the underlying continuous
value vectors <span class="math inline">\(y_n\)</span> are coded as truncated parameters. The key to
coding the model in Stan is declaring the latent vector <span class="math inline">\(z\)</span> in two
parts, based on whether the corresponding value of <span class="math inline">\(y\)</span> is 0 or 1.
Otherwise, the model is identical to the seemingly unrelated
regression model in the previous section.</p>
<p>First, we introduce a sum function for two-dimensional arrays of
integers; this is going to help us calculate how many total 1 values
there are in <span class="math inline">\(y\)</span>.</p>
<pre><code>functions {
  int sum2d(int[,] a) {
    int s = 0;
    for (i in 1:size(a))
      s += sum(a[i]);
    return s;
  }
}</code></pre>
<p>The function is trivial, but it’s not a built-in for Stan and it’s easier to
understand the rest of the model if it’s pulled into its own function
so as not to create a distraction.</p>
<p>The data declaration block is much like for the seemingly unrelated
regressions, but the observations <code>y</code> are now integers
constrained to be 0 or 1.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=1&gt; D;
  int&lt;lower=0&gt; N;
  int&lt;lower=0,upper=1&gt; y[N,D];
  vector[K] x[N];
}</code></pre>
<p>After declaring the data, there is a rather involved transformed data
block whose sole purpose is to sort the data array <code>y</code> into
positive and negative components, keeping track of indexes so that
<code>z</code> can be easily reassembled in the transformed parameters
block.</p>
<pre><code>transformed data {
  int&lt;lower=0&gt; N_pos;
  int&lt;lower=1,upper=N&gt; n_pos[sum2d(y)];
  int&lt;lower=1,upper=D&gt; d_pos[size(n_pos)];
  int&lt;lower=0&gt; N_neg;
  int&lt;lower=1,upper=N&gt; n_neg[(N * D) - size(n_pos)];
  int&lt;lower=1,upper=D&gt; d_neg[size(n_neg)];

  N_pos = size(n_pos);
  N_neg = size(n_neg);
  {
    int i;
    int j;
    i = 1;
    j = 1;
    for (n in 1:N) {
      for (d in 1:D) {
        if (y[n,d] == 1) {
          n_pos[i] = n;
          d_pos[i] = d;
          i += 1;
        } else {
          n_neg[j] = n;
          d_neg[j] = d;
          j += 1;
        }
      }
    }
  }
}</code></pre>
<p>The variables <code>N_pos</code> and <code>N_neg</code> are set to the number of
true (1) and number of false (0) observations in <code>y</code>. The loop
then fills in the sequence of indexes for the positive and negative
values in four arrays.</p>
<p>The parameters are declared as follows.</p>
<pre><code>parameters {
  matrix[D, K] beta;
  cholesky_factor_corr[D] L_Omega;
  vector&lt;lower=0&gt;[N_pos] z_pos;
  vector&lt;upper=0&gt;[N_neg] z_neg;
}</code></pre>
<p>These include the regression coefficients <code>beta</code> and the Cholesky
factor of the correlation matrix, <code>L_Omega</code>. This time there is
no scaling because the covariance matrix has unit scale (i.e., it is a
correlation matrix; see above).</p>
<p>The critical part of the parameter declaration is that the latent real
value <span class="math inline">\(z\)</span> is broken into positive-constrained and negative-constrained
components, whose size was conveniently calculated in the transformed
data block. The transformed data block’s real work was to allow the
transformed parameter block to reconstruct <span class="math inline">\(z\)</span>.</p>
<pre><code>transformed parameters {
  vector[D] z[N];
  for (n in 1:N_pos)
    z[n_pos[n], d_pos[n]] = z_pos[n];
  for (n in 1:N_neg)
    z[n_neg[n], d_neg[n]] = z_neg[n];
}</code></pre>
<p>At this point, the model is simple, pretty much recreating the
seemingly unrelated regression.</p>
<pre><code>model {
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(beta) ~ normal(0, 5);
  {
    vector[D] beta_x[N];
    for (n in 1:N)
      beta_x[n] = beta * x[n];
    z ~ multi_normal_cholesky(beta_x, L_Omega);
  }
}</code></pre>
<p>This simple form of model is made possible by the Albert and
Chib-style constraints on <code>z</code>.</p>
<p>Finally, the correlation matrix itself can be put back together in the
generated quantities block if desired.</p>
<pre><code>generated quantities {
  corr_matrix[D] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
}</code></pre>
<p>The same could be done for the seemingly unrelated
regressions in the previous section.</p>
</div>
</div>
<div id="applications-of-pseudorandom-number-generation" class="section level2">
<h2><span class="header-section-number">5.16</span> Applications of Pseudorandom Number Generation</h2>
<p>The main application of pseudorandom number generator (PRNGs) is for
posterior inference, including prediction and posterior predictive
checks. They can also be used for pure data simulation, which is like
a posterior predictive check with no conditioning. See the function
reference manual for a complete description of the syntax and usage of
pseudorandom number generators.</p>
<div id="prediction" class="section level3 unnumbered">
<h3>Prediction</h3>
<p>Consider predicting unobserved outcomes using linear
regression. Given predictors <span class="math inline">\(x_1, \ldots, x_N\)</span> and observed outcomes
<span class="math inline">\(y_1,\ldots,y_N\)</span>, and assuming a standard linear regression with
intercept <span class="math inline">\(\alpha\)</span>, slope <span class="math inline">\(\beta\)</span>, and error scale <span class="math inline">\(\sigma\)</span>, along with
improper uniform priors, the posterior over the parameters given <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span> is</p>
<p><span class="math display">\[
p(\alpha, \beta, \sigma \, | \, x, y)
\propto
\prod_{n=1}^N
  \mathsf{normal}(y_n \, | \, \alpha + \beta x_n, \sigma).
\]</span></p>
<p>For this model, the posterior predictive inference for a new outcome
<span class="math inline">\(\tilde{y}_m\)</span> given a predictor <span class="math inline">\(\tilde{x}_m\)</span>, conditioned on the
observed data <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, is
<span class="math display">\[
p(\tilde{y}_n \, | \, \tilde{x}_n, x, y)
= \int_{(\alpha,\beta,\sigma)}
  \mathsf{normal}(\tilde{y}_n \, | \, \alpha + \beta \tilde{x}_n, \sigma)
  *
  p(\alpha, \beta, \sigma \, | \, x, y)
  \
  \mathrm{d}(\alpha,\beta,\sigma).
\]</span></p>
<p>To code the posterior predictive inference in Stan, a standard linear
regression is combined with a random number in the generated
quantities block.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;
  vector[N] x;
  int&lt;lower=0&gt; N_tilde;
  vector[N_tilde] x_tilde;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
generated quantities {
  vector[N_tilde] y_tilde;
  for (n in 1:N_tilde)
    y_tilde[n] = normal_rng(alpha + beta * x_tilde[n], sigma);
}</code></pre>
<p>Given observed predictors <span class="math inline">\(x\)</span> and outcomes <span class="math inline">\(y\)</span>, <code>y_tilde</code> will
be drawn according to <span class="math inline">\(p(\tilde{y} \, | \, \tilde{x}, y, x)\)</span>. This
means that, for example, the posterior mean for <code>y_tilde</code> is the
estimate of the outcome that minimizes expected square error
(conditioned on the data and model).</p>
</div>
<div id="posterior-predictive-checks" class="section level3 unnumbered">
<h3>Posterior Predictive Checks</h3>
<p>A good way to investigate the fit of a model to the data, a critical
step in Bayesian data analysis, is to generate simulated data
according to the parameters of the model. This is carried out with
exactly the same procedure as before, only the observed data
predictors <span class="math inline">\(x\)</span> are used in place of new predictors <span class="math inline">\(\tilde{x}\)</span> for
unobserved outcomes. If the model fits the data well, the predictions
for <span class="math inline">\(\tilde{y}\)</span> based on <span class="math inline">\(x\)</span> should match the observed data <span class="math inline">\(y\)</span>.</p>
<p>To code posterior predictive checks in Stan requires only a slight
modification of the prediction code to use <span class="math inline">\(x\)</span> and <span class="math inline">\(N\)</span> in place of
<span class="math inline">\(\tilde{x}\)</span> and <span class="math inline">\(\tilde{N}\)</span>,</p>
<pre><code>generated quantities {
  vector[N] y_tilde;
  for (n in 1:N)
    y_tilde[n] = normal_rng(alpha + beta * x[n], sigma);
}</code></pre>
<p><span class="citation">Gelman et al. (<a href="#ref-GelmanEtAl:2013">2013</a>)</span> recommend choosing several posterior draws
<span class="math inline">\(\tilde{y}^{(1)}, \ldots, \tilde{y}^{(M)}\)</span> and plotting each of them
alongside the data <span class="math inline">\(y\)</span> that was actually observed. If the model fits
well, the simulated <span class="math inline">\(\tilde{y}\)</span> will look like the actual data <span class="math inline">\(y\)</span>.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-ChungEtAl:2013">
<p>Chung, Yeojin, Sophia Rabe-Hesketh, Vincent Dorie, Andrew Gelman, and Jingchen Liu. 2013. “A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models.” <em>Psychometrika</em> 78 (4): 685–709.</p>
</div>
<div id="ref-GelmanHill:2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel-Hierarchical Models</em>. Cambridge, United Kingdom: Cambridge University Press.</p>
</div>
<div id="ref-Curtis:2010">
<p>Curtis, S. McKay. 2010. “BUGS Code for Item Response Theory.” <em>Journal of Statistical Software</em> 36 (1). American Statistical Association: 1–34.</p>
</div>
<div id="ref-barnard-mcculloch-meng:2000">
<p>Barnard, John, Robert McCulloch, and Xiao-Li Meng. 2000. “Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage.” <em>Statistica Sinica</em>, 1281–1311.</p>
</div>
<div id="ref-Zellner:1962">
<p>Zellner, Arnold. 1962. “An Efficient Method of Estimating Seemingly Unrelated Regression Equations and Tests for Aggregation Bias.” <em>Journal of the American Statistical Association</em> 57: 348–68.</p>
</div>
<div id="ref-Greene:2011">
<p>Greene, William H. 2011. <em>Econometric Analysis</em>. 7th ed. Prentice-Hall.</p>
</div>
<div id="ref-AlbertChib:1993">
<p>Albert, J. H., and S. Chib. 1993. “Bayesian Analysis of Binary and Polychotomous Response Data.” <em>Journal of the American Statistical Association</em> 88: 669–79.</p>
</div>
<div id="ref-GelmanEtAl:2013">
<p>Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. London: Chapman &amp;Hall/CRC Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Unlike in Python and R, which are interpreted, Stan is translated to C++ and compiled, so loops and assignment statements are fast. Vectorized code is faster in Stan because (a) the expression tree used to compute derivatives can be simplified, leading to fewer virtual function calls, and (b) computations that would be repeated in the looping version, such as <code>log(sigma)</code> in the above model, will be computed once and reused.<a href="regression-models.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>The <code>Phi_approx</code> function is a rescaled version of the inverse logit function, so while the scale is roughly the same <span class="math inline">\(\Phi\)</span>, the tails do not match.<a href="regression-models.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>The Bernoulli-logit distribution builds in the log link function, taking <span class="math display">\[\mathsf{BernoulliLogit}(y \mid \alpha) = \mathsf{Bernoulli}(y \mid \mathrm{logit}^{-1}(\alpha)).\]</span><a href="regression-models.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p><span class="citation">Gelman and Hill (<a href="#ref-GelmanHill:2007">2007</a>)</span> treat the <span class="math inline">\(\delta\)</span> term equivalently as the location parameter in the distribution of student abilities.<a href="regression-models.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>The prior is named for Lewandowski, Kurowicka, and Joe, as it was derived by inverting the random correlation matrix generation strategy of <span class="citation">Lewandowski, Kurowicka, and Joe (<a href="#ref-LewandowskiKurowickaJoe:2009">2009</a>)</span>.<a href="regression-models.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Thanks to Mike Lawrence for pointing this out in the GitHub issue for the manual.<a href="regression-models.html#fnref9" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="example-models-part.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="time-series-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
