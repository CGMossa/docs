<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="measurement-error-and-meta-analysis.html">
<link rel="next" href="sparse-ragged-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-overview.html#part-1-overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Overview</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>2</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="3" data-path="simple-examples.html"><a href="simple-examples.html"><i class="fa fa-check"></i><b>3</b> Simple Examples</a></li>
<li class="chapter" data-level="4" data-path="bayesian-workflow-1.html"><a href="bayesian-workflow-1.html"><i class="fa fa-check"></i><b>4</b> Bayesian Workflow</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="5" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>5</b> Regression Models</a></li>
<li class="chapter" data-level="6" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>6</b> Time-Series Models</a></li>
<li class="chapter" data-level="7" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>7</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="8" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>8</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="9" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>9</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="10" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>10</b> Finite Mixtures</a></li>
<li class="chapter" data-level="11" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>11</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="12" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>12</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="13" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>13</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="14" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>14</b> Clustering Models</a></li>
<li class="chapter" data-level="15" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>15</b> Gaussian Processes</a></li>
<li class="chapter" data-level="16" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>16</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="17" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>17</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="18" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>18</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="19" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>19</b> Modeling as Software Development</a></li>
<li class="chapter" data-level="20" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>20</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="21" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>21</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="22" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>22</b> User-Defined Functions</a></li>
<li class="chapter" data-level="23" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>23</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="24" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>24</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="25" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>25</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="26" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>26</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="27" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>27</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="28" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>28</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="29" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>29</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="30" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>30</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>31</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="latent-discrete.chapter" class="section level1">
<h1><span class="header-section-number">12</span> Latent Discrete Parameters</h1>
<p>Stan does not support sampling discrete parameters. So it is not
possible to directly translate BUGS or JAGS models with discrete
parameters (i.e., discrete stochastic nodes). Nevertheless, it is
possible to code many models that involve bounded discrete
parameters by marginalizing out the discrete parameters.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></p>
<p>This chapter shows how to code several widely-used models involving
latent discrete parameters. The next chapter, the <a href="clustering-chapter.html#clustering.chapter">clustering
chapter</a>, on clustering models, considers further
models involving latent discrete parameters.</p>
<div id="rao-blackwell.section" class="section level2">
<h2><span class="header-section-number">12.1</span> The Benefits of Marginalization</h2>
<p>Although it requires some algebra on the joint probability function,
a pleasant byproduct of the required calculations is the posterior
expectation of the marginalized variable, which is often the quantity
of interest for a model. This allows far greater exploration of the
tails of the distribution as well as more efficient sampling on an
iteration-by-iteration basis because the expectation at all possible
values is being used rather than itself being estimated through
sampling a discrete parameter.</p>
<p>Standard optimization algorithms, including expectation maximization
(EM), are often provided in applied statistics papers to describe
maximum likelihood estimation algorithms. Such derivations provide
exactly the marginalization needed for coding the model in Stan.</p>
</div>
<div id="change-point.section" class="section level2">
<h2><span class="header-section-number">12.2</span> Change Point Models</h2>
<p>The first example is a model of coal mining disasters in the U.K. for the years 1851–1962.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<div id="model-with-latent-discrete-parameter" class="section level3 unnumbered">
<h3>Model with Latent Discrete Parameter</h3>
<p><span class="citation">(Fonnesbeck et al. <a href="#ref-PyMC:2014">2013</a> Section 3.1)</span> provides a Poisson model of disaster
<span class="math inline">\(D_t\)</span> in year <span class="math inline">\(t\)</span> with two rate parameters, an early rate (<span class="math inline">\(e\)</span>)
and late rate (<span class="math inline">\(l\)</span>), that change at a given point in time <span class="math inline">\(s\)</span>. The
full model expressed using a latent discrete parameter <span class="math inline">\(s\)</span> is</p>
<p><span class="math display">\[
e  \sim  \mathsf{Exponential}(r_e)
\\
l  \sim  \mathsf{Exponential}(r_l)
\\
s \sim  \mathsf{Uniform}(1, T)
\\
D_t \sim  \mathsf{Poisson}(t &lt; s \ ? \ e \ : \ l)
\]</span></p>
<p>The last line uses the conditional operator (also known as the ternary
operator), which is borrowed from C and related languages. The
conditional operator has the same behavior as its counterpart in C++.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a></p>
<p>It uses a compact notation involving separating its three arguments by
a question mark (<code>?</code>) and a colon (<code>:</code>). The conditional
operator is defined by</p>
<p><span class="math display">\[
c \ ? \ x_1 \ : \ x_2
=
\begin{cases}
\ x_1 &amp; \mbox{if } c \mbox{ is true (i.e., non-zero), and}
\\
\ x_2 &amp; \mbox{if } c \mbox{ is false (i.e., zero).}
\end{cases}
\]</span>
As of version 2.10, Stan supports the conditional operator.</p>
</div>
<div id="marginalizing-out-the-discrete-parameter" class="section level3 unnumbered">
<h3>Marginalizing out the Discrete Parameter</h3>
<p>To code this model in Stan, the discrete parameter <span class="math inline">\(s\)</span> must be
marginalized out to produce a model defining the log of the
probability function <span class="math inline">\(p(e,l,D_t)\)</span>. The full joint probability factors
as</p>
<p><span class="math display">\[
p(e,l,s,D)
 =  p(e) \, p(l) \, p(s) \, p(D | s, e, l)
\\
 = 
\mathsf{Exponential}(e|r_e) \ \mathsf{Exponential}(l|r_l) \
\mathsf{Uniform}(s|1, T)
\prod_{t=1}^T \mathsf{Poisson}(D_t | t &lt; s \ ? \ e \ : \ l),
\]</span></p>
<p>To marginalize, an alternative factorization into prior and likelihood
is used,</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(e,l,D) &amp; = &amp; p(e,l) \, p(D|e,l),
\end{array}
\]</span></p>
<p>where the likelihood is defined by marginalizing <span class="math inline">\(s\)</span> as</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(D | e,l)
&amp; = &amp;
\sum_{s=1}^T p(s, D | e,l)
\\[3pt]
&amp; = &amp;
\sum_{s=1}^T p(s) p(D | s,e,l)
\\[3pt]
&amp; = &amp;
\sum_{s=1}^T \mathsf{Uniform}(s | 1,T)
\, \prod_{t=1}^T \mathsf{Poisson}(D_t | t &lt; s \ ? \ e \ : \ l)
\end{array}
\]</span></p>
<p>Stan operates on the log scale and thus requires the log likelihood,</p>
<p><span class="math display">\[
\log p(D | e,l) =
\mbox{log\_sum\_exp}_{s=1}^T
(
 \log \mathsf{Uniform}(s \, | \, 1, T)
+ \sum_{t=1}^T \log \mathsf{Poisson}(D_t \, | \, t &lt; s \ ?
\ e \ : \ l) ),
\]</span></p>
<p>where the log sum of exponents function is defined by
<span class="math display">\[
\mbox{log\_sum\_exp}_{n=1}^N \, \alpha_n
\ = \
\log \sum_{n=1}^N \mbox{exp}(\alpha_n).
\]</span></p>
<p>The log sum of exponents function allows the model to be coded
directly in Stan using the built-in function <code>log_sum_exp</code>,
which provides both arithmetic stability and efficiency for mixture
model calculations.</p>
</div>
<div id="coding-the-model-in-stan-1" class="section level3 unnumbered">
<h3>Coding the Model in Stan</h3>
<p>The Stan program for the change point model is shown in
the figure below. The transformed parameter
<code>lp[s]</code> stores the quantity <span class="math inline">\(\log p(s, D \, | \, e, l)\)</span>.</p>
<pre><code>data {
  real&lt;lower=0&gt; r_e;
  real&lt;lower=0&gt; r_l;

  int&lt;lower=1&gt; T;
  int&lt;lower=0&gt; D[T];
}
transformed data {
  real log_unif;
  log_unif = -log(T);
}
parameters {
  real&lt;lower=0&gt; e;
  real&lt;lower=0&gt; l;
}
transformed parameters {
  vector[T] lp;
  lp = rep_vector(log_unif, T);
  for (s in 1:T)
    for (t in 1:T)
      lp[s] = lp[s] + poisson_lpmf(D[t] | t &lt; s ? e : l);
}
model {
  e ~ exponential(r_e);
  l ~ exponential(r_l);
  target += log_sum_exp(lp);
}</code></pre>
<p>id:change-point-model.figure</p>
<p>A change point model in which disaster rates <code>D[t]</code> have one rate,
<code>e</code>, before the change point and a different rate, <code>l</code>, after the
change point. The change point itself, <code>s</code>, is marginalized out as
described in the text.</p>
<p>Although the change-point model is coded directly, the doubly nested
loop used for <code>s</code> and <code>t</code> is quadratic in <code>T</code>. Luke Wiklendt pointed
out that a linear alternative can be achieved by the use of dynamic
programming similar to the forward-backward algorithm for Hidden
Markov models; he submitted a slight variant of the following code to
replace the transformed parameters block of the above Stan program.</p>
<pre><code>transformed parameters {
    vector[T] lp;
    {
      vector[T + 1] lp_e;
      vector[T + 1] lp_l;
      lp_e[1] = 0;
      lp_l[1] = 0;
      for (t in 1:T) {
        lp_e[t + 1] = lp_e[t] + poisson_lpmf(D[t] | e);
        lp_l[t + 1] = lp_l[t] + poisson_lpmf(D[t] | l);
      }
      lp = rep_vector(log_unif + lp_l[T + 1], T)
           + head(lp_e, T) - head(lp_l, T);
    }
  }</code></pre>
<p>As should be obvious from looking at it, it has linear complexity in
<code>T</code> rather than quadratic. The result for the mining-disaster
data is about 20 times faster; the improvement will be greater for
larger <code>T</code>.</p>
<p>The key to understanding Wiklendt’s dynamic programming version is to
see that <code>head(lp_e)</code> holds the forward values, whereas
<code>lp_l[T + 1] - head(lp_l, T)</code> holds the backward values; the
clever use of subtraction allows <code>lp_l</code> to be accumulated
naturally in the forward direction.</p>
</div>
<div id="fitting-the-model-with-mcmc" class="section level3 unnumbered">
<h3>Fitting the Model with MCMC</h3>
<p>This model is easy to fit using MCMC with NUTS in its default
configuration. Convergence is fast and sampling produces roughly
one effective sample every two iterations. Because it is a relatively
small model (the inner double loop over time is roughly 20,000 steps),
it is fast.</p>
<p>The value of <code>lp</code> for each iteration for each change point is
available because it is declared as a transformed parameter. If the
value of <code>lp</code> were not of interest, it could be coded as a local
variable in the model block and thus avoid the I/O overhead of saving
values every iteration.</p>
</div>
<div id="posterior-distribution-of-the-discrete-change-point" class="section level3 unnumbered">
<h3>Posterior Distribution of the Discrete Change Point</h3>
<p>The value of <code>lp[s]</code> in a given iteration is given by <span class="math inline">\(\log p(s,D|e,l)\)</span> for the values of the early and late rates, <span class="math inline">\(e\)</span> and <span class="math inline">\(l\)</span>,
in the iteration. In each iteration after convergence, the early and
late disaster rates, <span class="math inline">\(e\)</span> and <span class="math inline">\(l\)</span>, are drawn from the posterior
<span class="math inline">\(p(e,l|D)\)</span> by MCMC sampling and the associated <code>lp</code> calculated.
The value of <code>lp</code> may be normalized to calculate <span class="math inline">\(p(s|e,l,D)\)</span> in
each iteration, based on on the current values of <span class="math inline">\(e\)</span> and <span class="math inline">\(l\)</span>.
Averaging over iterations provides an unnormalized probability
estimate of the change point being <span class="math inline">\(s\)</span> (see below for the normalizing
constant),</p>
<p><span class="math display">\[
\begin{array}{rcl}
p(s | D)
&amp; \propto &amp;
q(s | D)
\\[3pt]
&amp; = &amp;
\frac{1}{M} \sum_{m=1}^{M} \exp(`lp`[m,s]).
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mbox{lp}[m,s]\)</span> represents the value of <code>lp</code> in
posterior draw <span class="math inline">\(m\)</span> for change point <span class="math inline">\(s\)</span>. By averaging over draws,
<span class="math inline">\(e\)</span> and <span class="math inline">\(l\)</span> are themselves marginalized out, and the result has no
dependence on a given iteration’s value for <span class="math inline">\(e\)</span> and <span class="math inline">\(l\)</span>. A final
normalization then produces the quantity of interest, the posterior
probability of the change point being <span class="math inline">\(s\)</span> conditioned on the data <span class="math inline">\(D\)</span>,</p>
<p><span class="math display">\[
p(s | D)
=
\frac{q(s|D)}{\sum_{s&#39;=1}^T q(s&#39; | D)}.
\]</span></p>
<p>A plot of the values of <span class="math inline">\(\log p(s|D)\)</span> computed using Stan 2.4’s
default MCMC implementation is shown in the posterior plot.</p>
<p>Log probability of change point being in year, calculated analytically.</p>
<div class="figure">
<embed src="img/change-point-posterior.pdf" />
<p class="caption">Analytical change-point posterior (log scale)</p>
</div>
<p>The frequency of change points generated by sampling the discrete change
points.</p>
<div class="figure">
<embed src="img/s-discrete-posterior.pdf" />
<p class="caption">Sampled change-point posterior (linear scale).</p>
</div>
<p>id:change-point-posterior.figure</p>
<p>In order their range of estimates be visible, the first plot is on the log
scale and the second plot on the linear scale; note the narrower range
of years in the right-hand plot resulting from sampling. The posterior
mean of <span class="math inline">\(s\)</span> is roughly 1891.</p>
</div>
<div id="discrete-sampling" class="section level3 unnumbered">
<h3>Discrete Sampling</h3>
<p>The generated quantities block may be used to draw discrete parameter
values using the built-in pseudo-random number generators. For
example, with <code>lp</code> defined as above, the following program
draws a random value for <code>s</code> at every iteration.</p>
<pre><code>generated quantities {
  int&lt;lower=1,upper=T&gt; s;
  s = categorical_logit_rng(lp);
}</code></pre>
<p>A posterior histogram of draws for <span class="math inline">\(s\)</span> is shown on the second change
point posterior figure above.</p>
<p>Compared to working in terms of expectations, discrete sampling is
highly inefficient, especially for tails of distributions, so this
approach should only be used if draws from a distribution are
explicitly required. Otherwise, expectations should be computed in
the generated quantities block based on the posterior distribution for
<code>s</code> given by <code>softmax(lp)</code>.</p>
</div>
<div id="posterior-covariance" class="section level3 unnumbered">
<h3>Posterior Covariance</h3>
<p>The discrete sample generated for <span class="math inline">\(s\)</span> can be used to calculate
covariance with other parameters. Although the sampling approach is
straightforward, it is more statistically efficient (in the sense of
requiring far fewer iterations for the same degree of accuracy) to
calculate these covariances in expectation using <code>lp</code>.</p>
</div>
<div id="multiple-change-points" class="section level3 unnumbered">
<h3>Multiple Change Points</h3>
<p>There is no obstacle in principle to allowing multiple change points.
The only issue is that computation increases from linear to quadratic
in marginalizing out two change points, cubic for three change points,
and so on. There are three parameters, <code>e</code>, <code>m</code>, and
<code>l</code>, and two loops for the change point and then one over time,
with log densities being stored in a matrix.</p>
<pre><code>matrix[T, T] lp;
lp = rep_matrix(log_unif, T);
for (s1 in 1:T)
  for (s2 in 1:T)
    for (t in 1:T)
      lp[s1,s2] = lp[s1,s2]
        + poisson_lpmf(D[t] | t &lt; s1 ? e : (t &lt; s2 ? m : l));</code></pre>
<p>The matrix can then be converted back to a vector using
<code>to_vector</code> before being passed to <code>log_sum_exp</code>.</p>
</div>
</div>
<div id="mark-recapture-models" class="section level2">
<h2><span class="header-section-number">12.3</span> Mark-Recapture Models</h2>
<p>A widely applied field method in ecology is to capture (or sight)
animals, mark them (e.g., by tagging), then release them. This
process is then repeated one or more times, and is often done for
populations on an ongoing basis. The resulting data may be used to
estimate population size.</p>
<p>The first subsection describes a simple mark-recapture model that does
not involve any latent discrete parameters. The following subsections
describes the Cormack-Jolly-Seber model, which involves latent
discrete parameters for animal death.</p>
<div id="simple-mark-recapture-model" class="section level3 unnumbered">
<h3>Simple Mark-Recapture Model</h3>
<p>In the simplest case, a one-stage mark-recapture study produces the
following data</p>
<ul>
<li><span class="math inline">\(M\)</span> : number of animals marked in first capture,</li>
<li><span class="math inline">\(C\)</span> : number animals in second capture, and</li>
<li><span class="math inline">\(R\)</span> : number of marked animals in second capture.</li>
</ul>
<p>The estimand of interest is</p>
<ul>
<li><span class="math inline">\(N\)</span> : number of animals in the population.</li>
</ul>
<p>Despite the notation, the model will take <span class="math inline">\(N\)</span> to be a continuous
parameter; just because the population must be finite doesn’t mean the
parameter representing it must be. The parameter will be used to
produce a real-valued estimate of the population size.</p>
<p>The Lincoln-Petersen <span class="citation">(Lincoln <a href="#ref-Lincoln:1930">1930</a>,<span class="citation">@Petersen:1896</span>)</span> method for
estimating population size is</p>
<p><span class="math display">\[
\hat{N} = \frac{M C}{R}.
\]</span></p>
<p>This population estimate would arise from a probabilistic model in
which the number of recaptured animals is distributed binomially,
<span class="math display">\[
R \sim \mathsf{Binomial}(C, M / N)
\]</span>
given the total number of animals captured in the second round (<span class="math inline">\(C\)</span>)
with a recapture probability of <span class="math inline">\(M/N\)</span>, the fraction of the total
population <span class="math inline">\(N\)</span> marked in the first round.</p>
<pre><code>data {
  int&lt;lower=0&gt; M;
  int&lt;lower=0&gt; C;
  int&lt;lower=0,upper=min(M,C)&gt; R;
}
parameters {
  real&lt;lower=(C - R + M)&gt; N;
}
model {
  R ~ binomial(C, M / N);
}</code></pre>
<p>id:lincoln-petersen-model.figure</p>
<p>A probabilistic formulation of the Lincoln-Petersen
estimator for population size based on data from a one-step
mark-recapture study. The lower bound on <span class="math inline">\(N\)</span> is necessary to
efficiently eliminate impossible values.</p>
<p>The probabilistic variant of the Lincoln-Petersen estimator can be
directly coded in Stan as shown in the Lincon-Petersen model figure.
The Lincoln-Petersen estimate is the maximum likelihood estimate (MLE)
for this model.</p>
<p>To ensure the MLE is the Lincoln-Petersen estimate, an improper
uniform prior for <span class="math inline">\(N\)</span> is used; this could (and should) be replaced
with a more informative prior if possible based on knowledge of the
population under study.</p>
<p>The one tricky part of the model is the lower bound <span class="math inline">\(C - R + M\)</span> placed
on the population size <span class="math inline">\(N\)</span>. Values below this bound are impossible
because it is otherwise not possible to draw <span class="math inline">\(R\)</span> samples out of the
<span class="math inline">\(C\)</span> animals recaptured. Implementing this lower bound is necessary to
ensure sampling and optimization can be carried out in an
unconstrained manner with unbounded support for parameters on the
transformed (unconstrained) space. The lower bound in the declaration
for <span class="math inline">\(C\)</span> implies a variable transform
<span class="math inline">\(f : (C-R+M,\infty) \rightarrow (-\infty,+\infty)\)</span> defined by
<span class="math inline">\(f(N) = \log(N - (C - R + M))\)</span>; the reference manual contains full
details of all constrained parameter transforms.</p>
</div>
<div id="cormack-jolly-seber-with-discrete-parameter" class="section level3 unnumbered">
<h3>Cormack-Jolly-Seber with Discrete Parameter</h3>
<p>The Cormack-Jolly-Seber (CJS) model
[<span class="citation">Cormack (<a href="#ref-Cormack:1964">1964</a>)</span>; Jolly:1965; Seber:1965] is an open-population model
in which the population may change over time due to death; the
presentation here draws heavily on <span class="citation">Schofield (<a href="#ref-Schofield:2007">2007</a>)</span>.</p>
<p>The basic data are</p>
<ul>
<li><span class="math inline">\(I\)</span>: number of individuals,</li>
<li><span class="math inline">\(T\)</span>: number of capture periods, and</li>
<li><span class="math inline">\(y_{i,t}\)</span>: Boolean indicating if individual <span class="math inline">\(i\)</span> was captured at
time <span class="math inline">\(t\)</span>.</li>
</ul>
<p>Each individual is assumed to have been captured at least once because
an individual only contributes information conditionally after they
have been captured the first time.</p>
<p>There are two Bernoulli parameters in the model,</p>
<ul>
<li><span class="math inline">\(\phi_t\)</span> : probability that animal alive at time <span class="math inline">\(t\)</span> survives
until <span class="math inline">\(t + 1\)</span> and</li>
<li><span class="math inline">\(p_t\)</span> : probability that animal alive at time <span class="math inline">\(t\)</span> is captured at
time <span class="math inline">\(t\)</span>.</li>
</ul>
<p>These parameters will both be given uniform priors, but information
should be used to tighten these priors in practice.</p>
<p>The CJS model also employs a latent discrete parameter <span class="math inline">\(z_{i,t}\)</span>
indicating for each individual <span class="math inline">\(i\)</span> whether it is alive at time <span class="math inline">\(t\)</span>,
distributed as</p>
<p><span class="math display">\[
z_{i,t} \sim \mathsf{Bernoulli}(z_{i,t-1} \ ? \ 0 \ : \ \phi_{t-1}).
\]</span></p>
<p>The conditional prevents the model positing zombies; once an animal is
dead, it stays dead. The data distribution is then simple to express
conditional on <span class="math inline">\(z\)</span> as</p>
<p><span class="math display">\[
_{i,t} \sim \mathsf{Bernoulli}(z_{i,t} \ ? \ 0 \ : \ p_t).
\]</span></p>
<p>The conditional enforces the constraint that dead animals cannot be captured.</p>
</div>
<div id="collective-cormack-jolly-seber-model" class="section level3 unnumbered">
<h3>Collective Cormack-Jolly-Seber Model</h3>
<p>This subsection presents an implementation of the model in terms of
counts for different history profiles for individuals over three
capture times. It assumes exchangeability of the animals in that each
is assigned the same capture and survival probabilities.</p>
<p>In order to ease the marginalization of the latent discrete parameter
<span class="math inline">\(z_{i,t}\)</span>, the Stan models rely on a derived quantity <span class="math inline">\(\chi_t\)</span> for
the probability that an individual is never captured again if it is
alive at time <span class="math inline">\(t\)</span> (if it is dead, the recapture probability is zero).
this quantity is defined recursively by
<span class="math display">\[
\chi_t
=
\begin{cases}
1
&amp; \mbox{if } t = T
\\[3pt]
(1 - \phi_t) + \phi_t (1 - p_{t+1}) \chi_{t+1}
&amp; \mbox{ if } t &lt; T
\end{cases}
\]</span></p>
<p>The base case arises because if an animal was captured in the last
time period, the probability it is never captured again is 1 because
there are no more capture periods. The recursive case defining
<span class="math inline">\(\chi_{t}\)</span> in terms of <span class="math inline">\(\chi_{t+1}\)</span> involves two possibilities: (1)
not surviving to the next time period, with probability <span class="math inline">\((1 - \phi_t)\)</span>, or (2) surviving to the next time period with probability
<span class="math inline">\(\phi_t\)</span>, not being captured in the next time period with probability
<span class="math inline">\((1 - p_{t+1})\)</span>, and not being captured again after being alive in
period <span class="math inline">\(t+1\)</span> with probability <span class="math inline">\(\chi_{t+1}\)</span>.</p>
<p>With three capture times, there are three captured/not-captured
profiles an individual may have. These may be naturally coded as
binary numbers as follows.</p>

<p>History 0, for animals that are never captured, is unobservable
because only animals that are captured are observed. History 1, for
animals that are only captured in the last round, provides no
information for the CJS model, because capture/non-capture status is
only informative when conditioned on earlier captures. For the
remaining cases, the contribution to the likelihood is provided in the
final column.</p>
<p>By defining these probabilities in terms of <span class="math inline">\(\chi\)</span> directly, there is
no need for a latent binary parameter indicating whether an animal is
alive at time <span class="math inline">\(t\)</span> or not. The definition of <span class="math inline">\(\chi\)</span> is typically used
to define the likelihood (i.e., marginalize out the latent discrete
parameter) for the CJS model <span class="citation">(Schofield <a href="#ref-Schofield:2007">2007</a>, 9)</span>.</p>
<p>The Stan model defines <span class="math inline">\(\chi\)</span> as a transformed parameter based on
parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(p\)</span>. In the model block, the log probability is
incremented for each history based on its count. This second step is
similar to collecting Bernoulli observations into a binomial or
categorical observations into a multinomial, only it is coded directly
in the Stan program using <code>target~+=</code> rather than
being part of a built-in probability function.</p>

<p>id:cjs-history.figure</p>
<div id="identifiability-1" class="section level4 unnumbered">
<h4>Identifiability</h4>
<p>The parameters <span class="math inline">\(\phi_2\)</span> and <span class="math inline">\(p_3\)</span>, the probability of death at time 2
and probability of capture at time 3 are not identifiable, because both
may be used to account for lack of capture at time 3. Their product,
<span class="math inline">\(\beta_3 = \phi_2 \, p_3\)</span>, is identified. The Stan model defines
<code>beta3</code> as a generated quantity. Unidentified parameters pose a
problem for Stan’s samplers’ adaptation. Although the problem posed
for adaptation is mild here because the parameters are bounded and
thus have proper uniform priors, it would be better to formulate an
identified parameterization. One way to do this would be to formulate
a hierarchical model for the <span class="math inline">\(p\)</span> and <span class="math inline">\(\phi\)</span> parameters.</p>
</div>
</div>
<div id="individual-cormack-jolly-seber-model" class="section level3 unnumbered">
<h3>Individual Cormack-Jolly-Seber Model</h3>
<p>This section presents a version of the Cormack-Jolly-Seber (CJS) model
cast at the individual level rather than collectively as in the
previous subsection. It also extends the model to allow an arbitrary
number of time periods. The data will consist of the number <span class="math inline">\(T\)</span> of
capture events, the number <span class="math inline">\(I\)</span> of individuals, and a boolean flag
<span class="math inline">\(y_{i,t}\)</span> indicating if individual <span class="math inline">\(i\)</span> was observed at time <span class="math inline">\(t\)</span>. In
Stan,</p>
<pre><code>data {
  int&lt;lower=2&gt; T;
  int&lt;lower=0&gt; I;
  int&lt;lower=0,upper=1&gt; y[I, T];
}</code></pre>
<p>The advantages to the individual-level model is that it becomes
possible to add individual “random effects” that affect survival or
capture probability, as well as to avoid the combinatorics involved in
unfolding <span class="math inline">\(2^T\)</span> observation histories for <span class="math inline">\(T\)</span> capture times.</p>
<div id="utility-functions" class="section level4 unnumbered">
<h4>Utility Functions</h4>
<p>The individual CJS model is written involves several function
definitions. The first two are used in the transformed data block to
compute the first and last time period in which an animal was
captured.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<pre><code>functions {
  int first_capture(int[] y_i) {
    for (k in 1:size(y_i))
      if (y_i[k])
        return k;
    return 0;
  }
  int last_capture(int[] y_i) {
    for (k_rev in 0:(size(y_i) - 1)) {
      int k;
      k = size(y_i) - k_rev;
      if (y_i[k])
        return k;
    }
    return 0;
  }
  ...
}</code></pre>
<p>These two functions are used to define the first and last capture time
for each individual in the transformed data block.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
<pre><code>transformed data {
  int&lt;lower=0,upper=T&gt; first[I];
  int&lt;lower=0,upper=T&gt; last[I];
  vector&lt;lower=0,upper=I&gt;[T] n_captured;
  for (i in 1:I)
    first[i] = first_capture(y[i]);
  for (i in 1:I)
    last[i] = last_capture(y[i]);
  n_captured = rep_vector(0, T);
  for (t in 1:T)
    for (i in 1:I)
      if (y[i, t])
        n_captured[t] = n_captured[t] + 1;
}</code></pre>
<p>The transformed data block also defines <code>n_captured[t]</code>, which is
the total number of captures at time <code>t</code>. The variable
<code>n_captured</code> is defined as a vector instead of an integer array
so that it can be used in an elementwise vector operation in the generated
quantities block to model the population estimates at each time point.</p>
<p>The parameters and transformed parameters are as before, but now there
is a function definition for computing the entire vector <code>chi</code>, the
probability that if an individual is alive at <code>t</code> that it will
never be captured again.</p>
<pre><code>parameters {
  vector&lt;lower=0,upper=1&gt;[T-1] phi;
  vector&lt;lower=0,upper=1&gt;[T] p;
}
transformed parameters {
  vector&lt;lower=0,upper=1&gt;[T] chi;
  chi = prob_uncaptured(T,p,phi);
}</code></pre>
<p>The definition of <code>prob_uncaptured</code>, from the functions block,
is</p>
<pre><code>functions {
  ...
  vector prob_uncaptured(int T, vector p, vector phi) {
    vector[T] chi;
    chi[T] = 1.0;
    for (t in 1:(T - 1)) {
      int t_curr;
      int t_next;
      t_curr = T - t;
      t_next = t_curr + 1;
      chi[t_curr] = (1 - phi[t_curr])
                     + phi[t_curr]
                       * (1 - p[t_next])
                       * chi[t_next];
    }
    return chi;
  }
}</code></pre>
<p>The function definition directly follows the mathematical definition
of <span class="math inline">\(\chi_t\)</span>, unrolling the recursion into an iteration and
defining the elements of <code>chi</code> from <code>T</code> down to 1.</p>
</div>
<div id="the-model" class="section level4 unnumbered">
<h4>The Model</h4>
<p>Given the precomputed quantities, the model block directly encodes the
CJS model’s log likelihood function. All parameters are left with
their default uniform priors and the model simply encodes the log
probability of the observations <code>q</code> given the parameters <code>p</code>
and <code>phi</code> as well as the transformed parameter <code>chi</code> defined
in terms of <code>p</code> and <code>phi</code>.</p>
<pre><code>model {
  for (i in 1:I) {
    if (first[i] &gt; 0) {
      for (t in (first[i]+1):last[i]) {
        1 ~ bernoulli(phi[t-1]);
        y[i, t] ~ bernoulli(p[t]);
      }
      1 ~ bernoulli(chi[last[i]]);
    }
  }
}</code></pre>
<p>The outer loop is over individuals, conditional skipping individuals
<code>i</code> which are never captured. The never-captured check depends
on the convention of the first-capture and last-capture functions
returning 0 for <code>first</code> if an individual is never captured.</p>
<p>The inner loop for individual <code>i</code> first increments the log
probability based on the survival of the individual with probability
<code>phi[t-1]</code>. The outcome of 1 is fixed because the individual
must survive between the first and last capture (i.e., no zombies).
The loop starts after the first capture, because all
information in the CJS model is conditional on the first capture.</p>
<p>In the inner loop, the observed capture status <code>y[i,~t]</code> for
individual <code>i</code> at time <code>t</code> has a Bernoulli distribution
based on the capture probability <code>p[t]</code> at time <code>t</code>.</p>
<p>After the inner loop, the probability of an animal never being seen
again after being observed at time <code>last[i]</code> is included, because
<code>last[i]</code> was defined to be the last time period in which animal
<code>i</code> was observed.</p>
</div>
<div id="identified-parameters" class="section level4 unnumbered">
<h4>Identified Parameters</h4>
<p>As with the collective model described in the previous subsection,
this model does not identify <code>phi[T-1]</code> and <code>p[T]</code>, but
does identify their product, <code>beta</code>. Thus <code>beta</code> is defined
as a generated quantity to monitor convergence and report.</p>
<pre><code>generated quantities {
  real beta;
  ...

  beta = phi[T-1] * p[T];
  ...
}</code></pre>
<p>The parameter <code>p[1]</code> is also not modeled and will just be uniform
between 0 and 1. A more finely articulated model might have a
hierarchical or time-series component, in which case <code>p[1]</code> would
be an unknown initial condition and both <code>phi[T-1]</code> and
<code>p[T]</code> could be identified.</p>
</div>
<div id="population-size-estimates" class="section level4 unnumbered">
<h4>Population Size Estimates</h4>
<p>The generated quantities also calculates an estimate of the population
mean at each time <code>t</code> in the same way as in the simple
mark-recapture model as the number of individuals captured at time
<code>t</code> divided by the probability of capture at time <code>t</code>. This
is done with the elementwise division operation for vectors
(<code>./</code>) in the generated quantities block.</p>
<pre><code>generated quantities {
  ...
  vector&lt;lower=0&gt;[T] pop;
  ...
  pop = n_captured ./ p;
  pop[1] = -1;
}</code></pre>
</div>
<div id="generalizing-to-individual-effects" class="section level4 unnumbered">
<h4>Generalizing to Individual Effects</h4>
<p>All individuals are modeled as having the same capture probability,
but this model could be easily generalized to use a logistic
regression here based on individual-level inputs to be used as
predictors.</p>
</div>
</div>
</div>
<div id="data-coding-and-diagnostic-accuracy-models" class="section level2">
<h2><span class="header-section-number">12.4</span> Data Coding and Diagnostic Accuracy Models</h2>
<p>Although seemingly disparate tasks, the rating/coding/annotation of
items with categories and diagnostic testing for disease or other
conditions share several characteristics which allow their statistical
properties to modeled similarly.</p>
<div id="diagnostic-accuracy" class="section level3 unnumbered">
<h3>Diagnostic Accuracy</h3>
<p>Suppose you have diagnostic tests for a condition of varying
sensitivity and specificity. Sensitivity is the probability a test
returns positive when the patient has the condition and specificity is
the probability that a test returns negative when the patient does not
have the condition. For example, mammograms and puncture biopsy tests
both test for the presence of breast cancer. Mammograms have high
sensitivity and low specificity, meaning lots of false positives,
whereas puncture biopsies are the opposite, with low sensitivity and
high specificity, meaning lots of false negatives.</p>
<p>There are several estimands of interest in such studies. An
epidemiological study may be interested in the prevalence of a kind of
infection, such as malaria, in a population. A test development study
might be interested in the diagnostic accuracy of a new test. A health
care worker performing tests might be interested in the disease status
of a particular patient.</p>
</div>
<div id="data-coding" class="section level3 unnumbered">
<h3>Data Coding</h3>
<p>Humans are often given the task of coding (equivalently rating or
annotating) data. For example, journal or grant reviewers rate
submissions, a political study may code campaign commercials as to
whether they are attack ads or not, a natural language processing
study might annotate Tweets as to whether they are positive or
negative in overall sentiment, or a dentist looking at an X-ray
classifies a patient as having a cavity or not. In all of these
cases, the data coders play the role of the diagnostic tests and all
of the same estimands are in play — data coder accuracy and bias,
true categories of items being coded, or the prevalence of various
categories of items in the data.</p>
</div>
<div id="noisy-categorical-measurement-model" class="section level3 unnumbered">
<h3>Noisy Categorical Measurement Model</h3>
<p>In this section, only categorical ratings are considered, and the
challenge in the modeling for Stan is to marginalize out the discrete
parameters.</p>
<p><span class="citation">Dawid and Skene (<a href="#ref-DawidSkene:1979">1979</a>)</span> introduce a noisy-measurement model for
coding and apply it in the epidemiologial setting of coding what
<a href="mailto:doct@s">doct@s</a> say aboutpatient histories; the same model can be used
for diagnostic procedures.</p>
<div id="data-1" class="section level4 unnumbered">
<h4>Data</h4>
<p>The data for the model consists of <span class="math inline">\(J\)</span> raters (diagnostic tests), <span class="math inline">\(I\)</span>
items (patients), and <span class="math inline">\(K\)</span> categories (condition statuses) to annotate,
with <span class="math inline">\(y_{i, j} \in 1{:}K\)</span> being the rating provided by rater <span class="math inline">\(j\)</span> for
item <span class="math inline">\(i\)</span>. In a diagnostic test setting for a particular condition,
the raters are diagnostic procedures and often <span class="math inline">\(K=2\)</span>, with values
signaling the presence or absence of the condition.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
<p>It is relatively straightforward to extend Dawid and Skene’s model to
deal with the situation where not every rater rates each item exactly
once.</p>
</div>
</div>
<div id="model-parameters" class="section level3 unnumbered">
<h3>Model Parameters</h3>
<p>The model is based on three parameters, the first of which is discrete:</p>
<ul>
<li><span class="math inline">\(z_i\)</span> : a value in <span class="math inline">\(1{:}K\)</span> indicating the true category of item <span class="math inline">\(i\)</span>,</li>
<li><span class="math inline">\(\pi\)</span> : a <span class="math inline">\(K\)</span>-simplex for the prevalence of the <span class="math inline">\(K\)</span>
categories in the population, and</li>
<li><span class="math inline">\(\theta_{j,k}\)</span> : a <span class="math inline">\(K\)</span>-simplex for the response of annotator <span class="math inline">\(j\)</span>
to an item of true category <span class="math inline">\(k\)</span>.</li>
</ul>
</div>
<div id="noisy-measurement-model" class="section level3 unnumbered">
<h3>Noisy Measurement Model</h3>
<p>The true category of an item is assumed to be generated by a simple
categorical distribution based on item prevalence,
<span class="math display">\[
z_i \sim \mathsf{Categorical}(\pi).
\]</span></p>
<p>The rating <span class="math inline">\(y_{i, j}\)</span> provided for item <span class="math inline">\(i\)</span> by rater <span class="math inline">\(j\)</span> is modeled as
a categorical response of rater <span class="math inline">\(i\)</span> to an item of category <span class="math inline">\(z_i\)</span>,<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a></p>
<p><span class="math display">\[
y_{i, j} \sim \mathsf{Categorical}(\theta_{j,\pi_{z[i]}}).
\]</span></p>
<div id="priors-and-hierarchical-modeling" class="section level4 unnumbered">
<h4>Priors and Hierarchical Modeling</h4>
<p>Dawid and Skene provided maximum likelihood estimates for <span class="math inline">\(\theta\)</span> and
<span class="math inline">\(\pi\)</span>, which allows them to generate probability estimates for each <span class="math inline">\(z_i\)</span>.</p>
<p>To mimic Dawid and Skene’s maximum likelihood model, the parameters
<span class="math inline">\(\theta_{j,k}\)</span> and <span class="math inline">\(\pi\)</span> can be given uniform priors over
<span class="math inline">\(K\)</span>-simplexes. It is straightforward to generalize to Dirichlet
priors,
<span class="math display">\[
\pi \sim \mathsf{Dirichlet}(\alpha)
\]</span>
and
<span class="math display">\[
\theta_{j,k} \sim \mathsf{Dirichlet}(\beta_k)
\]</span>
with fixed hyperparameters <span class="math inline">\(\alpha\)</span> (a vector) and <span class="math inline">\(\beta\)</span> (a matrix
or array of vectors). The prior for <span class="math inline">\(\theta_{j,k}\)</span> must be allowed to
vary in <span class="math inline">\(k\)</span>, so that, for instance, <span class="math inline">\(\beta_{k,k}\)</span> is large enough to
allow the prior to favor better-than-chance annotators over random or
adversarial ones.</p>
<p>Because there are <span class="math inline">\(J\)</span> coders, it would be natural to extend the model
to include a hierarchical prior for <span class="math inline">\(\beta\)</span> and to partially pool the
estimates of coder accuracy and bias.</p>
</div>
<div id="marginalizing-out-the-true-category" class="section level4 unnumbered">
<h4>Marginalizing out the True Category</h4>
<p>Because the true category parameter <span class="math inline">\(z\)</span> is discrete, it must be
marginalized out of the joint posterior in order to carry out sampling
or maximum likelihood estimation in Stan. The joint posterior factors
as
<span class="math display">\[
p(y, \theta, \pi) = p(y | \theta,\pi) \, p(\pi) \, p(\theta),
\]</span>
where <span class="math inline">\(p(y | \theta,\pi)\)</span> is derived by marginalizing <span class="math inline">\(z\)</span> out of</p>
<p><span class="math display">\[
p(z, y | \theta, \pi)
\ = \
\prod_{i=1}^I \left( \mathsf{Categorical}(z_i | \pi)
                     \prod_{j=1}^J
                     \mathsf{Categorical}(y_{i, j}|\theta_{j, z[i]})
              \right).
\]</span></p>
<p>This can be done item by item, with
<span class="math display">\[
p(y | \theta, \pi)
\ = \
\prod_{i=1}^I \sum_{k=1}^K
  \left( \mathsf{Categorical}(z_i | \pi)
         \prod_{j=1}^J
         \mathsf{Categorical}(y_{i, j}|\theta_{j, z[i]})
  \right).
\]</span></p>
<p>In the missing data model, only the observed labels would be used in
the inner product.</p>
<p><span class="citation">Dawid and Skene (<a href="#ref-DawidSkene:1979">1979</a>)</span> derive exactly the same equation in their
Equation~(2.7), required for the E-step in their expectation
maximization (EM) algorithm. Stan requires the marginalized
probability function on the log scale,
<span class="math display">\[
\begin{array}{l}
\mbox{ } \ \log p(y | \theta, \pi)
\\[3pt]
\mbox{ } \  = \
\sum_{i=1}^I \log \left( \sum_{k=1}^K \exp
  \left( \log \mathsf{Categorical}(z_i | \pi)
         + \sum_{j=1}^J
         \log \mathsf{Categorical}(y_{i, j}|\theta_{j, z[i]})
  \right) \right),
\end{array}
\]</span>
which can be directly coded using Stan’s built-in <code>log_sum_exp</code>
function.</p>
</div>
</div>
<div id="stan-implementation" class="section level3 unnumbered">
<h3>Stan Implementation</h3>
<p>The Stan program for the Dawid and Skene model is provided below <span class="citation">Dawid and Skene (<a href="#ref-DawidSkene:1979">1979</a>)</span>.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=1&gt; I;
  int&lt;lower=1&gt; J;

  int&lt;lower=1,upper=K&gt; y[I, J];

  vector&lt;lower=0&gt;[K] alpha;
  vector&lt;lower=0&gt;[K] beta[K];
}
parameters {
  simplex[K] pi;
  simplex[K] theta[J, K];
}
transformed parameters {
  vector[K] log_q_z[I];
  for (i in 1:I) {
    log_q_z[i] = log(pi);
    for (j in 1:J)
      for (k in 1:K)
        log_q_z[i, k] = log_q_z[i, k]
                         + log(theta[j, k, y[i, j]]);
  }
}
model {
  pi ~ dirichlet(alpha);
  for (j in 1:J)
    for (k in 1:K)
      theta[j, k] ~ dirichlet(beta[k]);

  for (i in 1:I)
    target += log_sum_exp(log_q_z[i]);
}</code></pre>
<p>id:dawid-skene-model.figure</p>
<p>The model marginalizes out the discrete parameter <span class="math inline">\(z\)</span>, storing the
unnormalized conditional probability <span class="math inline">\(\log q(z_i=k|\theta,\pi)\)</span> in<br />
<code>log_q_z[i,~k]</code>.</p>
<p>The Stan model converges quickly and mixes well using NUTS starting at
diffuse initial points, unlike the equivalent model implemented with
Gibbs sampling over the discrete parameter. Reasonable weakly
informative priors are <span class="math inline">\(\alpha_k = 3\)</span> and <span class="math inline">\(\beta_{k,k} = 2.5 K\)</span> and
<span class="math inline">\(\beta_{k,k&#39;} = 1\)</span> if <span class="math inline">\(k \neq k&#39;\)</span>. Taking <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta_k\)</span> to
be unit vectors and applying optimization will produce the same answer
as the expectation maximization (EM) algorithm of
<span class="citation">Dawid and Skene (<a href="#ref-DawidSkene:1979">1979</a>)</span>.</p>
<div id="inference-for-the-true-category" class="section level4 unnumbered">
<h4>Inference for the True Category</h4>
<p>The quantity <code>log_q_z[i]</code> is defined as a transformed
parameter. It encodes the (unnormalized) log of <span class="math inline">\(p(z_i | \theta, \pi)\)</span>. Each iteration provides a value conditioned on that
iteration’s values for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\pi\)</span>. Applying the softmax
function to <code>log_q_z[i]</code> provides a simplex corresponding to
the probability mass function of <span class="math inline">\(z_i\)</span> in the posterior. These may
be averaged across the iterations to provide the posterior probability
distribution over each <span class="math inline">\(z_i\)</span>.</p>

</div>
</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-PyMC:2014">
<p>Fonnesbeck, Chris, Anand Patil, David Huard, and John Salvatier. 2013. <em>PyMC User’s Guide</em>.</p>
</div>
<div id="ref-Lincoln:1930">
<p>Lincoln, F. C. 1930. “Calculating Waterfowl Abundance on the Basis of Banding Returns.” <em>United States Department of Agriculture Circular</em> 118: 1–4.</p>
</div>
<div id="ref-Cormack:1964">
<p>Cormack, R. M. 1964. “Estimates of Survival from the Sighting of Marked Animals.” <em>Biometrika</em> 51 (3/4): 429–38.</p>
</div>
<div id="ref-Schofield:2007">
<p>Schofield, Matthew R. 2007. “Hierarchical Capture-Recapture Models.” PhD thesis, Department of of Statistics, University of Otago, Dunedin.</p>
</div>
<div id="ref-DawidSkene:1979">
<p>Dawid, A. P., and A. M. Skene. 1979. “Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.” <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 28 (1): 20–28.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>The computations are similar to those involved in expectation maximization (EM) algorithms <span class="citation">Dempster, Laird, and Rubin (<a href="#ref-dempster-et-al:1977">1977</a>)</span>.<a href="latent-discrete-chapter.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>The source of the data is <span class="citation">Jarrett (<a href="#ref-Jarret:1979">1979</a>)</span>, which itself is a note correcting an earlier data collection.<a href="latent-discrete-chapter.html#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>The R counterpart, <code>ifelse</code>, is slightly different in that it is typically used in a vectorized situation. The conditional operator is not (yet) vectorized in Stan.<a href="latent-discrete-chapter.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>An alternative would be to compute this on the outside and feed it into the Stan model as preprocessed data. Yet another alternative encoding would be a sparse one recording only the capture events along with their time and identifying the individual captured.<a href="latent-discrete-chapter.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>Both functions return 0 if the individual represented by the input array was never captured. Individuals with no captures are not relevant for estimating the model because all probability statements are conditional on earlier captures. Typically they would be removed from the data, but the program allows them to be included even though they make not contribution to the log probability function.<a href="latent-discrete-chapter.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>Diagnostic procedures are often ordinal, as in stages of cancer in oncological diagnosis or the severity of a cavity in dental diagnosis. Dawid and Skene’s model may be used as is or naturally generalized for ordinal ratings using a latent continuous rating and cutpoints as in ordinal logistic regression.<a href="latent-discrete-chapter.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>In the subscript, <span class="math inline">\(z[i]\)</span> is written as <span class="math inline">\(z_i\)</span> to
improve legibility.<a href="latent-discrete-chapter.html#fnref24" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="measurement-error-and-meta-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sparse-ragged-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
