<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="part-4-review-of-statistical-inference.html">
<link rel="next" href="mle-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-overview.html#part-1-overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Overview</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>2</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="3" data-path="simple-examples.html"><a href="simple-examples.html"><i class="fa fa-check"></i><b>3</b> Simple Examples</a></li>
<li class="chapter" data-level="4" data-path="bayesian-workflow-1.html"><a href="bayesian-workflow-1.html"><i class="fa fa-check"></i><b>4</b> Bayesian Workflow</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="5" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>5</b> Regression Models</a></li>
<li class="chapter" data-level="6" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>6</b> Time-Series Models</a></li>
<li class="chapter" data-level="7" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>7</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="8" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>8</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="9" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>9</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="10" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>10</b> Finite Mixtures</a></li>
<li class="chapter" data-level="11" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>11</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="12" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>12</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="13" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>13</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="14" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>14</b> Clustering Models</a></li>
<li class="chapter" data-level="15" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>15</b> Gaussian Processes</a></li>
<li class="chapter" data-level="16" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>16</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="17" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>17</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="18" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>18</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="19" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>19</b> Modeling as Software Development</a></li>
<li class="chapter" data-level="20" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>20</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="21" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>21</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="22" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>22</b> User-Defined Functions</a></li>
<li class="chapter" data-level="23" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>23</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="24" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>24</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="25" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>25</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="26" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>26</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="27" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>27</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="28" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>28</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="29" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>29</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="30" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>30</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>31</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-data-analysis-1" class="section level1">
<h1><span class="header-section-number">28</span> Bayesian Data Analysis</h1>
<div id="model-building" class="section level2">
<h2><span class="header-section-number">28.1</span> Model Building</h2>
</div>
<div id="inference" class="section level2">
<h2><span class="header-section-number">28.2</span> Inference</h2>
<div id="basic-quantities" class="section level3 unnumbered">
<h3>Basic Quantities</h3>
<p>The mechanics of Bayesian inference follow directly from Bayes’s rule.
To fix notation, let <span class="math inline">\(y\)</span> represent observed quantities such as data
and let <span class="math inline">\(\theta\)</span> represent unknown quantities such as parameters and
future observations. Both <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> will be modeled as random.
Let <span class="math inline">\(x\)</span> represent known, but unmodeled quantities such as constants,
hyperparameters, and predictors.</p>
</div>
<div id="probability-functions" class="section level3 unnumbered">
<h3>Probability Functions</h3>
<p>The probability function <span class="math inline">\(p(y,\theta)\)</span> is the joint probability
function of the data <span class="math inline">\(y\)</span> and parameters <span class="math inline">\(\theta\)</span>. The constants and
predictors <span class="math inline">\(x\)</span> are implicitly understood as being part of the
conditioning. The conditional probability function <span class="math inline">\(p(y|\theta)\)</span> of
the data <span class="math inline">\(y\)</span> given parameters <span class="math inline">\(\theta\)</span> and constants <span class="math inline">\(x\)</span> is called the
sampling probability function; it is also called the likelihood
function when viewed as a function of <span class="math inline">\(\theta\)</span> for fixed <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p>The probability function <span class="math inline">\(p(\theta)\)</span> over the parameters given the
constants <span class="math inline">\(x\)</span> is called the prior because it characterizes the probability
of the parameters before any data are observed. The conditional
probability function <span class="math inline">\(p(\theta|y)\)</span> is called the posterior because
it characterizes the probability of parameters given observed data <span class="math inline">\(y\)</span>
and constants <span class="math inline">\(x\)</span>.</p>
</div>
<div id="bayess-rule" class="section level3 unnumbered">
<h3>Bayes’s Rule</h3>
<p>The technical apparatus of Bayesian inference hinges on the following
chain of equations, known in various forms as Bayes’s rule (where
again, the constants <span class="math inline">\(x\)</span> are implicit).</p>
<p><span class="math display">\[
\begin{array}{rcll}
p(\theta|y)  &amp; =  &amp; \displaystyle \frac{p(\theta,y)}{p(y)}
&amp;  \ \ \ \ \ \mbox{ [definition of  conditional probability]}
\\[16pt]
&amp; = &amp; \displaystyle \frac{p(y|\theta) \, p(\theta)}{p(y)}
&amp; \ \ \ \ \ \mbox{ [chain rule]}
\\[16pt]
&amp; = &amp; \displaystyle \frac{p(y|\theta) \, p(\theta)}
                        {\int_{\Theta} p(y,\theta) \, d\theta}
&amp; \ \ \ \ \ \mbox{ [law of total probability]}
\\[16pt]
&amp; = &amp; \displaystyle \frac{p(y|\theta) \, p(\theta)}
                        {\int_{\Theta} p(y|\theta) \, p(\theta) \, d\theta}
&amp; \ \ \ \ \ \mbox{ [chain rule]}
\\[16pt]
&amp; \propto &amp; \displaystyle p(y|\theta) \, p(\theta)
\end{array}
\]</span></p>
<p>Bayes’s rule “inverts” the probability of the posterior
<span class="math inline">\(p(\theta|y)\)</span>, expressing it solely in terms of the likelihood
<span class="math inline">\(p(y|\theta)\)</span> and prior <span class="math inline">\(p(\theta)\)</span> (again, with constants and
predictors <span class="math inline">\(x\)</span> implicit). The last step is important for Stan, which
only requires probability functions to be characterized up to a
constant multiplier.</p>
</div>
<div id="predictive-inference-1" class="section level3 unnumbered">
<h3>Predictive Inference</h3>
<p>The uncertainty in the estimation of parameters <span class="math inline">\(\theta\)</span> from the data
<span class="math inline">\(y\)</span> (given the model) is characterized by the posterior <span class="math inline">\(p(\theta|y)\)</span>.
The posterior is thus crucial for Bayesian predictive inference.</p>
<p>If <span class="math inline">\(\tilde{y}\)</span> is taken to represent new, perhaps as yet unknown,
observations, along with corresponding constants and predictors
<span class="math inline">\(\tilde{x}\)</span>, then the posterior predictive probability function is
given by</p>
<p><span class="math display">\[
p(\tilde{y}|y)
= \int_{\Theta} p(\tilde{y}|\theta)
                \, p(\theta|y) \, d\theta.
\]</span>
Here, both the original constants and predictors <span class="math inline">\(x\)</span> and the new
constants and predictors <span class="math inline">\(\tilde{x}\)</span> are implicit. Like the posterior
itself, predictive inference is characterized probabilistically.
Rather than using a point estimate of the parameters <span class="math inline">\(\theta\)</span>,
predictions are made based on averaging the predictions over a range
of <span class="math inline">\(\theta\)</span> weighted by the posterior probability <span class="math inline">\(p(\theta|y)\)</span> of
<span class="math inline">\(\theta\)</span> given data <span class="math inline">\(y\)</span> (and constants <span class="math inline">\(x\)</span>).</p>
<p>The posterior may also be used to estimate event probabilities. For
instance, the probability that a parameter <span class="math inline">\(\theta_k\)</span> is greater than
zero is characterized probabilistically by</p>
<p><span class="math display">\[
\mbox{Pr}(\theta_k &gt; 0)
= \int_{\Theta} \mbox{I}(\theta_k &gt; 0) \, p(\theta|y) \, d\theta.
\]</span></p>
<p>The indicator function, <span class="math inline">\(\mbox{I}(\phi)\)</span>, evaluates to one if the
proposition <span class="math inline">\(\phi\)</span> is true and evaluates to zero otherwise.</p>
<p>Comparisons involving future observables may be carried out in
the same way. For example, the probability that <span class="math inline">\(\tilde{y}_n &gt; \tilde{y}_{n&#39;}\)</span> can be characterized using the posterior predictive
probability function as
<span class="math display">\[
\mbox{Pr}(\tilde{y}_n &gt; \tilde{y}_{n&#39;})
= \int_{\Theta} \int_{Y} \mbox{I}(\tilde{y}_n &gt; \tilde{y}_{n&#39;}) \,
p(\tilde{y}|\theta) p(\theta|y) \, d\tilde{y} \, d\theta.
\]</span></p>
</div>
</div>
<div id="model-checking-and-evaluation" class="section level2">
<h2><span class="header-section-number">28.3</span> Model Checking and Evaluation</h2>
<div id="fake-data-simulation" class="section level3 unnumbered">
<h3>Fake-Data Simulation</h3>
</div>
<div id="posterior-predictive-checking-1" class="section level3 unnumbered">
<h3>Posterior Predictive Checking</h3>
<p>After the parameters are fit to data, they can be used to simulate a
new data set by running the model inferences in the forward
direction. These replicated data sets can then be compared to the
original data either visually or statistically to assess model fit
<span class="citation">(Gelman et al. <a href="#ref-GelmanEtAl:2013">2013</a>, Chapter 6)</span>.</p>
<p>In Stan, posterior simulations can be generated in two ways. The
first approach is to treat the predicted variables as parameters and
then define their distributions in the model block. The second
approach, which also works for discrete variables, is to generate
replicated data using random-number generators in the generated
quantities block.</p>
<!--
### Exploratory Data Analysis as Model Checking {-}

### Cross Validation {-}

## Model Expansion

## Decision Analysis
-->

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-GelmanEtAl:2013">
<p>Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. London: Chapman &amp;Hall/CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="part-4-review-of-statistical-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mle-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
